{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-71db8d46da02>:13: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Epoch: 0001, Cost: 5.596248036\n",
      "Epoch: 0002, Cost: 1.788306716\n",
      "Epoch: 0003, Cost: 1.177188766\n",
      "Epoch: 0004, Cost: 0.928310735\n",
      "Epoch: 0005, Cost: 0.788465833\n",
      "Epoch: 0006, Cost: 0.697128146\n",
      "Epoch: 0007, Cost: 0.631512049\n",
      "Epoch: 0008, Cost: 0.581189868\n",
      "Epoch: 0009, Cost: 0.542011129\n",
      "Epoch: 0010, Cost: 0.510350427\n",
      "Learning Finished!\n",
      "Accuracy: 0.8826\n",
      "Label:  [6]\n",
      "Prediction:  [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nEpoch: 0001 Cost: 5.745170949\\nEpoch: 0002 Cost: 1.780056722\\nEpoch: 0003 Cost: 1.122778654\\n...\\nEpoch: 0048 Cost: 0.271918680\\nEpoch: 0049 Cost: 0.270640434\\nEpoch: 0050 Cost: 0.269054370\\nLearning Finished!\\nAccuracy: 0.9194\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lab 7 Learning rate and Evaluation\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(123)  # reproducibility\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "\n",
    "num_epochs = 10\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
    "    )\n",
    ")\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# train my model\n",
    "with tf.Session() as sess:\n",
    "    # initialize\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(f\"Epoch: {(epoch + 1):04d}, Cost: {avg_cost:.9f}\")\n",
    "\n",
    "    print(\"Learning Finished!\")\n",
    "\n",
    "    # Test model and check accuracy\n",
    "    print(\n",
    "        \"Accuracy:\",\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}),\n",
    "    )\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], axis=1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(\n",
    "            tf.argmax(hypothesis, axis=1), feed_dict={X: mnist.test.images[r : r + 1]}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "Epoch: 0001 Cost: 5.745170949\n",
    "Epoch: 0002 Cost: 1.780056722\n",
    "Epoch: 0003 Cost: 1.122778654\n",
    "...\n",
    "Epoch: 0048 Cost: 0.271918680\n",
    "Epoch: 0049 Cost: 0.270640434\n",
    "Epoch: 0050 Cost: 0.269054370\n",
    "Learning Finished!\n",
    "Accuracy: 0.9194\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-17633a94831a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "mnist.test.images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a659a50d30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "# %matplotlib inline # Only use this if using iPython\n",
    "\n",
    "image_index = 123 # You may select anything up to 60,000\n",
    "\n",
    "print(y_train[image_index]) # The label is 8\n",
    "plt.imshow(x_train[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a656369198>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# sess = tf.InteractiveSession()\n",
    "\n",
    "image = np.array([[[[1], [2], [3]], [[4], [5], [6]], [[7], [8], [9]]]], dtype = np.float32)\n",
    "\n",
    "print(image.shape)\n",
    "# plt.imshow(image.reshape(3,3), cmap=‘Greys’) # error Greys\n",
    "\n",
    "plt.imshow(image.reshape(3,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (1, 3, 3, 1)\n",
      "weight.shape (2, 2, 1, 1)\n",
      "conv2d_img (1, 2, 2, 1)\n",
      "[[12. 16.]\n",
      " [24. 28.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# image = np.array([[[[1], [2], [3]], [[4], [5], [6]], [[7], [8], [9]]]], dtype = np.float32)\n",
    "\n",
    "print('image.shape', image.shape)\n",
    "\n",
    "weight = tf.constant([[[[1.]], [[1.]]], [[[1.]], [[1.]]]])\n",
    "\n",
    "print('weight.shape', weight.shape)\n",
    "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='VALID')\n",
    "conv2d_img = conv2d.eval()\n",
    "print('conv2d_img', conv2d_img.shape)\n",
    "                       \n",
    "con2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate(conv2d_img) :\n",
    "                       print(one_img.reshape(2, 2))\n",
    "                       plt.subplot(1, 2, i+1), plt.imshow(one_img.reshape(2, 2), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a6000a8160>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "\n",
    "\n",
    "img = mnist.train.images[55].reshape(28,28)\n",
    "plt.imshow(img, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D_10:0\", shape=(1, 14, 14, 7), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABLCAYAAABgOHyfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFOJJREFUeJztnXtQlNUbx79HEbyA3BRBEbHUFLWLGFmpZeWoeakZzTGzzH6llandTK3RLk5T1kxZppXTJI1mV0vNzGuFY2qmllGoiZo3EAETdEVAeX9/wHv4vsouyO6+7K7PZ4bh2cOy7/nuvu/Z9zzneZ6jDMOAIAiC4P/Uq+sOCIIgCJ5BBnRBEIQAQQZ0QRCEAEEGdEEQhABBBnRBEIQAQQZ0QRCEAEEGdEEQhADBrQFdKdVfKbVHKZWplJrqqU7ZiWjwDUSDbyAa/BzDMGr1A6A+gH0ArgAQDGAngKTavl5d/IgG3/gRDb7xIxr8/0dVvAmXjFLqRgAvGYbRr+LxtIoviNec/U+zZs2MNm3a1Op43uD06dPIzs5G+/btAQDHjh1DVlaWwzCMUGf/ExoaakRFRdnWx+ooLi5GYWEhmjdvDgAoLCxEQUGBSw0NGjQwGjZsaFsfq+P8+fMoKSlBo0aNAAAlJSUoKSlxqUEp5Q8pzi41REREGC1btrSzPy45c+YM8vLykJCQAADIy8tDbm6uSw2NGjUywsLCbOtjdZSWlsLhcCAiIgJAuSaHw+FSQ/369Y2goCDb+lgbSkpK8gzDaF7d89xR0QrAYXp8BMANFz5JKTUWwFgASEhIwJYtW9w4pGdZsmQJ1qxZgw8//BAAsGjRIjz00ENnLnwea4iMjMTkyZPt7agLfv/9d+zatQsjR44EAGzduhWLFi1yqSEkJATJycn2dtQFubm5OHHiBK666ioAQE5ODnbv3u1Sg5/gUkNsbCw+/fRT2zvljHXr1mHTpk2YMWMGAGDFihWYMWOGSw2hoaEYPny4vR11QWZmJg4dOoTbbrsNALBnzx6sW7fOpYagoCDEx8fb29FLZP/+/Qdr8jx3fOiqiraL7poMw5hvGEZ3wzC6N2vWzI3DeR4nsxOXGkJDnX7R+xIuNTRo0KAu+nSpuNRQFx2qBS41REZG1kWfnFKb68GcVfk4LjXUqxc4sSHuKDkCoDU9jgeQ5V537CU+Ph5HjhzRj48ePQoApXXWoVoQERGBkydP6scVtl9pCA4ORnFxsX5cYfuVBif4lYaYmBgcO3ZMPz5+/DjgZxpCQ0Nx+vRp/bjC9isN7uDOgP4bgPZKqbZKqWAAIwAs90y37KF79+7IzMzEgQMHUFJSgi+//BIATlb3f75EQkICcnNzkZ+fj3PnzmHHjh2An2lo2rQpioqKUFRUhLKyMnMg8SsNTvArDZ07d8bhw4dx9OhRlJaWYvXq1YCfaYiJiUFBQQEKCwtx/vx57N27F/AzDe5Qax+6YRjnlFJPAFiN8pXljw3D+NtjPbOBoKAgzJ49GwMHDkRZWRlGjx6N9PT0s3Xdr0uhfv36GDZsGObNm4eysjL06NEDWVlZfqVBKYV27dohPT0dhmEgNjYWDofDrzQ4wa80BAUFYcqUKRg/fjzKysowZMgQ7N2716801KtXD7169cLy5cthGAY6deqE/Px8v9LgDm4t7RqGsRLASg/1xSU8Jf/2228BAMuWLdNtZ85Urnuwn9tcsQeAoUOHart793I37IABAzBgwADd/uKLL3qw11bOnTun7Y4dOwIArr/+et3GGv766y9t//135ffkwYOVayPmImLnzp3RuXNn3f799997sNdWOMLHjA7iBaWcnBxtHz5cuWb+448/avvs2crry+x3dHQ0oqOjdfu///7ruU5fgPneA8CECRP08U0KCwu1XVZWpu2dO3dq+/PPP9f2f//955V+uoLXQRYvXgwAmDNnjm5jfzifG71799b2sGHDtB0cHAwA6NmzJ3r27Knb582b58FeW7niiiu0fc011wDQbk8AwIkTJ7RdWlrpNeHz58CBA9pu3LgxACAxMRGJiYm6/ddff/Vcpy8gJiZG2+3atQMAHXEGwOLCcjgc2l63bp222YfPr1cbAmc1QBAE4TJHBnRBEIQAwaej6YuKirQ9dWplBu8PP/wAAHjmmWd0G4dEnjp1Sts//fSTtp999lltm7HnQKXrwhvw1Lht27barlhwwltvvaXb2M3Sp08fbZsxtYDVBcBTUm8mO7GL4vz589o2o2vq16+v2/Ly8rR91113afuTTz7RNutht0yrVq081OOLuf3227X9/vvva3vjxo0ArC4h/sxat64M5GI94eHh2n733Xe1zW4zT6NUZaTwuHHjtG2eE/weX3fdddpOS0vT9meffaZt8zoCgA8++EDbTZo08VCPL6ZHjx7a5v6arp0bb7xRt4WEhGibQzxZ+9KlS7WdmZmpbW8mCvF4wfkcpquFj81upZtvvlnbX3zxhbZZ86FDh7RtJkddCnKHLgiCECDIgC4IghAg+LTLhVenOarAnK7ceuutVf4fT3s3b96sbZ4q1WY6U1PYBXH33Xdrm6dWZg2PESNG6LYhQ4Zom10ba9eu1fb+/fu1PWXKFG1nZ2e7220LnAGYkpKi7UWLFmnbjJ7YtGlTla/BLrOHH35Y22+88Ya2uRQET5k9AWtgFxtHAb3++usAnL9/gwcP1jZHi1TENwOwusG8ydNPP63tpKQkbZuuRD7vOdmM3QKcSMduI44qYp2egF04r776apXtK1eWB8ux+4q58sortd2hQwdtz549W9s33FBZeWT79u1u9Phi+Fx64IEHtL1r1y5tm+eys6ia5csr03TYnTRr1ixtsxu5Nu47uUMXBEEIEGRAFwRBCBB8zuXCK8QVaewAKl0UALB7924A1mnYwoULtZ2RkaHtrVu3apunQt4sjMTlRNldYiZPAJVRKZwAYiZ3ANYol2nTpmn7qaee0ran3SwMR1RwpAe/5++99x4Aq9uIE0DMRA8AGDhwoLaXLFmibXbLeBqe0nPkE3/2ZnLUTTfdpNv69eunbXa5sM3npjddLpyMwq4QTmYyE55iY2N1m+nCAKzJQW+++aa2b7nlFm1783PgaKymTZtqm6/Hxx9/HIDVbcLnz+jRo7XNbr89e/Zo+59//vFQjy+G3agcrfLNN99o+6uvvgJgfV9ZA7su+f3m69vdKCm5QxcEQQgQZEAXBEEIEHzO5cLTV44QYffC888/D8A69WLXCk9t1q9fr2276k8XFBRom6fGHJlgJiH0799ft/GK/cSJE7XN7oq4uDjPdtYJ7KLgZJRXXnlF2/fccw+AShcYYJ1icmQJJw15c3rPcE2W116r3EiLNXTq1AmANQGEk7RMVwBgrTNiV2SLs+iKjz76SNvjx48HYE3qMuuKXPjcrl27atuuz4Ejtvbt26dtTiwyPx++5rmmzJNPPqltdtvk5+d7trNO4JK88+fP13ZFhVYAlbWlcnNzdRu7yTh5i6PVOCLJXeQOXRAEIUDw6Tv0Fi1aaPvPP//Utrlg+OCDD+o2vnvibz93q5fVBl7oHDRokLb5bv2FF14AAMsWZDyz4DtKXqC0C14E4vh5jpk1F+x4MY718v/99ttv2uYYXG9SUlKibV4g5XPs66+/BmCt5skVOvnuiSvn2QVX4uMFwO+++07b5h06V0/8+eeftc1VGJ3FeXsTnrVxLDZXfjSvAy4NwNcujwW84FsXuw3xrIBn1WY1VY7v58qL/H9c8oKvNXeRO3RBEIQAQQZ0QRCEAMGnXS68MNiwYUNtm4sSvCkBT1u4uP3IkSO1fe2113q0r85gFwS7WR555BFt9+3bF4B10YTjvSdPnqxtTpu3a4NnXpDimGuOkzUXf9jFxYtHXDmO2+1yuXBs+XPPPadtjuU3F9M5Vprj6nkRmt02rMebcFz/yy+/rG0un3D11Vdf9NwuXbpom91GHFxg1wI751RwjDbHxJsBC9u2bdNt7KLgc4nHAnareRPeiIarpnK/NmzYAMB6zfNGFt26ddM2lyTxZBkSuUMXBEEIEGRAFwRBCBB8zuXCqf8c48nTetPVwvGt5oYRgHVaye4XuzDjswHr/qYclWNWfuToCi4NwHHgPIXjiAFvwhXteEMIjjYw3S8cF8xuMK5Ex58Da/MmL730krbnzp2rbY64MV0tHCfPFQHZLVAXERWcA8DVOKvadIRdQjNnztQ2l83gqAu74IgcLp/A55JZsZPdlVwxkSNe2HXK++16E65YyRF1v/zyi7bN65TzXTj3hK9jdq968nqodkBXSn0MYBCA44ZhdKloiwLwBYBEAP8CGG4Yhv075daQSZMmYe3atYiJicEff/wBoPxCvu+++3Dw4EG0adMGixcvti3xqDasXLkS+/btQ3h4uK794HA4kJqaihMnTiAqKgpjxoyxbcCvDTt37kROTg6CgoK0T7K0tBQZGRkoLi5GSEgIkpKSbFsnqA1FRUWWzb79kZkzZ2Ljxo2IiorS9UcKCgowdepUZGVloWXLlpg1a5bli9rXWLhwIdLT0xEcHIx7770XQPnm0atXr8apU6cQFhaGfv36WfztlwM1ueVIBdD/grapANYbhtEewPqKxz7LiBEjLPXUgfJFpT59+iAjIwN9+vSxLDL5Il27drXc+QPlCy4dOnTA9OnT0aFDB0shMF8kPj7eUnwJKF9UioyMREpKCiIjIy3xub5IgwYNfPpLsyYMHDgQ77zzjqVtwYIFSElJwbJly5CSkoIFCxbUUe9qRo8ePfDEE09Y2nbs2IH4+HiMGjUK8fHxlsX8y4Vq79ANw9iglEq8oPkuALdW2J8A+BnAFHgA/hDMtGzAml5uVpfjAYz36ONqfhERERgwYIClgD9QPg00V6Dvv/9+3HHHHZZkHnfgqSIn1/Cdp7k6z5XbHnvsMW1zckKrVq3QqlWri9Kc09PTMWHCBADlSUlz5syx7HvpDjx952gInuKa0352V6xYsULbHBmQkZGB4OBgnD171nKc/Px87Wpq0aIFdu7caUnDdweuXsmuII4AMZO5Ro0apdvMFO4LbS4lYBfssuPPn6sw5uTkALAmfbVp00bbZhIbUJ4clZKSgqysLMtx0tLSdPTYoEGDMHbsWEyaNMkjGngzEN5c5O2339Z2r169Lvo77xfKlRTNLyMudQAABw4c0KUDOnbsiKVLl1oindyB92jlvVjN9x6ofM85moXdLHz+eMvtWFunYAvDMLIBoOK3/emYbnL8+HE9UMXFxVnqL/gLp06d0gN/eHi4bb5pT1JSUqLDGENCQiyhd4J95Ofna/968+bNLWGc/sKZM2f0jUiTJk1sq1XjS3h9lUcpNVYptU0pte3Cb1R/gTXYFX/saViDvw6arKGu+1JbWAPfvfkTrMFfB03WYFehNTuobZRLjlIqzjCMbKVUHIDjzp5oGMZ8APMBIDk52XD2PBOuo8GREZwYYn4Lc1QIVzLjDQqcERMTg+zsbMTFxSE7O9vl6j9rSEhIqFYDL5oNHTpU2zwVM1fKORqBa3VwdTlnhIWFoaCgAOHh4SgoKLBsrOFKQ1hYWLUaDh48qG2u1rdq1Sptm7VNeHZzoZulOoKDg/WCaHFxscsFUdaglKpWA7t/uG4I15sx9zHlKBj+P09vIsIakpKSqtXA1wBHVPB+uuZUf9y4cbqNk9F4H1FnREdHIzc3F82bN0dubq6l4qQrDTExMdVq4Bs57gsfw3R3NmvWTLdxlBTXo3FG48aN4XA40KRJEzgcDkulSlcaQkJCqtXALt/Fixdrm88V08XHX3KcKJWWllatBnep7R36cgDmFiKjASxz8VyfZPDgwXqXo4ULF1rCqfyFLl266EzHrVu3Wkqj+gvR0dHaD5mTk2PxdQv20bt3bz04rVixwpLR6S8kJibqgXf37t2WjM7LhZqELX6G8gXQZkqpIwBeBPA6gC+VUv8DcAjAPc5foe4ZNWoUNmzYgLy8PLRt2xYzZszA5MmTMXLkSKSmpqJ169aWeF9fJDU1FZmZmTh9+jSmT5+OO++8E3379sWCBQuwZcsWREZGYsyYMXXdTZdkZGSgoKAApaWl2Lx5MxITE5GQkICMjAwcO3ZMhy0K3mXatGnYvn07Tp48if79++PRRx/FmDFjMGXKFCxduhSxsbE+H/W1Zs0aHD16FGfPnkVqaipSUlKQnJyMVatWYdeuXQgNDbXsNXC5oLjUq7dJTk42tmzZ4vI5POXmEDZOBjGTDDjBwFP1QYKDg7cbhtHd2d8TEhIMrrNSFZwcZSZMANYNOcwaLlw3hCNb3GHixIkuNYSFhRmcKFEVPG1kHyMnRJifFbtcPOWPTEtLc6mhJi4Xvtvn0sQcm2xGTPG55sE1BpcakpKSDC6fXBXcF06uYVeMGf3CGjx1XXfr1s2lhpiYGGP48OEuX4MjzDgCjM990yXGf/fUxg9z5851qSEkJMSIj493+RpcwpqjpHgMMiNxOFLPU+fS/v37XWowkdR/QRCEAEEGdEEQhADB52q58BSF6zr406IlR7nwVJ9tX8dZhIA/xSdzIg4ng/gT7ILkMEd/Cnlk9xDbfK3z3r++CLuw0tPTq7R9AblDFwRBCBBkQBcEQQgQbI1yUUrlAnAAsDtltNklHLONYRhOs4xEg1uIBkI0uIVoqAJbB3QAUEptq0n4jS8fUzT4xjFFg28cUzT4zjHF5SIIghAgyIAuCIIQINTFgD4/AI4pGnzjmKLBN44pGnzkmLb70AVBEATvIC4XQRCEAEEGdEEQhADB1gFdKdVfKbVHKZWplPLKxtJKqY+VUseVUn9RW5RSaq1Sam/F70g3Xl801OwYoqH61xcNNTuGaKgphmHY8gOgPoB9AK4AEAxgJ4AkLxynN4BuAP6itjcATK2wpwKYJRpEg2gQDYGgwXIcT3fchaAbAaymx9MATPPSsRIveOP2AIirsOMA7BENokE0iIZA0MA/drpcWgE4TI+PVLTZQQvDMLIBoOJ3TDXPd4ZocA/RUIlocA/RUAV2DuiqijZ/i5kUDb6BaPANRIOPYeeAfgRAa3ocDyDLpmPnKKXiAKDi9/Favo5ocA/RUIlocA/RUAV2Dui/AWivlGqrlAoGMALAcpuOvRzA6Ap7NIBltXwd0eAeoqES0eAeoqEqvOH8d7EocCeAf1C+qvyCl47xGYBsAKUo//b9H4BoAOsB7K34HSUaRINoEA2BosH8kdR/QRCEAEEyRQVBEAIEGdAFQRACBBnQBUEQAgQZ0AVBEAIEGdAFQRACBBnQBUEQAgQZ0AVBEAKE/wM0InuPk+wZOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 7 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = img.reshape( -1, 28, 28, 1)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([ 3, 3, 1, 7], stddev=0.01))\n",
    "\n",
    "conv2d = tf.nn.conv2d(img, W1, strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "print(conv2d )\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "conv2d_img = conv2d.eval()\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "\n",
    "for i, one_img in enumerate(conv2d_img) :\n",
    "                       plt.subplot(1, 7, i+1), plt.imshow(one_img.reshape(14, 14), cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2614adc3be0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 2 # You may select anything up to 60,000\n",
    "\n",
    "print(y_train[image_index]) # The label is 4\n",
    "plt.imshow(x_train[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-35-31e17164e94f>:39: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch: 0001 cost = 184.221405341\n",
      "Epoch: 0002 cost = 44.003591563\n",
      "Epoch: 0003 cost = 27.635045133\n",
      "Epoch: 0004 cost = 19.166390351\n",
      "Epoch: 0005 cost = 13.808683334\n",
      "Epoch: 0006 cost = 10.519455102\n",
      "Learning Finished!\n",
      "Accuracy: 0.9303\n",
      "Label:  [2]\n",
      "Prediction:  [2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEpoch: 0001 cost = 141.207671860\\nEpoch: 0002 cost = 38.788445864\\nEpoch: 0003 cost = 23.977515479\\nEpoch: 0004 cost = 16.315132428\\nEpoch: 0005 cost = 11.702554882\\nEpoch: 0006 cost = 8.573139748\\nEpoch: 0007 cost = 6.370995680\\nEpoch: 0008 cost = 4.537178684\\nEpoch: 0009 cost = 3.216900532\\nEpoch: 0010 cost = 2.329708954\\nEpoch: 0011 cost = 1.715552875\\nEpoch: 0012 cost = 1.189857912\\nEpoch: 0013 cost = 0.820965160\\nEpoch: 0014 cost = 0.624131458\\nEpoch: 0015 cost = 0.454633765\\nLearning Finished!\\nAccuracy: 0.9455\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lab 10 MNIST and NN\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(123)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 6    # Accuracy: 0.9305  # epochs 15, Accuracy: 0.9455, Epoch: 0006 cost = 8.573139748\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()\n",
    "\n",
    "'''\n",
    "Epoch: 0001 cost = 141.207671860\n",
    "Epoch: 0002 cost = 38.788445864\n",
    "Epoch: 0003 cost = 23.977515479\n",
    "Epoch: 0004 cost = 16.315132428\n",
    "Epoch: 0005 cost = 11.702554882\n",
    "Epoch: 0006 cost = 8.573139748\n",
    "Epoch: 0007 cost = 6.370995680\n",
    "Epoch: 0008 cost = 4.537178684\n",
    "Epoch: 0009 cost = 3.216900532\n",
    "Epoch: 0010 cost = 2.329708954\n",
    "Epoch: 0011 cost = 1.715552875\n",
    "Epoch: 0012 cost = 1.189857912\n",
    "Epoch: 0013 cost = 0.820965160\n",
    "Epoch: 0014 cost = 0.624131458\n",
    "Epoch: 0015 cost = 0.454633765\n",
    "Learning Finished!\n",
    "Accuracy: 0.9455\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 0.309621046\n",
      "Epoch: 0002 cost = 0.112416850\n",
      "Epoch: 0003 cost = 0.075379597\n",
      "Epoch: 0004 cost = 0.055403218\n",
      "Epoch: 0005 cost = 0.039604680\n",
      "Epoch: 0006 cost = 0.029997048\n",
      "Learning Finished!\n",
      "Accuracy: 0.9766\n",
      "Label:  [0]\n",
      "Prediction:  [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEpoch: 0001 cost = 0.301498963\\nEpoch: 0002 cost = 0.107252513\\nEpoch: 0003 cost = 0.064888892\\nEpoch: 0004 cost = 0.044463030\\nEpoch: 0005 cost = 0.029951642\\nEpoch: 0006 cost = 0.020663404\\nEpoch: 0007 cost = 0.015853033\\nEpoch: 0008 cost = 0.011764387\\nEpoch: 0009 cost = 0.008598264\\nEpoch: 0010 cost = 0.007383116\\nEpoch: 0011 cost = 0.006839140\\nEpoch: 0012 cost = 0.004672963\\nEpoch: 0013 cost = 0.003979437\\nEpoch: 0014 cost = 0.002714260\\nEpoch: 0015 cost = 0.004707661\\nLearning Finished!\\nAccuracy: 0.9783\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lab 10 MNIST and Xavier\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(123)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 6     # epochs = 15, Accuracy: 0.9783\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()\n",
    "\n",
    "'''\n",
    "Epoch: 0001 cost = 0.301498963\n",
    "Epoch: 0002 cost = 0.107252513\n",
    "Epoch: 0003 cost = 0.064888892\n",
    "Epoch: 0004 cost = 0.044463030\n",
    "Epoch: 0005 cost = 0.029951642\n",
    "Epoch: 0006 cost = 0.020663404\n",
    "Epoch: 0007 cost = 0.015853033\n",
    "Epoch: 0008 cost = 0.011764387\n",
    "Epoch: 0009 cost = 0.008598264\n",
    "Epoch: 0010 cost = 0.007383116\n",
    "Epoch: 0011 cost = 0.006839140\n",
    "Epoch: 0012 cost = 0.004672963\n",
    "Epoch: 0013 cost = 0.003979437\n",
    "Epoch: 0014 cost = 0.002714260\n",
    "Epoch: 0015 cost = 0.004707661\n",
    "Learning Finished!\n",
    "Accuracy: 0.9783\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-fe21dae2ff4e>:11: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-fe21dae2ff4e>:53: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch: 0001 cost = 0.291563575\n",
      "Epoch: 0002 cost = 0.101755542\n",
      "Epoch: 0003 cost = 0.071341329\n",
      "Epoch: 0004 cost = 0.052176375\n",
      "Epoch: 0005 cost = 0.041552496\n",
      "Epoch: 0006 cost = 0.033437219\n",
      "Epoch: 0007 cost = 0.027995250\n",
      "Epoch: 0008 cost = 0.027004075\n",
      "Epoch: 0009 cost = 0.023694695\n",
      "Epoch: 0010 cost = 0.019384865\n",
      "Epoch: 0011 cost = 0.018703812\n",
      "Epoch: 0012 cost = 0.017348291\n",
      "Epoch: 0013 cost = 0.016754357\n",
      "Epoch: 0014 cost = 0.013416878\n",
      "Epoch: 0015 cost = 0.014078607\n",
      "Learning Finished!\n",
      "Accuracy: 0.98\n",
      "Label:  [1]\n",
      "Prediction:  [1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEpoch: 0001 cost = 0.266061549\\nEpoch: 0002 cost = 0.080796588\\nEpoch: 0003 cost = 0.049075800\\nEpoch: 0004 cost = 0.034772298\\nEpoch: 0005 cost = 0.024780529\\nEpoch: 0006 cost = 0.017072763\\nEpoch: 0007 cost = 0.014031383\\nEpoch: 0008 cost = 0.013763446\\nEpoch: 0009 cost = 0.009164047\\nEpoch: 0010 cost = 0.008291388\\nEpoch: 0011 cost = 0.007319742\\nEpoch: 0012 cost = 0.006434021\\nEpoch: 0013 cost = 0.005684378\\nEpoch: 0014 cost = 0.004781207\\nEpoch: 0015 cost = 0.004342310\\nLearning Finished!\\nAccuracy: 0.9742\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lab 10 MNIST and Deep learning\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(123)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()\n",
    "\n",
    "'''\n",
    "Epoch: 0001 cost = 0.266061549\n",
    "Epoch: 0002 cost = 0.080796588\n",
    "Epoch: 0003 cost = 0.049075800\n",
    "Epoch: 0004 cost = 0.034772298\n",
    "Epoch: 0005 cost = 0.024780529\n",
    "Epoch: 0006 cost = 0.017072763\n",
    "Epoch: 0007 cost = 0.014031383\n",
    "Epoch: 0008 cost = 0.013763446\n",
    "Epoch: 0009 cost = 0.009164047\n",
    "Epoch: 0010 cost = 0.008291388\n",
    "Epoch: 0011 cost = 0.007319742\n",
    "Epoch: 0012 cost = 0.006434021\n",
    "Epoch: 0013 cost = 0.005684378\n",
    "Epoch: 0014 cost = 0.004781207\n",
    "Epoch: 0015 cost = 0.004342310\n",
    "Learning Finished!\n",
    "Accuracy: 0.9742\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 10 MNIST and High-level TF API\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm, dropout\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.01  # we can use large learning rate using Batch Normalization\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "keep_prob = 0.7\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "train_mode = tf.placeholder(tf.bool, name='train_mode')\n",
    "\n",
    "# layer output size\n",
    "hidden_output_size = 512\n",
    "final_output_size = 10\n",
    "\n",
    "xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "bn_params = {\n",
    "    'is_training': train_mode,\n",
    "    'decay': 0.9,\n",
    "    'updates_collections': None\n",
    "}\n",
    "\n",
    "# We can build short code using 'arg_scope' to avoid duplicate code\n",
    "# same function with different arguments\n",
    "with arg_scope([fully_connected],\n",
    "               activation_fn=tf.nn.relu,\n",
    "               weights_initializer=xavier_init,\n",
    "               biases_initializer=None,\n",
    "               normalizer_fn=batch_norm,\n",
    "               normalizer_params=bn_params\n",
    "               ):\n",
    "    hidden_layer1 = fully_connected(X, hidden_output_size, scope=\"h1\")\n",
    "    h1_drop = dropout(hidden_layer1, keep_prob, is_training=train_mode)\n",
    "    hidden_layer2 = fully_connected(h1_drop, hidden_output_size, scope=\"h2\")\n",
    "    h2_drop = dropout(hidden_layer2, keep_prob, is_training=train_mode)\n",
    "    hidden_layer3 = fully_connected(h2_drop, hidden_output_size, scope=\"h3\")\n",
    "    h3_drop = dropout(hidden_layer3, keep_prob, is_training=train_mode)\n",
    "    hidden_layer4 = fully_connected(h3_drop, hidden_output_size, scope=\"h4\")\n",
    "    h4_drop = dropout(hidden_layer4, keep_prob, is_training=train_mode)\n",
    "    hypothesis = fully_connected(h4_drop, final_output_size, activation_fn=None, scope=\"hypothesis\")\n",
    "\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict_train = {X: batch_xs, Y: batch_ys, train_mode: True}\n",
    "        feed_dict_cost = {X: batch_xs, Y: batch_ys, train_mode: False}\n",
    "        opt = sess.run(optimizer, feed_dict=feed_dict_train)\n",
    "        c = sess.run(cost, feed_dict=feed_dict_cost)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost))\n",
    "    #print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, train_mode: False}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], train_mode: False}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()\n",
    "\n",
    "'''\n",
    "[Epoch:    1] cost = 0.519417209\n",
    "[Epoch:    2] cost = 0.432551052\n",
    "[Epoch:    3] cost = 0.404978843\n",
    "[Epoch:    4] cost = 0.392039919\n",
    "[Epoch:    5] cost = 0.382165317\n",
    "[Epoch:    6] cost = 0.377987834\n",
    "[Epoch:    7] cost = 0.372577601\n",
    "[Epoch:    8] cost = 0.367208552\n",
    "[Epoch:    9] cost = 0.365525589\n",
    "[Epoch:   10] cost = 0.361964276\n",
    "[Epoch:   11] cost = 0.359540287\n",
    "[Epoch:   12] cost = 0.356423751\n",
    "[Epoch:   13] cost = 0.354478216\n",
    "[Epoch:   14] cost = 0.353212552\n",
    "[Epoch:   15] cost = 0.35230893\n",
    "Learning Finished!\n",
    "Accuracy: 0.9826\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 10 MNIST and Dropout\n",
    "# SELU implementation from https://github.com/bioinf-jku/SNNs/blob/master/selu.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Tensorflow Implementation of the Scaled ELU function and Dropout\n",
    "'''\n",
    "\n",
    "import numbers\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.layers import utils\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "def selu(x):\n",
    "    with ops.name_scope('elu') as scope:\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        return scale*tf.where(x>=0.0, x, alpha*tf.nn.elu(x))\n",
    "\n",
    "\n",
    "def dropout_selu(x, keep_prob, alpha= -1.7580993408473766, fixedPointMean=0.0, fixedPointVar=1.0,\n",
    "                 noise_shape=None, seed=None, name=None, training=False):\n",
    "    \"\"\"Dropout to a value with rescaling.\"\"\"\n",
    "\n",
    "    def dropout_selu_impl(x, rate, alpha, noise_shape, seed, name):\n",
    "        keep_prob = 1.0 - rate\n",
    "        x = ops.convert_to_tensor(x, name=\"x\")\n",
    "        if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n",
    "            raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n",
    "                                             \"range (0, 1], got %g\" % keep_prob)\n",
    "        keep_prob = ops.convert_to_tensor(keep_prob, dtype=x.dtype, name=\"keep_prob\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        alpha = ops.convert_to_tensor(alpha, dtype=x.dtype, name=\"alpha\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        if tensor_util.constant_value(keep_prob) == 1:\n",
    "            return x\n",
    "\n",
    "        noise_shape = noise_shape if noise_shape is not None else array_ops.shape(x)\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += random_ops.random_uniform(noise_shape, seed=seed, dtype=x.dtype)\n",
    "        binary_tensor = math_ops.floor(random_tensor)\n",
    "        ret = x * binary_tensor + alpha * (1-binary_tensor)\n",
    "\n",
    "        a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "\n",
    "        b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "        ret = a * ret + b\n",
    "        ret.set_shape(x.get_shape())\n",
    "        return ret\n",
    "\n",
    "    with ops.name_scope(name, \"dropout\", [x]) as name:\n",
    "        return utils.smart_cond(training,\n",
    "                                lambda: dropout_selu_impl(x, keep_prob, alpha, noise_shape, seed, name),\n",
    "                                lambda: array_ops.identity(x))\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = selu(tf.matmul(X, W1) + b1)\n",
    "L1 = dropout_selu(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = selu(tf.matmul(L1, W2) + b2)\n",
    "L2 = dropout_selu(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = selu(tf.matmul(L2, W3) + b3)\n",
    "L3 = dropout_selu(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = selu(tf.matmul(L3, W4) + b4)\n",
    "L4 = dropout_selu(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()\n",
    "\n",
    "'''\n",
    "Epoch: 0001 cost = 0.447322626\n",
    "Epoch: 0002 cost = 0.157285590\n",
    "Epoch: 0003 cost = 0.121884535\n",
    "Epoch: 0004 cost = 0.098128681\n",
    "Epoch: 0005 cost = 0.082901778\n",
    "Epoch: 0006 cost = 0.075337573\n",
    "Epoch: 0007 cost = 0.069752543\n",
    "Epoch: 0008 cost = 0.060884363\n",
    "Epoch: 0009 cost = 0.055276413\n",
    "Epoch: 0010 cost = 0.054631256\n",
    "Epoch: 0011 cost = 0.049675195\n",
    "Epoch: 0012 cost = 0.049125314\n",
    "Epoch: 0013 cost = 0.047231930\n",
    "Epoch: 0014 cost = 0.041290121\n",
    "Epoch: 0015 cost = 0.043621063\n",
    "Learning Finished!\n",
    "Accuracy: 0.9804\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable W1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2ad6cf722d44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m W1 = tf.get_variable(\"W1\", shape=[784, 512],\n\u001b[1;32m---> 33\u001b[1;33m                      initializer=tf.contrib.layers.xavier_initializer())\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mL1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1326\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1328\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m   1329\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1330\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1088\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m    433\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    402\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    741\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 743\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    744\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable W1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "# Lab 10 MNIST and Dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 5\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( ogits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()\n",
    "\n",
    "'''\n",
    "Epoch: 0001 cost = 0.447322626\n",
    "Epoch: 0002 cost = 0.157285590\n",
    "Epoch: 0003 cost = 0.121884535\n",
    "Epoch: 0004 cost = 0.098128681\n",
    "Epoch: 0005 cost = 0.082901778\n",
    "Epoch: 0006 cost = 0.075337573\n",
    "Epoch: 0007 cost = 0.069752543\n",
    "Epoch: 0008 cost = 0.060884363\n",
    "Epoch: 0009 cost = 0.055276413\n",
    "Epoch: 0010 cost = 0.054631256\n",
    "Epoch: 0011 cost = 0.049675195\n",
    "Epoch: 0012 cost = 0.049125314\n",
    "Epoch: 0013 cost = 0.047231930\n",
    "Epoch: 0014 cost = 0.041290121\n",
    "Epoch: 0015 cost = 0.043621063\n",
    "Learning Finished!\n",
    "Accuracy: 0.9804\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
