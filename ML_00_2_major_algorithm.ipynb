{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "0 6.313676 [0.9977196] [2.0147095]\n",
      "20 0.99175376 [1.6774234] [2.0287082]\n",
      "40 0.8686492 [1.722185] [1.8782511]\n",
      "60 0.7612325 [1.7613485] [1.7369088]\n",
      "80 0.6674253 [1.7979351] [1.6048198]\n",
      "100 0.5855026 [1.8321257] [1.4813812]\n",
      "120 0.51395875 [1.864077] [1.3660268]\n",
      "140 0.45147935 [1.8939358] [1.2582269]\n",
      "160 0.39691558 [1.921839] [1.1574873]\n",
      "180 0.34926453 [1.947915] [1.0633452]\n",
      "200 0.3076504 [1.972283] [0.97536844]\n",
      "220 0.27130866 [1.9950553] [0.8931534]\n",
      "240 0.23957105 [2.0163362] [0.8163226]\n",
      "260 0.21185443 [2.0362234] [0.7445235]\n",
      "280 0.18764928 [2.0548081] [0.6774266]\n",
      "300 0.16651043 [2.0721757] [0.6147239]\n",
      "320 0.14805022 [2.088406] [0.5561277]\n",
      "340 0.13192847 [2.1035733] [0.50136906]\n",
      "360 0.11784929 [2.1177473] [0.4501965]\n",
      "380 0.1055539 [2.130993] [0.4023754]\n",
      "400 0.09481613 [2.143371] [0.35768613]\n",
      "420 0.08543895 [2.1549387] [0.3159236]\n",
      "440 0.07724957 [2.1657486] [0.27689612]\n",
      "460 0.07009794 [2.1758506] [0.24042456]\n",
      "480 0.06385224 [2.1852913] [0.20634155]\n",
      "500 0.05839784 [2.1941133] [0.17449066]\n",
      "520 0.053634424 [2.2023578] [0.14472578]\n",
      "540 0.049474645 [2.2100623] [0.11691016]\n",
      "560 0.045841686 [2.2172623] [0.09091627]\n",
      "580 0.04266915 [2.2239904] [0.06662464]\n",
      "600 0.03989849 [2.2302783] [0.04392394]\n",
      "620 0.037478916 [2.2361538] [0.02270999]\n",
      "640 0.035365686 [2.2416453] [0.00288542]\n",
      "660 0.033520523 [2.2467768] [-0.01564095]\n",
      "680 0.031908773 [2.2515724] [-0.03295405]\n",
      "700 0.03050138 [2.2560534] [-0.0491332]\n",
      "720 0.029272337 [2.2602415] [-0.06425289]\n",
      "740 0.028198898 [2.2641551] [-0.0783824]\n",
      "760 0.027261477 [2.2678125] [-0.09158657]\n",
      "780 0.026442891 [2.2712302] [-0.10392586]\n",
      "800 0.025728006 [2.2744243] [-0.11545718]\n",
      "820 0.025103718 [2.2774088] [-0.12623319]\n",
      "840 0.024558507 [2.2801983] [-0.13630356]\n",
      "860 0.024082337 [2.282805] [-0.1457144]\n",
      "880 0.023666386 [2.2852411] [-0.15450898]\n",
      "900 0.023303216 [2.2875173] [-0.1627276]\n",
      "920 0.022986189 [2.2896447] [-0.17040779]\n",
      "940 0.022709215 [2.291633] [-0.17758514]\n",
      "960 0.022467306 [2.2934904] [-0.18429238]\n",
      "980 0.022256007 [2.2952266] [-0.1905603]\n",
      "1000 0.022071531 [2.296849] [-0.19641766]\n",
      "1020 0.021910552 [2.298365] [-0.20189145]\n",
      "1040 0.021769812 [2.2997823] [-0.20700692]\n",
      "1060 0.021646943 [2.3011065] [-0.21178745]\n",
      "1080 0.021539664 [2.3023436] [-0.21625495]\n",
      "1100 0.021445896 [2.3035002] [-0.2204298]\n",
      "1120 0.021364052 [2.3045805] [-0.22433114]\n",
      "1140 0.0212926 [2.3055904] [-0.227977]\n",
      "1160 0.021230284 [2.306534] [-0.23138401]\n",
      "1180 0.021175725 [2.307416] [-0.23456785]\n",
      "1200 0.021128163 [2.3082402] [-0.23754328]\n",
      "1220 0.021086542 [2.3090103] [-0.24032375]\n",
      "1240 0.021050185 [2.30973] [-0.2429222]\n",
      "1260 0.021018535 [2.3104026] [-0.24535058]\n",
      "1280 0.020990819 [2.311031] [-0.24761973]\n",
      "1300 0.020966623 [2.3116186] [-0.24974039]\n",
      "1320 0.020945583 [2.3121674] [-0.25172222]\n",
      "1340 0.02092714 [2.3126805] [-0.25357422]\n",
      "1360 0.020910963 [2.3131597] [-0.25530478]\n",
      "1380 0.020896886 [2.3136077] [-0.2569221]\n",
      "1400 0.020884708 [2.3140264] [-0.2584334]\n",
      "1420 0.020873982 [2.3144174] [-0.2598457]\n",
      "1440 0.020864582 [2.314783] [-0.26116553]\n",
      "1460 0.020856405 [2.3151248] [-0.26239905]\n",
      "1480 0.020849256 [2.315444] [-0.2635517]\n",
      "1500 0.020843003 [2.3157425] [-0.26462883]\n",
      "1520 0.020837607 [2.3160212] [-0.26563543]\n",
      "1540 0.020832816 [2.3162816] [-0.26657608]\n",
      "1560 0.020828623 [2.3165252] [-0.2674551]\n",
      "1580 0.020825034 [2.3167527] [-0.2682765]\n",
      "1600 0.020821864 [2.3169653] [-0.26904428]\n",
      "1620 0.020819088 [2.3171642] [-0.26976168]\n",
      "1640 0.020816673 [2.31735] [-0.2704322]\n",
      "1660 0.020814631 [2.3175232] [-0.2710587]\n",
      "1680 0.020812774 [2.3176854] [-0.2716442]\n",
      "1700 0.020811161 [2.317837] [-0.27219123]\n",
      "1720 0.020809641 [2.3179786] [-0.27270257]\n",
      "1740 0.020808427 [2.318111] [-0.27318037]\n",
      "1760 0.020807423 [2.3182344] [-0.27362686]\n",
      "1780 0.020806504 [2.3183503] [-0.27404407]\n",
      "1800 0.020805618 [2.318458] [-0.274434]\n",
      "1820 0.020804903 [2.3185592] [-0.2747984]\n",
      "1840 0.020804364 [2.3186533] [-0.275139]\n",
      "1860 0.02080372 [2.3187416] [-0.27545723]\n",
      "1880 0.02080336 [2.318824] [-0.2757546]\n",
      "1900 0.020802878 [2.318901] [-0.27603248]\n",
      "1920 0.02080245 [2.318973] [-0.27629212]\n",
      "1940 0.020802185 [2.3190403] [-0.27653483]\n",
      "1960 0.020801902 [2.319103] [-0.2767616]\n",
      "1980 0.020801667 [2.3191614] [-0.2769735]\n",
      "2000 0.02080152 [2.3192163] [-0.2771716]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# x_train = [1,2,3, 5]\n",
    "# y_train = [3,5,7, 8]\n",
    "# y_train = [3, 7, 8, 12]\n",
    "\n",
    "x_train = [1,2,3, 4, 5]\n",
    "y_train = [2.2, 4.3, 6.5, 8.9, 11.5 ]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = x_train*W + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001) :\n",
    "    sess.run(train)\n",
    "    if step % 20 ==0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x =tf.placeholder(tf.float32)\n",
    "y =tf.placeholder(tf.float32)\n",
    "add_xy= x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "[5. 6. 7. 8.]\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "print(sess.run(add_xy,feed_dict={x:1, y:2}))\n",
    "print(sess.run(add_xy, feed_dict={x:[3,4, 5, 6], y:[2,2, 2, 2]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 5. 7. 7.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(add_xy, feed_dict={x:[3,4,5,5], y:[1, 1, 2, 2]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variable linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  180597.45 \n",
      "Prediction:\n",
      " [[-216.73132]\n",
      " [-275.02283]\n",
      " [-263.77264]\n",
      " [-283.48074]\n",
      " [-216.56308]]\n",
      "10 Cost:  68.4937 \n",
      "Prediction:\n",
      " [[159.10255]\n",
      " [176.75732]\n",
      " [181.34508]\n",
      " [201.24326]\n",
      " [128.04366]]\n",
      "20 Cost:  66.515114 \n",
      "Prediction:\n",
      " [[160.21034]\n",
      " [178.14453]\n",
      " [182.68304]\n",
      " [202.70226]\n",
      " [129.11398]]\n",
      "30 Cost:  66.190956 \n",
      "Prediction:\n",
      " [[160.1844 ]\n",
      " [178.16891]\n",
      " [182.67827]\n",
      " [202.699  ]\n",
      " [129.14478]]\n",
      "40 Cost:  65.86851 \n",
      "Prediction:\n",
      " [[160.15512]\n",
      " [178.18912]\n",
      " [182.66946]\n",
      " [202.69138]\n",
      " [129.17238]]\n",
      "50 Cost:  65.54773 \n",
      "Prediction:\n",
      " [[160.12589]\n",
      " [178.20927]\n",
      " [182.66064]\n",
      " [202.68375]\n",
      " [129.19987]]\n",
      "60 Cost:  65.2286 \n",
      "Prediction:\n",
      " [[160.09677]\n",
      " [178.2294 ]\n",
      " [182.65193]\n",
      " [202.67613]\n",
      " [129.22734]]\n",
      "70 Cost:  64.91136 \n",
      "Prediction:\n",
      " [[160.06772]\n",
      " [178.24942]\n",
      " [182.64317]\n",
      " [202.66856]\n",
      " [129.25471]]\n",
      "80 Cost:  64.595695 \n",
      "Prediction:\n",
      " [[160.03876]\n",
      " [178.26944]\n",
      " [182.63449]\n",
      " [202.66101]\n",
      " [129.28203]]\n",
      "90 Cost:  64.281715 \n",
      "Prediction:\n",
      " [[160.00986]\n",
      " [178.28937]\n",
      " [182.62578]\n",
      " [202.65344]\n",
      " [129.30925]]\n",
      "100 Cost:  63.96949 \n",
      "Prediction:\n",
      " [[159.98106]\n",
      " [178.30927]\n",
      " [182.61713]\n",
      " [202.64595]\n",
      " [129.33643]]\n",
      "110 Cost:  63.658875 \n",
      "Prediction:\n",
      " [[159.95232]\n",
      " [178.32907]\n",
      " [182.60847]\n",
      " [202.63843]\n",
      " [129.36351]]\n",
      "120 Cost:  63.349964 \n",
      "Prediction:\n",
      " [[159.92368]\n",
      " [178.34886]\n",
      " [182.59985]\n",
      " [202.6309 ]\n",
      " [129.3905 ]]\n",
      "130 Cost:  63.042774 \n",
      "Prediction:\n",
      " [[159.89511]\n",
      " [178.36858]\n",
      " [182.59128]\n",
      " [202.62344]\n",
      " [129.41745]]\n",
      "140 Cost:  62.73705 \n",
      "Prediction:\n",
      " [[159.8666 ]\n",
      " [178.38821]\n",
      " [182.5827 ]\n",
      " [202.61595]\n",
      " [129.44432]]\n",
      "150 Cost:  62.433083 \n",
      "Prediction:\n",
      " [[159.83818]\n",
      " [178.40784]\n",
      " [182.57414]\n",
      " [202.60852]\n",
      " [129.47112]]\n",
      "160 Cost:  62.130737 \n",
      "Prediction:\n",
      " [[159.80981]\n",
      " [178.42737]\n",
      " [182.56563]\n",
      " [202.60109]\n",
      " [129.49783]]\n",
      "170 Cost:  61.829906 \n",
      "Prediction:\n",
      " [[159.78157]\n",
      " [178.4469 ]\n",
      " [182.55713]\n",
      " [202.59369]\n",
      " [129.5245 ]]\n",
      "180 Cost:  61.530926 \n",
      "Prediction:\n",
      " [[159.75339]\n",
      " [178.46632]\n",
      " [182.54866]\n",
      " [202.58629]\n",
      " [129.55107]]\n",
      "190 Cost:  61.233276 \n",
      "Prediction:\n",
      " [[159.72527]\n",
      " [178.48573]\n",
      " [182.54019]\n",
      " [202.57892]\n",
      " [129.57759]]\n",
      "200 Cost:  60.93739 \n",
      "Prediction:\n",
      " [[159.69725]\n",
      " [178.50507]\n",
      " [182.53178]\n",
      " [202.57155]\n",
      " [129.60403]]\n"
     ]
    }
   ],
   "source": [
    "# 6. Multi-variable linear regression\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(123)  # for reproducibility\n",
    "\n",
    "x_data = [[73., 80., 75.],\n",
    "          [93., 88., 93.],\n",
    "          [89., 91., 90.],\n",
    "          [96., 98., 100.],\n",
    "          [73., 66., 70.]]\n",
    "y_data = [[152.],\n",
    "          [185.],\n",
    "          [180.],\n",
    "          [196.],\n",
    "          [142.]]\n",
    "\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(201):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 10 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score will be  [[143.77242]]\n",
      "Other scores will be  [[234.62083]\n",
      " [173.826  ]]\n"
     ]
    }
   ],
   "source": [
    "## ask my score \n",
    "\n",
    "print(\"Your score will be \", sess.run(\n",
    "    hypothesis, feed_dict={X: [[100, 75, 90]]}))\n",
    "\n",
    "print(\"Other scores will be \", sess.run(hypothesis,\n",
    "                                        feed_dict={X: [[60, 70, 110], [90, 100, 80]]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variable linear regression with file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.array([[1,2,3],[4,5,6]])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 3) [[ 73.  80.  75.]\n",
      " [ 93.  88.  93.]\n",
      " [ 89.  91.  90.]\n",
      " [ 96.  98. 100.]\n",
      " [ 73.  66.  70.]\n",
      " [ 53.  46.  55.]\n",
      " [ 69.  74.  77.]\n",
      " [ 47.  56.  60.]\n",
      " [ 87.  79.  90.]\n",
      " [ 79.  70.  88.]\n",
      " [ 69.  70.  73.]\n",
      " [ 70.  65.  74.]\n",
      " [ 93.  95.  91.]\n",
      " [ 79.  80.  73.]\n",
      " [ 70.  73.  78.]\n",
      " [ 93.  89.  96.]\n",
      " [ 78.  75.  68.]\n",
      " [ 81.  90.  93.]\n",
      " [ 88.  92.  86.]\n",
      " [ 78.  83.  77.]\n",
      " [ 82.  86.  90.]\n",
      " [ 86.  82.  89.]\n",
      " [ 78.  83.  85.]\n",
      " [ 76.  83.  71.]\n",
      " [ 96.  93.  95.]] 25\n",
      "(25, 1) [[152.]\n",
      " [185.]\n",
      " [180.]\n",
      " [196.]\n",
      " [142.]\n",
      " [101.]\n",
      " [149.]\n",
      " [115.]\n",
      " [175.]\n",
      " [164.]\n",
      " [141.]\n",
      " [141.]\n",
      " [184.]\n",
      " [152.]\n",
      " [148.]\n",
      " [192.]\n",
      " [147.]\n",
      " [183.]\n",
      " [177.]\n",
      " [159.]\n",
      " [177.]\n",
      " [175.]\n",
      " [175.]\n",
      " [149.]\n",
      " [192.]]\n",
      "0 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "100 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "200 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "300 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "400 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "500 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "600 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "700 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "800 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "900 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "1000 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "1100 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "1200 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "1300 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "1400 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "1500 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "1600 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "1700 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "1800 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "1900 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "2000 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "2100 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "2200 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "2300 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "2400 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "2500 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "2600 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "2700 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "2800 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "2900 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n",
      "3000 Cost:  9808.028 [[-0.66434914]\n",
      " [ 1.3075912 ]\n",
      " [ 0.17436366]] [-1.4149401]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(123)  # for reproducibility\n",
    "\n",
    "xy = np.loadtxt('c:/anaconda/pytfworks/data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "\n",
    "print(x_data.shape, x_data, len(x_data))\n",
    "print(y_data.shape, y_data)\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis for matrix \n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# hypothesis = X * W + b  # single layer\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.005)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(3001):\n",
    "    cost_val, W_val, b_val = sess.run(        \n",
    "        [cost, W, b], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 100 == 0:\n",
    "        print(step, \"Cost: \", cost_val,W_val, b_val)\n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variable linear regression with air-pax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 2) [[  1. 100.]\n",
      " [  2. 100.]\n",
      " [  3. 100.]\n",
      " [  4. 100.]\n",
      " [  5. 100.]\n",
      " [  6. 100.]\n",
      " [  7. 100.]\n",
      " [  8. 100.]\n",
      " [  9. 100.]\n",
      " [ 10. 100.]\n",
      " [ 11. 100.]\n",
      " [ 12. 100.]\n",
      " [ 13. 100.]\n",
      " [ 14. 100.]\n",
      " [ 15. 100.]\n",
      " [ 16. 100.]\n",
      " [ 17. 100.]\n",
      " [ 18. 100.]\n",
      " [ 19. 100.]\n",
      " [ 20. 100.]\n",
      " [ 21. 100.]\n",
      " [ 22. 100.]\n",
      " [ 23. 100.]\n",
      " [ 24. 100.]\n",
      " [ 25. 100.]\n",
      " [ 26. 100.]\n",
      " [ 27. 100.]\n",
      " [ 28. 100.]\n",
      " [ 29. 100.]\n",
      " [ 30. 100.]\n",
      " [ 31. 100.]\n",
      " [ 32. 100.]\n",
      " [ 33. 100.]\n",
      " [ 34. 100.]\n",
      " [ 35. 100.]\n",
      " [ 36. 100.]\n",
      " [ 37. 100.]\n",
      " [ 38. 100.]\n",
      " [ 39. 100.]\n",
      " [ 40. 100.]\n",
      " [ 41. 100.]\n",
      " [ 42. 100.]\n",
      " [ 43. 100.]\n",
      " [ 44. 100.]\n",
      " [ 45. 100.]\n",
      " [ 46. 100.]\n",
      " [ 47. 100.]\n",
      " [ 48. 100.]\n",
      " [ 49. 100.]\n",
      " [ 50. 100.]\n",
      " [ 51. 100.]\n",
      " [ 52. 100.]\n",
      " [ 53. 100.]\n",
      " [ 54. 100.]\n",
      " [ 55. 100.]\n",
      " [ 56. 100.]\n",
      " [ 57. 100.]\n",
      " [ 58. 100.]\n",
      " [ 59. 100.]\n",
      " [ 60. 100.]] 60\n",
      "(60, 1) [[0.13857678]\n",
      " [0.        ]\n",
      " [0.25842696]\n",
      " [0.15355805]\n",
      " [0.2434457 ]\n",
      " [0.4344569 ]\n",
      " [0.4494382 ]\n",
      " [0.49438202]\n",
      " [0.23595506]\n",
      " [0.21348314]\n",
      " [0.16479401]\n",
      " [0.3483146 ]\n",
      " [0.15730338]\n",
      " [0.11235955]\n",
      " [0.53932583]\n",
      " [0.2846442 ]\n",
      " [0.39700374]\n",
      " [0.6666667 ]\n",
      " [0.6329588 ]\n",
      " [0.72659177]\n",
      " [0.43820223]\n",
      " [0.3857678 ]\n",
      " [0.39700374]\n",
      " [0.5543071 ]\n",
      " [0.3483146 ]\n",
      " [0.19850187]\n",
      " [0.5131086 ]\n",
      " [0.37453184]\n",
      " [0.505618  ]\n",
      " [0.7490637 ]\n",
      " [0.7116105 ]\n",
      " [0.82397   ]\n",
      " [0.46441948]\n",
      " [0.4906367 ]\n",
      " [0.40449437]\n",
      " [0.505618  ]\n",
      " [0.35955057]\n",
      " [0.15730338]\n",
      " [0.6441948 ]\n",
      " [0.5355805 ]\n",
      " [0.6329588 ]\n",
      " [0.78651685]\n",
      " [0.8202247 ]\n",
      " [0.90636706]\n",
      " [0.56554306]\n",
      " [0.576779  ]\n",
      " [0.39325842]\n",
      " [0.8389513 ]\n",
      " [0.3895131 ]\n",
      " [0.26217228]\n",
      " [0.5917603 ]\n",
      " [0.47940075]\n",
      " [0.6254682 ]\n",
      " [0.83146065]\n",
      " [0.835206  ]\n",
      " [1.        ]\n",
      " [0.64044946]\n",
      " [0.5543071 ]\n",
      " [0.45692885]\n",
      " [0.7340824 ]]\n",
      "0 Cost:  2117.331 [-0.00356703 -0.64121395] [-0.45786333]\n",
      "100 Cost:  2117.331 [-0.00356703 -0.64121395] [-0.45786333]\n",
      "200 Cost:  2117.331 [-0.00356703 -0.64121395] [-0.45786333]\n",
      "300 Cost:  2117.331 [-0.00356703 -0.64121395] [-0.45786333]\n",
      "400 Cost:  2117.331 [-0.00356703 -0.64121395] [-0.45786333]\n",
      "500 Cost:  2117.331 [-0.00356703 -0.64121395] [-0.45786333]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(123)  # for reproducibility\n",
    "\n",
    "# xy = np.loadtxt('c:\\\\anaconda\\pytfworks\\data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "## air-pax2 , air-pax3 normalization\n",
    "xy = np.loadtxt('c:/anaconda/pytfworks/air-pax3.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "# x_data = xy[:, [0]]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "## air-pa.csv \n",
    "# xy = np.loadtxt('c:/anaconda/pytfworks/air-pax2.csv', delimiter=',', dtype=np.float32) \n",
    "# x_data = xy[:, 0:-1]\n",
    "# x_data = xy[:, [0]]\n",
    "#  y_data = xy[:, [-1]]\n",
    "\n",
    "\n",
    "# Make sure the shape and data are OK\n",
    "\n",
    "print(x_data.shape, x_data, len(x_data))\n",
    "print(y_data.shape, y_data)\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "# hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "hypothesis = X * W + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=8)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(501):\n",
    "    cost_val, W_val, b_val = sess.run(\n",
    "        [cost, W, b], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 100 == 0:\n",
    "        print(step, \"Cost: \", cost_val,W_val, b_val)\n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic : result depend on range ( 101 or 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9872174\n",
      "200 0.5150103\n",
      "400 0.4744432\n",
      "600 0.4482436\n",
      "800 0.42839512\n",
      "1000 0.41178045\n",
      "\n",
      "Hypothesis:  [[0.21039113]\n",
      " [0.25743848]\n",
      " [0.68897986]\n",
      " [0.62904453]\n",
      " [0.7894882 ]\n",
      " [0.9337667 ]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.8333333\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2],\n",
    "          [2, 3],\n",
    "          [3, 1],\n",
    "          [4, 3],\n",
    "          [5, 3],\n",
    "          [6, 2]]\n",
    "y_data = [[0],\n",
    "          [0],\n",
    "          [0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.1276329\n",
      "200 0.39806572\n",
      "400 0.35327193\n",
      "600 0.32879838\n",
      "800 0.3127677\n",
      "1000 0.30072278\n",
      "1200 0.29078642\n",
      "1400 0.28209135\n",
      "1600 0.27420577\n",
      "1800 0.26690054\n",
      "2000 0.26004642\n",
      "2200 0.25356564\n",
      "2400 0.2474084\n",
      "2600 0.24154043\n",
      "2800 0.23593648\n",
      "3000 0.23057638\n",
      "3200 0.2254438\n",
      "3400 0.22052455\n",
      "3600 0.21580581\n",
      "3800 0.21127613\n",
      "4000 0.20692526\n",
      "4200 0.2027434\n",
      "4400 0.19872145\n",
      "4600 0.19485106\n",
      "4800 0.1911244\n",
      "5000 0.18753408\n",
      "5200 0.1840731\n",
      "5400 0.18073516\n",
      "5600 0.17751403\n",
      "5800 0.17440413\n",
      "6000 0.1713999\n",
      "6200 0.16849633\n",
      "6400 0.16568872\n",
      "6600 0.16297264\n",
      "6800 0.1603438\n",
      "7000 0.1577981\n",
      "7200 0.15533195\n",
      "7400 0.15294172\n",
      "7600 0.15062414\n",
      "7800 0.14837588\n",
      "8000 0.14619416\n",
      "8200 0.144076\n",
      "8400 0.14201875\n",
      "8600 0.14002\n",
      "8800 0.13807718\n",
      "9000 0.13618807\n",
      "9200 0.13435058\n",
      "9400 0.1325626\n",
      "9600 0.13082221\n",
      "9800 0.12912749\n",
      "10000 0.12747683\n",
      "\n",
      "Hypothesis:  [[0.0220536 ]\n",
      " [0.14508364]\n",
      " [0.26011664]\n",
      " [0.8026452 ]\n",
      " [0.9521755 ]\n",
      " [0.98447305]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# sigmoid\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2],\n",
    "          [2, 3],\n",
    "          [3, 1],\n",
    "          [4, 3],\n",
    "          [5, 3],\n",
    "          [6, 2]]\n",
    "y_data = [[0],\n",
    "          [0],\n",
    "          [0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\anaconda\\envs\\py27\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "0 0.7507258 [array([[-0.98517185, -0.773219  ],\n",
      "       [ 0.21301189, -1.2062278 ]], dtype=float32), array([[-0.36349294],\n",
      "       [ 0.58081985]], dtype=float32)]\n",
      "100 0.6970954 [array([[-0.9791955 , -0.7451178 ],\n",
      "       [ 0.19668037, -1.1928732 ]], dtype=float32), array([[-0.38500795],\n",
      "       [ 0.4420594 ]], dtype=float32)]\n",
      "200 0.69603467 [array([[-0.9765449 , -0.7153862 ],\n",
      "       [ 0.17496823, -1.1771438 ]], dtype=float32), array([[-0.377329 ],\n",
      "       [ 0.3649061]], dtype=float32)]\n",
      "300 0.69540894 [array([[-0.97445893, -0.69052726],\n",
      "       [ 0.15382548, -1.1643192 ]], dtype=float32), array([[-0.36850664],\n",
      "       [ 0.29602244]], dtype=float32)]\n",
      "400 0.6948705 [array([[-0.972685  , -0.67066973],\n",
      "       [ 0.13359582, -1.1543665 ]], dtype=float32), array([[-0.36063594],\n",
      "       [ 0.23063347]], dtype=float32)]\n",
      "500 0.69439554 [array([[-0.971179  , -0.65562975],\n",
      "       [ 0.11419772, -1.1470145 ]], dtype=float32), array([[-0.35380146],\n",
      "       [ 0.16777101]], dtype=float32)]\n",
      "600 0.69396627 [array([[-0.9699156 , -0.6452509 ],\n",
      "       [ 0.09553225, -1.1420414 ]], dtype=float32), array([[-0.34794486],\n",
      "       [ 0.10681962]], dtype=float32)]\n",
      "700 0.6935678 [array([[-0.9688739 , -0.6394172 ],\n",
      "       [ 0.07751082, -1.1392868 ]], dtype=float32), array([[-0.34300485],\n",
      "       [ 0.04723623]], dtype=float32)]\n",
      "800 0.6931871 [array([[-0.96803606, -0.6380487 ],\n",
      "       [ 0.06005512, -1.1386459 ]], dtype=float32), array([[-0.33892477],\n",
      "       [-0.01148382]], dtype=float32)]\n",
      "900 0.6928122 [array([[-0.96738726, -0.64109546],\n",
      "       [ 0.04309594, -1.1400659 ]], dtype=float32), array([[-0.3356539 ],\n",
      "       [-0.06981992]], dtype=float32)]\n",
      "1000 0.69243175 [array([[-0.9669147 , -0.6485359 ],\n",
      "       [ 0.02657171, -1.1435437 ]], dtype=float32), array([[-0.3331452],\n",
      "       [-0.1282352]], dtype=float32)]\n",
      "1100 0.6920344 [array([[-0.96660733, -0.66037494],\n",
      "       [ 0.01042771, -1.1491235 ]], dtype=float32), array([[-0.33135456],\n",
      "       [-0.18718486]], dtype=float32)]\n",
      "1200 0.69160825 [array([[-0.9664559 , -0.67664295],\n",
      "       [-0.00538484, -1.1568974 ]], dtype=float32), array([[-0.33023983],\n",
      "       [-0.2471227 ]], dtype=float32)]\n",
      "1300 0.6911407 [array([[-0.9664518, -0.6973978],\n",
      "       [-0.0209092, -1.1670043]], dtype=float32), array([[-0.32976022],\n",
      "       [-0.30850896]], dtype=float32)]\n",
      "1400 0.6906174 [array([[-0.96658826, -0.7227256 ],\n",
      "       [-0.03618363, -1.1796331 ]], dtype=float32), array([[-0.32987508],\n",
      "       [-0.3718164 ]], dtype=float32)]\n",
      "1500 0.6900215 [array([[-0.9668588 , -0.75274366],\n",
      "       [-0.05124167, -1.1950227 ]], dtype=float32), array([[-0.33054286],\n",
      "       [-0.43753877]], dtype=float32)]\n",
      "1600 0.68933344 [array([[-0.9672574 , -0.7876066 ],\n",
      "       [-0.06611223, -1.2134652 ]], dtype=float32), array([[-0.33172047],\n",
      "       [-0.50619876]], dtype=float32)]\n",
      "1700 0.6885293 [array([[-0.9677786 , -0.82751036],\n",
      "       [-0.08081964, -1.2353127 ]], dtype=float32), array([[-0.33336118],\n",
      "       [-0.57835716]], dtype=float32)]\n",
      "1800 0.6875793 [array([[-0.9684163 , -0.8727016 ],\n",
      "       [-0.09538344, -1.2609793 ]], dtype=float32), array([[-0.33541363],\n",
      "       [-0.65462416]], dtype=float32)]\n",
      "1900 0.6864462 [array([[-0.96916425, -0.9234878 ],\n",
      "       [-0.10981815, -1.2909544 ]], dtype=float32), array([[-0.33781946],\n",
      "       [-0.7356707 ]], dtype=float32)]\n",
      "2000 0.6850824 [array([[-0.9700152 , -0.98024917],\n",
      "       [-0.12413266, -1.3258089 ]], dtype=float32), array([[-0.34051043],\n",
      "       [-0.8222405 ]], dtype=float32)]\n",
      "2100 0.68342733 [array([[-0.9709602 , -1.0434544 ],\n",
      "       [-0.13832942, -1.3662114 ]], dtype=float32), array([[-0.34340578],\n",
      "       [-0.9151628 ]], dtype=float32)]\n",
      "2200 0.681403 [array([[-0.97198796, -1.1136781 ],\n",
      "       [-0.15240341, -1.4129422 ]], dtype=float32), array([[-0.34640697],\n",
      "       [-1.0153587 ]], dtype=float32)]\n",
      "2300 0.6789101 [array([[-0.9730845, -1.1916176],\n",
      "       [-0.1663405, -1.4669107]], dtype=float32), array([[-0.34939262],\n",
      "       [-1.1238408 ]], dtype=float32)]\n",
      "2400 0.6758233 [array([[-0.97423095, -1.2781036 ],\n",
      "       [-0.18011604, -1.5291715 ]], dtype=float32), array([[-0.35221195],\n",
      "       [-1.2416941 ]], dtype=float32)]\n",
      "2500 0.6719909 [array([[-0.97540283, -1.3740994 ],\n",
      "       [-0.19369183, -1.6009237 ]], dtype=float32), array([[-0.35467753],\n",
      "       [-1.3700176 ]], dtype=float32)]\n",
      "2600 0.66723937 [array([[-0.9765699 , -1.4806662 ],\n",
      "       [-0.20701417, -1.683488  ]], dtype=float32), array([[-0.35655937],\n",
      "       [-1.5098177 ]], dtype=float32)]\n",
      "2700 0.6613921 [array([[-0.9776927 , -1.5988665 ],\n",
      "       [-0.22001158, -1.7782179 ]], dtype=float32), array([[-0.35758278],\n",
      "       [-1.661812  ]], dtype=float32)]\n",
      "2800 0.65430623 [array([[-0.9787242 , -1.7295746 ],\n",
      "       [-0.23259448, -1.8863206 ]], dtype=float32), array([[-0.35743776],\n",
      "       [-1.8261657 ]], dtype=float32)]\n",
      "2900 0.6459274 [array([[-0.97961134, -1.8731743 ],\n",
      "       [-0.24465774, -2.0085542 ]], dtype=float32), array([[-0.35580108],\n",
      "       [-2.0021904 ]], dtype=float32)]\n",
      "3000 0.6363429 [array([[-0.9803004 , -2.029193  ],\n",
      "       [-0.25608876, -2.1448293 ]], dtype=float32), array([[-0.35237658],\n",
      "       [-2.1881375 ]], dtype=float32)]\n",
      "3100 0.62580264 [array([[-0.98074496, -2.1960156 ],\n",
      "       [-0.26678088, -2.293881  ]], dtype=float32), array([[-0.34693953],\n",
      "       [-2.3812327 ]], dtype=float32)]\n",
      "3200 0.61468595 [array([[-0.9809142, -2.3708766],\n",
      "       [-0.2766485, -2.4531806]], dtype=float32), array([[-0.3393692],\n",
      "       [-2.5780075]], dtype=float32)]\n",
      "3300 0.6034278 [array([[-0.98079926, -2.5502167 ],\n",
      "       [-0.28563878, -2.6192517 ]], dtype=float32), array([[-0.32965413],\n",
      "       [-2.7747986 ]], dtype=float32)]\n",
      "3400 0.5924353 [array([[-0.9804142 , -2.7302983 ],\n",
      "       [-0.29373717, -2.788268  ]], dtype=float32), array([[-0.3178752],\n",
      "       [-2.9682412]], dtype=float32)]\n",
      "3500 0.582029 [array([[-0.97979116, -2.9077704 ],\n",
      "       [-0.30096245, -2.95666   ]], dtype=float32), array([[-0.3041766],\n",
      "       [-3.1556094]], dtype=float32)]\n",
      "3600 0.57241786 [array([[-0.9789738, -3.0800278],\n",
      "       [-0.3073573, -3.1215405]], dtype=float32), array([[-0.28873852],\n",
      "       [-3.334946  ]], dtype=float32)]\n",
      "3700 0.56370366 [array([[-0.97801167, -3.24531   ],\n",
      "       [-0.31297955, -3.2808504 ]], dtype=float32), array([[-0.2717548],\n",
      "       [-3.5050578]], dtype=float32)]\n",
      "3800 0.5559044 [array([[-0.97695345, -3.4026053 ],\n",
      "       [-0.31789306, -3.4333093 ]], dtype=float32), array([[-0.25341678],\n",
      "       [-3.6653984 ]], dtype=float32)]\n",
      "3900 0.54898095 [array([[-0.97584534, -3.5514944 ],\n",
      "       [-0.32216203, -3.5782585 ]], dtype=float32), array([[-0.2339026],\n",
      "       [-3.8159106]], dtype=float32)]\n",
      "4000 0.542863 [array([[-0.97472775, -3.6919694 ],\n",
      "       [-0.32584578, -3.715501  ]], dtype=float32), array([[-0.21337287],\n",
      "       [-3.9568775 ]], dtype=float32)]\n",
      "4100 0.5374662 [array([[-0.97363734, -3.82429   ],\n",
      "       [-0.32899892, -3.8451455 ]], dtype=float32), array([[-0.1919674],\n",
      "       [-4.088788 ]], dtype=float32)]\n",
      "4200 0.5327042 [array([[-0.972604 , -3.94887  ],\n",
      "       [-0.331669 , -3.9674852]], dtype=float32), array([[-0.16980545],\n",
      "       [-4.212241  ]], dtype=float32)]\n",
      "4300 0.52849466 [array([[-0.9716535, -4.066202 ],\n",
      "       [-0.3338975, -4.0829253]], dtype=float32), array([[-0.14698751],\n",
      "       [-4.3278823 ]], dtype=float32)]\n",
      "4400 0.52476364 [array([[-0.97080773, -4.17681   ],\n",
      "       [-0.33571953, -4.191915  ]], dtype=float32), array([[-0.12359653],\n",
      "       [-4.4363513 ]], dtype=float32)]\n",
      "4500 0.5214457 [array([[-0.9700845 , -4.2812076 ],\n",
      "       [-0.33716485, -4.294915  ]], dtype=float32), array([[-0.09970072],\n",
      "       [-4.5382643 ]], dtype=float32)]\n",
      "4600 0.518484 [array([[-0.96949977, -4.3798914 ],\n",
      "       [-0.33825853, -4.39238   ]], dtype=float32), array([[-0.07535513],\n",
      "       [-4.6341977 ]], dtype=float32)]\n",
      "4700 0.5158298 [array([[-0.9690676, -4.4733233],\n",
      "       [-0.3390211, -4.484745 ]], dtype=float32), array([[-0.05060372],\n",
      "       [-4.724678  ]], dtype=float32)]\n",
      "4800 0.513442 [array([[-0.96879965, -4.5619364 ],\n",
      "       [-0.3394695 , -4.572402  ]], dtype=float32), array([[-0.02548096],\n",
      "       [-4.8101845 ]], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4900 0.5112847 [array([[-0.9687073 , -4.64612   ],\n",
      "       [-0.33961764, -4.6557274 ]], dtype=float32), array([[-1.3191195e-05],\n",
      "       [-4.8911552e+00]], dtype=float32)]\n",
      "5000 0.509328 [array([[-0.96880084, -4.7262325 ],\n",
      "       [-0.33947587, -4.7350607 ]], dtype=float32), array([[ 0.02578022],\n",
      "       [-4.9679775 ]], dtype=float32)]\n",
      "5100 0.5075463 [array([[-0.9690903 , -4.802595  ],\n",
      "       [-0.33905262, -4.810712  ]], dtype=float32), array([[ 0.05188588],\n",
      "       [-5.0410013 ]], dtype=float32)]\n",
      "5200 0.5059178 [array([[-0.9695852, -4.8754992],\n",
      "       [-0.3383534, -4.8829627]], dtype=float32), array([[ 0.07829563],\n",
      "       [-5.110541  ]], dtype=float32)]\n",
      "5300 0.5044235 [array([[-0.9702954 , -4.9452105 ],\n",
      "       [-0.33738175, -4.9520674 ]], dtype=float32), array([[ 0.10500592],\n",
      "       [-5.1768765 ]], dtype=float32)]\n",
      "5400 0.5030474 [array([[-0.97123134, -5.0119658 ],\n",
      "       [-0.336139  , -5.018257  ]], dtype=float32), array([[ 0.13201733],\n",
      "       [-5.240262  ]], dtype=float32)]\n",
      "5500 0.5017757 [array([[-0.9724035 , -5.075983  ],\n",
      "       [-0.33462456, -5.0817413 ]], dtype=float32), array([[ 0.15933411],\n",
      "       [-5.300924  ]], dtype=float32)]\n",
      "5600 0.5005961 [array([[-0.97382325, -5.13746   ],\n",
      "       [-0.33283576, -5.1427126 ]], dtype=float32), array([[ 0.18696393],\n",
      "       [-5.3590636 ]], dtype=float32)]\n",
      "5700 0.49949843 [array([[-0.9755029 , -5.196569  ],\n",
      "       [-0.33076775, -5.2013383 ]], dtype=float32), array([[ 0.21491773],\n",
      "       [-5.4148717 ]], dtype=float32)]\n",
      "5800 0.49847326 [array([[-0.97745574, -5.253473  ],\n",
      "       [-0.32841384, -5.257779  ]], dtype=float32), array([[ 0.24320954],\n",
      "       [-5.46851   ]], dtype=float32)]\n",
      "5900 0.49751258 [array([[-0.97969705, -5.3083177 ],\n",
      "       [-0.3257653 , -5.3121777 ]], dtype=float32), array([[ 0.27185652],\n",
      "       [-5.520132  ]], dtype=float32)]\n",
      "6000 0.49660936 [array([[-0.9822417 , -5.3612385 ],\n",
      "       [-0.32281053, -5.3646636 ]], dtype=float32), array([[ 0.30087885],\n",
      "       [-5.569873  ]], dtype=float32)]\n",
      "6100 0.495757 [array([[-0.9851083, -5.412355 ],\n",
      "       [-0.3195361, -5.415355 ]], dtype=float32), array([[ 0.33029982],\n",
      "       [-5.6178575 ]], dtype=float32)]\n",
      "6200 0.49494994 [array([[-0.9883168 , -5.4617796 ],\n",
      "       [-0.31592566, -5.464362  ]], dtype=float32), array([[ 0.36014596],\n",
      "       [-5.6642    ]], dtype=float32)]\n",
      "6300 0.494183 [array([[-0.99188906, -5.509614  ],\n",
      "       [-0.3119601 , -5.5117846 ]], dtype=float32), array([[ 0.39044744],\n",
      "       [-5.7090034 ]], dtype=float32)]\n",
      "6400 0.49345127 [array([[-0.9958503 , -5.555954  ],\n",
      "       [-0.30761677, -5.5577135 ]], dtype=float32), array([[ 0.42123806],\n",
      "       [-5.7523637 ]], dtype=float32)]\n",
      "6500 0.49275053 [array([[-1.0002285 , -5.600887  ],\n",
      "       [-0.30286962, -5.602233  ]], dtype=float32), array([[ 0.45255587],\n",
      "       [-5.794368  ]], dtype=float32)]\n",
      "6600 0.49207684 [array([[-1.0050552 , -5.64449   ],\n",
      "       [-0.29768828, -5.64542   ]], dtype=float32), array([[ 0.48444325],\n",
      "       [-5.8350964 ]], dtype=float32)]\n",
      "6700 0.49142593 [array([[-1.0103667 , -5.686839  ],\n",
      "       [-0.29203755, -5.6873465 ]], dtype=float32), array([[ 0.5169482],\n",
      "       [-5.8746223]], dtype=float32)]\n",
      "6800 0.4907943 [array([[-1.0162039 , -5.7280006 ],\n",
      "       [-0.28587678, -5.7280774 ]], dtype=float32), array([[ 0.55012405],\n",
      "       [-5.9130173 ]], dtype=float32)]\n",
      "6900 0.4901781 [array([[-1.0226132 , -5.7680383 ],\n",
      "       [-0.27915898, -5.7676764 ]], dtype=float32), array([[ 0.5840309],\n",
      "       [-5.9503436]], dtype=float32)]\n",
      "7000 0.48957378 [array([[-1.0296487 , -5.807012  ],\n",
      "       [-0.27182955, -5.806196  ]], dtype=float32), array([[ 0.61873657],\n",
      "       [-5.9866614 ]], dtype=float32)]\n",
      "7100 0.48897737 [array([[-1.037372  , -5.8449755 ],\n",
      "       [-0.26382506, -5.8436894 ]], dtype=float32), array([[ 0.6543178],\n",
      "       [-6.022025 ]], dtype=float32)]\n",
      "7200 0.48838487 [array([[-1.0458548, -5.8819814],\n",
      "       [-0.2550717, -5.880203 ]], dtype=float32), array([[ 0.6908619],\n",
      "       [-6.0564866]], dtype=float32)]\n",
      "7300 0.4877921 [array([[-1.0551807, -5.9180784],\n",
      "       [-0.245483 , -5.9157834]], dtype=float32), array([[ 0.7284683],\n",
      "       [-6.0900946]], dtype=float32)]\n",
      "7400 0.48719394 [array([[-1.0654471 , -5.9533114 ],\n",
      "       [-0.23495774, -5.9504695 ]], dtype=float32), array([[ 0.76725066],\n",
      "       [-6.1228952 ]], dtype=float32)]\n",
      "7500 0.486585 [array([[-1.07677   , -5.9877243 ],\n",
      "       [-0.22337633, -5.9843    ]], dtype=float32), array([[ 0.8073415],\n",
      "       [-6.154933 ]], dtype=float32)]\n",
      "7600 0.4859589 [array([[-1.0892864 , -6.021357  ],\n",
      "       [-0.21059743, -6.0173073 ]], dtype=float32), array([[ 0.84889305],\n",
      "       [-6.1862493 ]], dtype=float32)]\n",
      "7700 0.4853081 [array([[-1.1031611, -6.054249 ],\n",
      "       [-0.1964527, -6.049524 ]], dtype=float32), array([[ 0.8920844],\n",
      "       [-6.216884 ]], dtype=float32)]\n",
      "7800 0.48462328 [array([[-1.1185918, -6.0864387],\n",
      "       [-0.180741 , -6.0809765]], dtype=float32), array([[ 0.9371261],\n",
      "       [-6.2468767]], dtype=float32)]\n",
      "7900 0.48389262 [array([[-1.1358194 , -6.117961  ],\n",
      "       [-0.16322105, -6.111691  ]], dtype=float32), array([[ 0.98426723],\n",
      "       [-6.276264  ]], dtype=float32)]\n",
      "8000 0.48310176 [array([[-1.1551405 , -6.1488523 ],\n",
      "       [-0.14360192, -6.1416864 ]], dtype=float32), array([[ 1.0338057],\n",
      "       [-6.305086 ]], dtype=float32)]\n",
      "8100 0.4822316 [array([[-1.1769221, -6.179148 ],\n",
      "       [-0.1215319, -6.17098  ]], dtype=float32), array([[ 1.0861003],\n",
      "       [-6.3333797]], dtype=float32)]\n",
      "8200 0.48125702 [array([[-1.2016251 , -6.208885  ],\n",
      "       [-0.09658606, -6.1995835 ]], dtype=float32), array([[ 1.1415867],\n",
      "       [-6.361186 ]], dtype=float32)]\n",
      "8300 0.480145 [array([[-1.229832 , -6.238101 ],\n",
      "       [-0.0682529, -6.2275023]], dtype=float32), array([[ 1.200793 ],\n",
      "       [-6.3885446]], dtype=float32)]\n",
      "8400 0.47885066 [array([[-1.2622895 , -6.2668343 ],\n",
      "       [-0.03592394, -6.254735  ]], dtype=float32), array([[ 1.2643666],\n",
      "       [-6.415502 ]], dtype=float32)]\n",
      "8500 0.477313 [array([[-1.2999574e+00, -6.2951312e+00],\n",
      "       [ 1.1061163e-03, -6.2812676e+00]], dtype=float32), array([[ 1.3330992],\n",
      "       [-6.4421067]], dtype=float32)]\n",
      "8600 0.47544998 [array([[-1.3440765 , -6.3230414 ],\n",
      "       [ 0.04361095, -6.307078  ]], dtype=float32), array([[ 1.4079455],\n",
      "       [-6.468416 ]], dtype=float32)]\n",
      "8700 0.473152 [array([[-1.3962463 , -6.350623  ],\n",
      "       [ 0.09234151, -6.332119  ]], dtype=float32), array([[ 1.4900306],\n",
      "       [-6.4944987]], dtype=float32)]\n",
      "8800 0.4702826 [array([[-1.4584832 , -6.377948  ],\n",
      "       [ 0.14781111, -6.3563256 ]], dtype=float32), array([[ 1.580604 ],\n",
      "       [-6.5204334]], dtype=float32)]\n",
      "8900 0.4666876 [array([[-1.5332159 , -6.4051094 ],\n",
      "       [ 0.20987731, -6.379602  ]], dtype=float32), array([[ 1.6808995],\n",
      "       [-6.5463257]], dtype=float32)]\n",
      "9000 0.46223503 [array([[-1.6230696, -6.432204 ],\n",
      "       [ 0.277098 , -6.401824 ]], dtype=float32), array([[ 1.7918309],\n",
      "       [-6.572288 ]], dtype=float32)]\n",
      "9100 0.45688272 [array([[-1.7302911 , -6.459384  ],\n",
      "       [ 0.34610304, -6.4228535 ]], dtype=float32), array([[ 1.9135371],\n",
      "       [-6.5984674]], dtype=float32)]\n",
      "9200 0.4507457 [array([[-1.8557497 , -6.486783  ],\n",
      "       [ 0.41167274, -6.442565  ]], dtype=float32), array([[ 2.044978],\n",
      "       [-6.62503 ]], dtype=float32)]\n",
      "9300 0.44409156 [array([[-1.997953  , -6.5145316 ],\n",
      "       [ 0.46816438, -6.460889  ]], dtype=float32), array([[ 2.1839216],\n",
      "       [-6.652163 ]], dtype=float32)]\n",
      "9400 0.43725032 [array([[-2.1528878 , -6.5427294 ],\n",
      "       [ 0.51166505, -6.477825  ]], dtype=float32), array([[ 2.3274393],\n",
      "       [-6.680072 ]], dtype=float32)]\n",
      "9500 0.43051666 [array([[-2.315017  , -6.5714264 ],\n",
      "       [ 0.54114026, -6.4934387 ]], dtype=float32), array([[ 2.4725125],\n",
      "       [-6.708975 ]], dtype=float32)]\n",
      "9600 0.42410955 [array([[-2.4787579, -6.6006308],\n",
      "       [ 0.5579031, -6.507837 ]], dtype=float32), array([[ 2.6164265],\n",
      "       [-6.7390857]], dtype=float32)]\n",
      "9700 0.41816935 [array([[-2.6395395 , -6.630331  ],\n",
      "       [ 0.56434923, -6.5211425 ]], dtype=float32), array([[ 2.756949],\n",
      "       [-6.770577]], dtype=float32)]\n",
      "9800 0.41276878 [array([[-2.794166 , -6.6605015],\n",
      "       [ 0.5629781, -6.533484 ]], dtype=float32), array([[ 2.8923926],\n",
      "       [-6.803567 ]], dtype=float32)]\n",
      "9900 0.4079278 [array([[-2.9407034, -6.6911225],\n",
      "       [ 0.5559347, -6.5449824]], dtype=float32), array([[ 3.0216167],\n",
      "       [-6.838108 ]], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 0.40362865 [array([[-3.0781984 , -6.7221704 ],\n",
      "       [ 0.54489946, -6.555754  ]], dtype=float32), array([[ 3.143964],\n",
      "       [-6.87419 ]], dtype=float32)]\n",
      "weight - bias : [[-3.0781984  -6.7221704 ]\n",
      " [ 0.54489946 -6.555754  ]] [-0.19117634  1.2091676 ] [[ 3.143964]\n",
      " [-6.87419 ]] [0.09154845] \n",
      "Hypothesis:  [[0.0223037]\n",
      " [0.8705797]\n",
      " [0.5446593]\n",
      " [0.5707723]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "# Lab 9 XOR\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(123)  # for reproducibility\n",
    "learning_rate = 0.1\n",
    "\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={\n",
    "                  X: x_data, Y: y_data}), sess.run([W1, W2]))\n",
    "\n",
    "    # Accuracy report\n",
    "    w1, b1, w2, b2, h, c, a = sess.run([W1, b1, W2, b2, hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"weight - bias :\", w1, b1, w2, b2, \"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "w1 = ([[4.4,  6.6 ], \n",
    "       [4.4, 6.8 ]]) \n",
    "b1 = [-6.7,  -2.9]\n",
    "\n",
    "w2 = [[-10.3 ], \n",
    "      [ 9.5 ]]\n",
    "b2 = [-4.3 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess=tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00122935, 0.0521535 ],\n",
       "       [0.09112296, 0.9801597 ],\n",
       "       [0.09112296, 0.975873  ],\n",
       "       [0.89090323, 0.9999725 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = tf.sigmoid(tf.matmul(x_data, w1) + b1) \n",
    "# t1 = tf.matmul(x_data, w1) + b1 \n",
    "sess.run(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.8172042],\n",
       "       [ 4.0729504],\n",
       "       [ 4.0322275],\n",
       "       [-3.9765654]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t1 = [[0, 0],   [0, 1],    [0, 1],   [1, 1]]\n",
    "# t1 = np.array(t1, dtype=np.float32)\n",
    "t2 = tf.matmul( t1, w2) + b2\n",
    "sess.run(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "0 8.526441\n",
      "200 0.61032116\n",
      "400 0.5015863\n",
      "600 0.40680403\n",
      "800 0.3149947\n",
      "1000 0.24200627\n",
      "1200 0.21847081\n",
      "1400 0.1993032\n",
      "1600 0.18313006\n",
      "1800 0.16930401\n",
      "2000 0.15735441\n",
      "--------------\n",
      "[[1.0471681e-02 9.8951811e-01 1.0268356e-05]] [1]\n",
      "--------------\n",
      "[[0.79670674 0.18129857 0.02199463]] [0]\n",
      "--------------\n",
      "[[1.3321004e-08 3.6355868e-04 9.9963641e-01]] [2]\n",
      "--------------\n",
      "[[1.0471681e-02 9.8951811e-01 1.0268356e-05]\n",
      " [7.9670674e-01 1.8129857e-01 2.1994630e-02]\n",
      " [1.3321004e-08 3.6355868e-04 9.9963641e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Lab 6 Softmax Classifier\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(123)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "            _, cost_val = sess.run([optimizer, cost], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "            if step % 200 == 0:\n",
    "                print(step, cost_val)\n",
    "\n",
    "    print('--------------')\n",
    "    # Testing & One-hot encoding\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n",
    "    print(a, sess.run(tf.argmax(a, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n",
    "    print(b, sess.run(tf.argmax(b, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n",
    "    print(c, sess.run(tf.argmax(c, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n",
    "    print(all, sess.run(tf.argmax(all, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one_hot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "y=tf.one_hot([[0],[1],[2]], depth=4)\n",
    "sess = tf.Session()\n",
    "x = sess.run(y)\n",
    "print(x)  # instead .eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot_17:0\", shape=(2, 5), dtype=float32)\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "Y=[1, 3]\n",
    "y_one_hot=tf.one_hot(Y, 5)\n",
    "print(y_one_hot)\n",
    "\n",
    "sess = tf.Session()\n",
    "result = sess.run(y_one_hot)\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.  , 0.  ],\n",
       "       [0.25, 0.5 , 0.5 ],\n",
       "       [0.  , 1.  , 1.  ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = np.array([[ 20., -10., 1.],\n",
    "              [ 5., 0., 2.],\n",
    "              [ 0., 10., 3.]])\n",
    "X_MinMax = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) \n",
    "X_MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.  , 0.  ],\n",
       "       [0.25, 0.5 , 0.5 ],\n",
       "       [0.  , 1.  , 1.  ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min_max_scaler training using the transformer API\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "X_MinMax_train = min_max_scaler.fit_transform(X)\n",
    "\n",
    "X_MinMax_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        ],\n",
       "       [0.33333333, 0.2       ],\n",
       "       [0.66666667, 0.6       ],\n",
       "       [1.        , 1.        ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[1000, 1], [2000, 2], [3000, 4], [4000, 6]]\n",
    "X_MinMax_train = min_max_scaler.fit_transform(x)\n",
    "\n",
    "X_MinMax_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True,\n",
      "       feature_range=[[1000, 1], [2000, 2], [3000, 4], [4000, 6]])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    " \n",
    "x = [[1000, 1], [2000, 2], [3000, 4], [4000, 6]]\n",
    "# xy =sklearn.preprocessing.MinMaxScaler(x)\n",
    "xy =MinMaxScaler(x)\n",
    "print(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 1)\n",
      "one_hot Tensor(\"one_hot:0\", shape=(?, 1, 7), dtype=float32)\n",
      "reshape Tensor(\"Reshape:0\", shape=(?, 7), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-16-6d50ebf57cc2>:35: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Step:     0\tLoss: 4.340\tAcc: 16.83%\n",
      "Step:   100\tLoss: 0.681\tAcc: 80.20%\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 3 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 6 True Y: 4\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[False] Prediction: 6 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 6 True Y: 4\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 3 True Y: 2\n",
      "[False] Prediction: 1 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[False] Prediction: 3 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[False] Prediction: 0 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[False] Prediction: 3 True Y: 2\n",
      "[False] Prediction: 4 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 1 True Y: 2\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[False] Prediction: 4 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n"
     ]
    }
   ],
   "source": [
    "# 11. Softmax Classifier\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(123)  # for reproducibility\n",
    "\n",
    "# Predicting animal type based on various features\n",
    "xy = np.loadtxt('c:\\\\anaconda\\pytfworks\\data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "nb_classes = 7  # 0 ~ 6\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 16])\n",
    "Y = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 6\n",
    "\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
    "\n",
    "print(\"one_hot\", Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "print(\"reshape\", Y_one_hot)\n",
    "\n",
    "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                 labels=Y_one_hot)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(200):\n",
    "        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={\n",
    "                                 X: x_data, Y: y_data})\n",
    "            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(\n",
    "                step, loss, acc))\n",
    "\n",
    "    # Let's see if we can predict\n",
    "    pred = sess.run(prediction, feed_dict={X: x_data})\n",
    "    # y_data: (N,1) = flatten => (N, ) matches pred.shape\n",
    "    for p, y in zip(pred, y_data.flatten()):\n",
    "        print(\"[{}] Prediction: {} True Y: {}\".format(p == int(y), p, int(y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [3.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [3.],\n",
       "       [3.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [3.],\n",
       "       [6.],\n",
       "       [6.],\n",
       "       [6.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [3.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [5.],\n",
       "       [4.],\n",
       "       [4.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [5.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [3.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [3.],\n",
       "       [5.],\n",
       "       [5.],\n",
       "       [1.],\n",
       "       [5.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [6.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [5.],\n",
       "       [4.],\n",
       "       [6.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [3.],\n",
       "       [3.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [6.],\n",
       "       [3.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [6.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [6.],\n",
       "       [3.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [6.],\n",
       "       [3.],\n",
       "       [1.],\n",
       "       [5.],\n",
       "       [4.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [5.],\n",
       "       [0.],\n",
       "       [6.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural network with MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cd95510860>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "# %matplotlib inline # Only use this if using iPython\n",
    "\n",
    "image_index = 401 # You may select anything up to 60,000\n",
    "\n",
    "print(y_train[image_index]) # The label is 6\n",
    "plt.imshow(x_train[image_index], cmap='Greys')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  63, 255,  84,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0, 138, 253,  84,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  67, 246, 231,  19,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  26, 159, 252, 175,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          4, 179, 253, 253,  51,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        128, 252, 252, 151,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,\n",
       "        253, 252, 186,   6,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38, 175,\n",
       "        253, 227,  43,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   7, 154, 253,\n",
       "        251,  75,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 131, 252, 252,\n",
       "        125,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  10, 197, 252, 252,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  79, 252, 252, 102,\n",
       "          0,   0,   0,   0,  13, 188, 187,  13,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 141, 253, 206,  13,\n",
       "          0,   0,  10,  79, 254, 253, 253, 153,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 216, 252, 168,   0,\n",
       "          0,   0,  85, 252, 253, 227, 252, 177,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 253, 252, 168,   0,\n",
       "          0,   0, 172, 252, 194, 162, 252, 103,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 203, 252, 205,  13,\n",
       "          0, 101, 246, 252, 138, 243, 214,  15,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  76, 244, 253, 253,\n",
       "        141, 216, 253, 253, 254, 253, 156,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 131, 240, 252,\n",
       "        253, 252, 252, 252, 253, 189,  19,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81, 168,\n",
       "        168, 224, 252, 252, 106,  19,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  19, 153, 252,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = x_train[image_index]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ([[  1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   10,   10,   10,\n",
    "          255,   255,   255,   255,   255,   255,   255,   255,   255,   255,   255,   255,   255,\n",
    "          255,   255],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0,   0,  63, 255,  84,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0,   0, 138, 253,  84,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0,  67, 246, 231,  19,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,  26, 159, 252, 175,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          4, 179, 253, 253,  51,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "        128, 252, 252, 151,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,\n",
    "        253, 252, 186,   6,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38, 175,\n",
    "        253, 227,  43,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   7, 154, 253,\n",
    "        251,  75,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 131, 252, 252,\n",
    "        125,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  10, 197, 252, 252,\n",
    "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  79, 252, 252, 102,\n",
    "          0,   0,   0,   0,  13, 188, 187,  13,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 141, 253, 206,  13,\n",
    "          0,   0,  10,  79, 254, 253, 253, 153,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 216, 252, 168,   0,\n",
    "          0,   0,  85, 252, 253, 227, 252, 177,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 253, 252, 168,   0,\n",
    "          0,   0, 172, 252, 194, 162, 252, 103,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 203, 252, 205,  13,\n",
    "          0, 101, 246, 252, 138, 243, 214,  15,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  76, 244, 253, 253,\n",
    "        141, 216, 253, 253, 254, 253, 156,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 131, 240, 252,\n",
    "        253, 252, 252, 252, 253, 189,  19,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81, 168,\n",
    "        168, 224, 252, 252, 106,  19,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,  19, 153, 252,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0],\n",
    "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cd9570bb38>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(b, cmap='Greys')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cd95783400>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# sess = tf.InteractiveSession()\n",
    "\n",
    "image = np.array([[[[1], [2], [3]], [[4], [5], [6]], [[7], [8], [20]]]], dtype = np.float32)\n",
    "\n",
    "print(image.shape)\n",
    "# plt.imshow(image.reshape(3,3), cmap=Greys) # error Greys\n",
    "\n",
    "plt.imshow(image.reshape(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-3af1843e8e5a>:11: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-19-3af1843e8e5a>:39: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0001 cost = 158.619334755\n",
      "Epoch: 0002 cost = 39.091352374\n",
      "Epoch: 0003 cost = 24.419578648\n",
      "Epoch: 0004 cost = 16.995490589\n",
      "Epoch: 0005 cost = 12.286527738\n",
      "Epoch: 0006 cost = 9.140790272\n",
      "Learning Finished!\n",
      "Accuracy: 0.9285\n",
      "Label:  [9]\n",
      "Prediction:  [9]\n"
     ]
    }
   ],
   "source": [
    "# Lab 10 MNIST and NN\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(123)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 6    # Accuracy: 0.9305  # epochs 15, Accuracy: 0.9455, Epoch: 0006 cost = 8.573139748\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Learning started. It takes sometime.\n",
      "Epoch: 0001 cost = 0.383841882\n",
      "Learning Finished!\n",
      "Accuracy: 0.9634\n",
      "Label:  [6]\n",
      "Prediction:  [6]\n"
     ]
    }
   ],
   "source": [
    "## it takes too much time \n",
    "\n",
    "# Lab 11 MNIST and Convolutional Neural Network\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(123)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 1  # 15  too much time\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# L1 ImgIn shape=(?, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "#    Conv     -> (?, 28, 28, 32)\n",
    "#    Pool     -> (?, 14, 14, 32)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "'''\n",
    "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "'''\n",
    "\n",
    "# L2 ImgIn shape=(?, 14, 14, 32)\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "#    Conv      ->(?, 14, 14, 64)\n",
    "#    Pool      ->(?, 7, 7, 64)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2_flat = tf.reshape(L2, [-1, 7 * 7 * 64])\n",
    "'''\n",
    "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "Tensor(\"Reshape_1:0\", shape=(?, 3136), dtype=float32)\n",
    "'''\n",
    "\n",
    "# Final FC 7x7x64 inputs -> 10 outputs\n",
    "W3 = tf.get_variable(\"W3\", shape=[7 * 7 * 64, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L2_flat, W3) + b\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "\n",
    "print('Learning started. It takes sometime.')\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sample = \" hi if you want to go\"\n",
    "idx2char = list(set(sample))  # index -> char\n",
    "char2idx = {c: i for i, c in enumerate(idx2char)}  # char -> idex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hi if you want to go'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', 'a', 'n', 'g', 'f', 'u', 'y', ' ', 'h', 'i', 'o', 't']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': 0,\n",
       " 'a': 1,\n",
       " 'n': 2,\n",
       " 'g': 3,\n",
       " 'f': 4,\n",
       " 'u': 5,\n",
       " 'y': 6,\n",
       " ' ': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'o': 10,\n",
       " 't': 11}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 8, 9, 7, 9, 4, 7, 6, 10, 5, 7, 0, 1, 2, 11, 7, 11, 10, 7, 3]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_idx = [char2idx[c] for c in sample]\n",
    "x_data = [sample_idx[:-1]] \n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Sequence RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-30-8abd0f19a11d>:30: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-30-8abd0f19a11d>:34: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "0 loss: 2.3067274 Prediction: o ooo          \n",
      "1 loss: 2.15772 Prediction: o ooo   o y yo \n",
      "2 loss: 2.0345814 Prediction: o   ou         \n",
      "3 loss: 1.8417339 Prediction: o   ou  nn   ou\n",
      "4 loss: 1.6348447 Prediction: o   ou  ant you\n",
      "5 loss: 1.3637784 Prediction: o  oou want you\n",
      "6 loss: 1.0912617 Prediction: y  oou want you\n",
      "7 loss: 0.85352796 Prediction: y  you want you\n",
      "8 loss: 0.65058696 Prediction: yf you want you\n",
      "9 loss: 0.48393548 Prediction: yf you want you\n",
      "10 loss: 0.3609791 Prediction: yf you want you\n",
      "11 loss: 0.26403543 Prediction: if you want you\n",
      "12 loss: 0.19114648 Prediction: if you want you\n",
      "13 loss: 0.13320312 Prediction: if you want you\n",
      "14 loss: 0.08980853 Prediction: if you want you\n",
      "15 loss: 0.0601851 Prediction: if you want you\n",
      "16 loss: 0.04114111 Prediction: if you want you\n",
      "17 loss: 0.029269915 Prediction: if you want you\n",
      "18 loss: 0.021812346 Prediction: if you want you\n",
      "19 loss: 0.016804954 Prediction: if you want you\n",
      "20 loss: 0.013110973 Prediction: if you want you\n",
      "21 loss: 0.010220044 Prediction: if you want you\n",
      "22 loss: 0.007949304 Prediction: if you want you\n",
      "23 loss: 0.0062139346 Prediction: if you want you\n",
      "24 loss: 0.004926492 Prediction: if you want you\n",
      "25 loss: 0.003986721 Prediction: if you want you\n",
      "26 loss: 0.0032994633 Prediction: if you want you\n",
      "27 loss: 0.0027866568 Prediction: if you want you\n",
      "28 loss: 0.0023910457 Prediction: if you want you\n",
      "29 loss: 0.0020739292 Prediction: if you want you\n",
      "30 loss: 0.0018115033 Prediction: if you want you\n",
      "31 loss: 0.0015905886 Prediction: if you want you\n",
      "32 loss: 0.0014039989 Prediction: if you want you\n",
      "33 loss: 0.0012473022 Prediction: if you want you\n",
      "34 loss: 0.0011170863 Prediction: if you want you\n",
      "35 loss: 0.001009897 Prediction: if you want you\n",
      "36 loss: 0.0009220468 Prediction: if you want you\n",
      "37 loss: 0.0008501547 Prediction: if you want you\n",
      "38 loss: 0.000791051 Prediction: if you want you\n",
      "39 loss: 0.0007419372 Prediction: if you want you\n",
      "40 loss: 0.000700656 Prediction: if you want you\n",
      "41 loss: 0.00066531944 Prediction: if you want you\n",
      "42 loss: 0.0006346425 Prediction: if you want you\n",
      "43 loss: 0.00060753006 Prediction: if you want you\n",
      "44 loss: 0.00058314897 Prediction: if you want you\n",
      "45 loss: 0.00056104676 Prediction: if you want you\n",
      "46 loss: 0.0005408426 Prediction: if you want you\n",
      "47 loss: 0.0005222982 Prediction: if you want you\n",
      "48 loss: 0.0005051595 Prediction: if you want you\n",
      "49 loss: 0.00048937084 Prediction: if you want you\n"
     ]
    }
   ],
   "source": [
    "# Lab 12 Character Sequence RNN\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(123)  # reproducibility\n",
    "\n",
    "tf.reset_default_graph() # correct an error : Variable rnn/basic_lstm_cell/kernel already exists\n",
    "\n",
    "sample = \" if you want you\"\n",
    "idx2char = list(set(sample))  # index -> char\n",
    "char2idx = {c: i for i, c in enumerate(idx2char)}  # char -> idex\n",
    "\n",
    "# hyper parameters\n",
    "dic_size = len(char2idx)  # RNN input size (one hot size)\n",
    "hidden_size = len(char2idx)  # RNN output size\n",
    "num_classes = len(char2idx)  # final output size (RNN or softmax, etc.)\n",
    "batch_size = 1  # one sample data, one batch\n",
    "sequence_length = len(sample) - 1  # number of lstm rollings (unit #)\n",
    "\n",
    "sample_idx = [char2idx[c] for c in sample]  # char to index\n",
    "x_data = [sample_idx[:-1]]  # X data sample (0 ~ n-1) hello: hell\n",
    "y_data = [sample_idx[1:]]   # Y label sample (1 ~ n) hello: ello\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])  # X data\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
    "\n",
    "x_one_hot = tf.one_hot(X, num_classes)  # one hot: 1 -> 0 1 0 0 0 0 0 0 0 0\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_size, state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)   \n",
    "\n",
    "outputs, _states = tf.nn.dynamic_rnn(\n",
    "    cell, x_one_hot, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "# FC layer\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "outputs = tf.contrib.layers.fully_connected(outputs, num_classes, activation_fn=None)\n",
    "\n",
    "# reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits=outputs, targets=Y, weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_data})\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "\n",
    "        print(i, \"loss:\", l, \"Prediction:\", ''.join(result_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
