{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beomc\\Anaconda3\\envs\\py35tf\\python.exe\n",
      "3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 18:50:55) [MSC v.1915 64 bit (AMD64)]\n",
      "sys.version_info(major=3, minor=6, micro=8, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    " import sys\n",
    " print(sys.executable)\n",
    " print(sys.version)\n",
    " print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "# Create a constant op\n",
    "# This op is added as a node to the default graph\n",
    "\n",
    "hello = tf.constant(\"Hello, TensorFlow!\")\n",
    "\n",
    "# start a TF session\n",
    "sess = tf.Session()\n",
    "\n",
    "# run the op and get result\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "# 1. basic \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.add(1, 2,)\n",
    "b = tf.multiply(a, 3)\n",
    "c = tf.add(4, 5,)\n",
    "d = tf.multiply(c, 6,)\n",
    "e = tf.multiply(4, 5,)\n",
    "f = tf.div(c, 6,)\n",
    "g = tf.add(b, d)\n",
    "h = tf.multiply(g, f)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(h)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "# 2. add summary.FileWriter \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.add(1, 2,)\n",
    "b = tf.multiply(a, 3)\n",
    "c = tf.add(4, 5,)\n",
    "d = tf.multiply(c, 6,)\n",
    "e = tf.multiply(4, 5,)\n",
    "f = tf.div(c, 6,)\n",
    "g = tf.add(b, d)\n",
    "h = tf.multiply(g, f)\n",
    "  \n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"output\", sess.graph)\n",
    "    print(sess.run(h))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. adding names\n",
    "\n",
    "a = tf.add(1, 2, name=\"Add_these_numbers\")\n",
    "b = tf.multiply(a, 3)\n",
    "c = tf.add(4, 5, name=\"And_These_ones\")\n",
    "d = tf.multiply(c, 6, name=\"Multiply_these_numbers\")\n",
    "e = tf.multiply(4, 5, name=\"B_add\")\n",
    "f = tf.div(c, 6, name=\"B_mul\")\n",
    "g = tf.add(b, d)\n",
    "h = tf.multiply(g, f)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"output\", sess.graph)\n",
    "    print(sess.run(h))\n",
    "    writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "# 4. Here we are defining the name of the graph, scopes A, B and C.\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.add(1, 5, name=\"Add_these_numbers\") # 2 -> 5\n",
    "b = tf.multiply(a, 3)\n",
    "c = tf.add(4, 5, name=\"And_These_ones\")\n",
    "d = tf.multiply(c, 6, name=\"Multiply_these_numbers\")\n",
    "e = tf.multiply(4, 5, name=\"B_add\")\n",
    "f = tf.div(c, 6, name=\"B_mul\")\n",
    "g = tf.add(b, d)\n",
    "h = tf.multiply(g, f)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"MyOperationGroup\"):\n",
    "    with tf.name_scope(\"Scope_A\"):\n",
    "        a = tf.add(1, 5, name=\"Add_these_numbers\")  # 2 -> 5\n",
    "        b = tf.multiply(a, 3)\n",
    "    with tf.name_scope(\"Scope_B\"):\n",
    "        c = tf.add(4, 5, name=\"And_These_ones\")\n",
    "        d = tf.multiply(c, 6, name=\"Multiply_these_numbers\")\n",
    "\n",
    "with tf.name_scope(\"Scope_C\"):\n",
    "    e = tf.multiply(4, 5, name=\"B_add\")\n",
    "    f = tf.div(c, 6, name=\"B_mul\")\n",
    "    g = tf.add(b, d)\n",
    "    h = tf.multiply(g, f)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"board/test_2\", sess.graph)\n",
    "    print(sess.run(h))\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "# 4. Here we are defining the name of the graph, scopes A, B and C.\n",
    "import tensorflow as tf\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default(): \n",
    "    \n",
    "    a = tf.add(1, 5, name=\"Add_these_numbers\") # 2 -> 5\n",
    "    b = tf.multiply(a, 3)\n",
    "    c = tf.add(4, 5, name=\"And_These_ones\")\n",
    "    d = tf.multiply(c, 6, name=\"Multiply_these_numbers\")\n",
    "    e = tf.multiply(4, 5, name=\"B_add\")\n",
    "    f = tf.div(c, 6, name=\"B_mul\")\n",
    "    g = tf.add(b, d)\n",
    "    h = tf.multiply(g, f)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"MyOperationGroup\"):\n",
    "    with tf.name_scope(\"Scope_A\"):\n",
    "        a = tf.add(1, 5, name=\"Add_these_numbers\")  # 2 -> 5\n",
    "        b = tf.multiply(a, 3)\n",
    "                \n",
    "    with tf.name_scope(\"Scope_B\"):\n",
    "        c = tf.add(4, 5, name=\"And_These_ones\")\n",
    "        d = tf.multiply(c, 6, name=\"Multiply_these_numbers\")\n",
    "\n",
    "with tf.name_scope(\"Scope_C\"):\n",
    "    e = tf.multiply(4, 5, name=\"B_add\")\n",
    "    f = tf.div(c, 6, name=\"B_mul\")\n",
    "    g = tf.add(b, d)\n",
    "    h = tf.multiply(g, f)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"board/test_2\", sess.graph)\n",
    "    print(sess.run(h))\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default(): \n",
    "    \n",
    "#    a = tf.add(1, 5, name=\"Add_1_5\")   \n",
    "#    b = tf.multiply(a, 4, name=\"multiply_a_4\")\n",
    "#    c = tf.add(4, 2, name=\"add_4_2\")\n",
    "#    d = tf.multiply(c, 2, name=\"Multiply_c_6\")\n",
    "#    e = tf.div(b, d, name=\"divide_b_d\") \n",
    "    \n",
    "    with tf.name_scope(\"Scope_A\"):\n",
    "        a = tf.add(1, 5, name=\"Add_1_5\")   \n",
    "        b = tf.multiply(a, 4, name=\"multiply_a_4\")\n",
    "        \n",
    "    with tf.name_scope(\"Scope_B\"):\n",
    "        c = tf.add(4, 2, name=\"add_4_2\")\n",
    "        d = tf.multiply(c, 2, name=\"Multiply_c_6\")\n",
    "        \n",
    "        \n",
    "    with tf.name_scope(\"Scope_C\"):\n",
    "        e = tf.div(b, d, name=\"divide_b_d\") \n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(\"board/test_2\", sess.graph)\n",
    "        print(sess.run(e))\n",
    "        writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  여기까지 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12. 12. 12.]\n",
      " [18. 18. 18.]\n",
      " [32. 32. 32.]]\n"
     ]
    }
   ],
   "source": [
    "# remove log directory or log files in tensorboard\n",
    "# directory ./board/sample_1  --> board/test_1 \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.placeholder(tf.float32,shape=[3, 3],name='X')\n",
    "b = tf.constant([[1, 1, 1],[2, 2, 2],[3, 3, 3]],tf.float32,name='Y')\n",
    "c = tf.matmul(a,b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('./board/test_1',sess.graph)\n",
    "    print (sess.run(c,feed_dict={a:[[2, 2, 2],[3, 3, 3],[4, 5, 6]]}))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Do not use tf.reset_default_graph() to clear nested graphs. If you need a cleared graph, exit the nesting and create a new graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d90348fd3953>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# For Tensorboard with iPython\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\beomc\\anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mreset_default_graph\u001b[1;34m()\u001b[0m\n\u001b[0;32m   5537\u001b[0m   \"\"\"\n\u001b[0;32m   5538\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_default_graph_stack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_cleared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5539\u001b[1;33m     raise AssertionError(\"Do not use tf.reset_default_graph() to clear \"\n\u001b[0m\u001b[0;32m   5540\u001b[0m                          \u001b[1;34m\"nested graphs. If you need a cleared graph, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5541\u001b[0m                          \"exit the nesting and create a new graph.\")\n",
      "\u001b[1;31mAssertionError\u001b[0m: Do not use tf.reset_default_graph() to clear nested graphs. If you need a cleared graph, exit the nesting and create a new graph."
     ]
    }
   ],
   "source": [
    "## tf.reset_default_graph() 메서드는 기존에 생성된 Graph를 모두 삭제하고 Reset 시켜 중복 방지\n",
    "# 일반적인 Python 실행 환경에서는 큰 문제가 없지만, Context가 유지되는 Jupyter Notebook 환경에서 실행할 경우를 위해, \n",
    "# 반드시 tf.reset_default_graph() 메서드를 통해 Reset 필요\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "learning_rate = 0.1\n",
    "\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "conn1  = 10\n",
    "conn2  = 10\n",
    "conn3  = 10\n",
    "connR  = 1\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, conn1]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([conn1]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1, name='layer1')\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([conn1, conn2]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([conn2]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2, name='layer2')\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([conn2, conn3]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([conn3]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3, name='layer3')\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([conn3, connR]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([connR]), name='bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4, name='hypothesis')\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis), name='cost')\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate, name='train').minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32), name='accuracy')\n",
    "\n",
    "w1_hist = tf.summary.histogram(\"weight1\", W1)\n",
    "w2_hist = tf.summary.histogram(\"weight2\", W2)\n",
    "w3_hist = tf.summary.histogram(\"weight3\", W3)\n",
    "w4_hist = tf.summary.histogram(\"weight4\", W4)\n",
    "\n",
    "b1_hist = tf.summary.histogram(\"bias1\", b1)\n",
    "b2_hist = tf.summary.histogram(\"bias2\", b2)\n",
    "b3_hist = tf.summary.histogram(\"bias3\", b3)\n",
    "b4_hist = tf.summary.histogram(\"bias4\", b4)\n",
    "\n",
    "layer1_hist = tf.summary.histogram(\"layer1\", layer1)\n",
    "layer2_hist = tf.summary.histogram(\"layer2\", layer2)\n",
    "layer3_hist = tf.summary.histogram(\"layer3\", layer3)\n",
    "hypo_hist = tf.summary.histogram(\"hypothesis\", hypothesis)\n",
    "\n",
    "cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "accuracy_summ = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./summaries/deep_nn_for_xor\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('./summaries/deep_nn_for_xor')\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    tf.reset_default_graph() # For Tensorboard with iPython\n",
    "\n",
    "    for step in range(1001):\n",
    "        print(\"step:\", step)\n",
    "        summ, _ = sess.run([merged, train], feed_dict={X: x_data, Y: y_data})\n",
    "        writer.add_summary(summ, global_step=step)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), \n",
    "                  sess.run([W1, W2, W3, W4]))\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.reset_default_graph()\n",
    "https://stackoverflow.com/questions/44622488/why-old-nodes-are-visible-even-after-deleting-event-files-tensorflow\n",
    "adding tf.reset_default_graph() in the beginning of the code also solved the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using Graph.as_default():\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "  c = tf.constant(5.0)\n",
    "  assert c.graph is g\n",
    "import tensorflow as tf\n",
    "\n",
    "g1 = tf.Graph()\n",
    "g2 = tf.Graph()\n",
    "\n",
    "sess = tf.Session(graph=g1)\n",
    "\n",
    "g2.as_default                   # with 문 안에 없기 때문에 g2에 연산이 등록되지는 않는다.(g2 그래프를 세션에 등록하고 run 해봐야 empty라는 오류가 뜰 뿐이다)\n",
    "x = tf.Variable(3)\n",
    "with g1.as_default():\n",
    "    a = tf.Variable(3)\n",
    "    b = tf.Variable(5)\n",
    "    c = tf.add(a, b)\n",
    "g2.as_default\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(c)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 32.  39.  46.]\n",
      " [ 54.  65.  76.]\n",
      " [ 76.  91. 106.]]\n"
     ]
    }
   ],
   "source": [
    "# remove log directory or log files in tensorboard\n",
    "#\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.placeholder(tf.float32,shape=[3, 3],name='X')\n",
    "b = tf.constant([[5,5,5],[2,3,4],[4,5,6]],tf.float32,name='Y')\n",
    "c = tf.matmul(a,b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('./board/sample_1',sess.graph)\n",
    "    print (sess.run(c,feed_dict={a:[[2,3,4],[4,5,6],[6,7,8]]}))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(1.0)\n",
    "b = tf.constant(2.0)\n",
    "c = tf.constant(3.0)\n",
    "d = a + b * c\n",
    "\n",
    "# tensorboard에 point라는 이름으로 표시됨\n",
    "\n",
    "d_summary = tf.summary.scalar('function_of_abc', d)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('./board/sample_1', sess.graph)\n",
    "\n",
    "    result = sess.run([merged])\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    writer.add_summary(result[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "add = tf.add(X, Y)\n",
    "mul = tf.multiply(X, Y)\n",
    "\n",
    "# step 1: node 선택\n",
    "add_hist = tf.summary.scalar('add_scalar', add)\n",
    "mul_hist = tf.summary.scalar('mul_scalar', mul)\n",
    "\n",
    "# step 2: summary 통합. 두 개의 코드 모두 동작.\n",
    "merged = tf.summary.merge_all()\n",
    "# merged = tf.summary.merge([add_hist, mul_hist])\n",
    "\n",
    "# with \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # step 3: writer 생성\n",
    "    writer = tf.summary.FileWriter('./board/sample_2', sess.graph)\n",
    "\n",
    "    for step in range(10):\n",
    "        # step 4: 노드 추가\n",
    "        summary = sess.run(merged, feed_dict={X: step * 1.0, Y: 2.0})\n",
    "        writer.add_summary(summary, step)\n",
    "\n",
    "# step 5: 콘솔에서 명령 실행\n",
    "# tensorboard --logdir=./board/sample_2\n",
    "# 콘솔 폴더가 현재 폴더(.)가 아니라면, 절대 경로 지정\n",
    "# tensorboard --logdir=~/working/board/sample_2\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.0, 2.0, 3.0]], [[7.0, 8.0, 9.0]]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a rank 0 tensor; this is a scalar with shape []\n",
    "\n",
    "[1. ,2., 3.] # a rank 1 tensor; this is a vector with shape [3]\n",
    "[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]\n",
    "[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "node3 = tf.add(node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1: Tensor(\"Const_1:0\", shape=(), dtype=float32) node2: Tensor(\"Const_2:0\", shape=(), dtype=float32)\n",
      "node3:  Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"node1:\", node1, \"node2:\", node2)\n",
    "print(\"node3: \", node3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess.run(node1, node2):  [3.0, 4.0]\n",
      "sess.run(node3):  7.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(\"sess.run(node1, node2): \", sess.run([node1, node2]))\n",
    "print(\"sess.run(node3): \", sess.run(node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b  # + provides a shortcut for tf.add(a, b)\n",
    "\n",
    "print(sess.run(adder_node, feed_dict={a: 3, b: 4.5}))\n",
    "print(sess.run(adder_node, feed_dict={a: [1,3], b: [2, 4]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5\n"
     ]
    }
   ],
   "source": [
    "add_and_triple = adder_node * 3.\n",
    "print(sess.run(add_and_triple, feed_dict={a: 3, b:4.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 12.6649065 [0.74119896] [-0.8895259]\n",
      "20 0.12503852 [1.9862969] [-0.32602474]\n",
      "40 0.010406003 [2.0998652] [-0.2610272]\n",
      "60 0.008515586 [2.1059244] [-0.24402983]\n",
      "80 0.0077254977 [2.1019704] [-0.23211078]\n",
      "100 0.007016357 [2.0972755] [-0.22115958]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(123)  # for reproducibility\n",
    "\n",
    "# X and Y data\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [2, 4, 6]\n",
    "\n",
    "# Try to find values for W and b to compute y_data = x_data * W + b \n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Our hypothesis X * W + b\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "# Minimize\n",
    "q_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "xy_train = q_optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the line\n",
    "# range 1001 : W 1.94 -> 2001 W  1.998\n",
    "\n",
    "for step in range(101):\n",
    "    sess.run(xy_train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear_regression with feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 27.18444 [-0.5406993] <tf.Variable 'weight_12:0' shape=(1,) dtype=float32_ref> [0.92557]\n",
      "20 0.6263313 [1.1120901] <tf.Variable 'weight_12:0' shape=(1,) dtype=float32_ref> [1.5566108]\n",
      "40 0.35067144 [1.2998266] <tf.Variable 'weight_12:0' shape=(1,) dtype=float32_ref> [1.5476849]\n",
      "60 0.3165073 [1.346635] <tf.Variable 'weight_12:0' shape=(1,) dtype=float32_ref> [1.4810652]\n",
      "80 0.28743902 [1.3786646] <tf.Variable 'weight_12:0' shape=(1,) dtype=float32_ref> [1.4120429]\n",
      "100 0.2610565 [1.407991] <tf.Variable 'weight_12:0' shape=(1,) dtype=float32_ref> [1.3457373]\n"
     ]
    }
   ],
   "source": [
    "# Lab 2 Linear Regression\n",
    "# lab-02-2-linear_regression_feed.py\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "# Try to find values for W and b to compute y_data = W * x_data + b \n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Now we can use X and Y in place of x_data and y_data\n",
    "# # placeholders for a tensor that will be always fed using feed_dict\n",
    "# See http://stackoverflow.com/questions/36693740/\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None])\n",
    "Y = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = X * W + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the line\n",
    "for step in range(101):\n",
    "    cost_val, W_val, b_val, _ = sess.run([cost, W, b, train],\n",
    "                 feed_dict={X: [1, 2, 3], Y: [2, 4, 6]})\n",
    "    if step % 20 == 0:\n",
    "        print(step, cost_val, W_val, b_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9999969] b: [0.9999908] loss: 5.6999738e-11\n"
     ]
    }
   ],
   "source": [
    "# lab-02-3-linear_regression_tensorflow.org.py \n",
    "#\n",
    "\n",
    "# From https://www.tensorflow.org/get_started/get_started\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable([.3], tf.float32)\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "\n",
    "# Model input and output\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "linear_model = x * W + b\n",
    "\n",
    "# cost/loss function\n",
    "\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y))  # sum of the squares\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# training data\n",
    "x_train = [1, 2, 3, 4]\n",
    "y_train = [0, -1, -2, -3]\n",
    "\n",
    "# training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)  # reset values to wrong\n",
    "for i in range(1000):\n",
    "    sess.run(train, {x: x_train, y: y_train})\n",
    "\n",
    "# evaluate training accuracy\n",
    "curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\n",
    "print(\"W: %s b: %s loss: %s\" % (curr_W, curr_b, curr_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib as plt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# minimizing_cost_show_graph.py\n",
    " \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "tf.set_random_seed(123)  # for reproducibility\n",
    "\n",
    "X = [1, 2, 3]\n",
    "Y = [2, 4, 6]\n",
    "\n",
    "W = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for linear model X * W\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "\n",
    "# Variables for plotting cost function\n",
    "W_history = []\n",
    "cost_history = []\n",
    "\n",
    "for i in range(-30, 70):\n",
    "    curr_W = i * 0.1\n",
    "    curr_cost = sess.run(cost, feed_dict={W: curr_W})\n",
    "    W_history.append(curr_W)\n",
    "    cost_history.append(curr_cost)\n",
    "#    print(i, sess.run(cost), sess.run(W_history))\n",
    "\n",
    "# Show the cost function\n",
    "plt.plot(W_history, cost_history)\n",
    "# plt(W_history, cost_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1026988 [0.51390064]\n",
      "1 0.3136566 [0.740747]\n",
      "2 0.08921791 [0.8617317]\n",
      "3 0.025377547 [0.9262569]\n",
      "4 0.0072185085 [0.96067035]\n",
      "5 0.002053265 [0.9790242]\n",
      "6 0.0005840441 [0.98881286]\n",
      "7 0.00016612909 [0.9940335]\n",
      "8 4.725356e-05 [0.9968179]\n",
      "9 1.3441259e-05 [0.9983029]\n",
      "10 3.8236512e-06 [0.99909484]\n",
      "11 1.0874437e-06 [0.99951726]\n",
      "12 3.094101e-07 [0.9997425]\n",
      "13 8.800998e-08 [0.9998627]\n",
      "14 2.5050833e-08 [0.99992675]\n",
      "15 7.1082944e-09 [0.99996096]\n",
      "16 2.02186e-09 [0.9999792]\n",
      "17 5.7622646e-10 [0.9999889]\n",
      "18 1.6179176e-10 [0.9999941]\n",
      "19 4.694911e-11 [0.99999684]\n",
      "20 1.2998195e-11 [0.99999833]\n"
     ]
    }
   ],
   "source": [
    "# lab-03-2-minimizing_cost_gradient_update.py\n",
    "#\n",
    "\n",
    "# Lab 3 Minimizing Cost\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [2, 4, 6]\n",
    "\n",
    "# Try to find values for W and b to compute y_data = W * x_data\n",
    " \n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for linear model X * W\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X - Y) * X)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(21):\n",
    "    sess.run(update, feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.0\n",
      "20 2.2866666\n",
      "40 2.1057777\n",
      "60 2.0937185\n",
      "80 2.0929146\n"
     ]
    }
   ],
   "source": [
    "# lab-03-3-minimizing_cost_tf_optimizer.py\n",
    "# # Lab 3 Minimizing Cost\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "# tf Graph Input\n",
    "X = [1, 2, 3]\n",
    "Y = [2, 4.2, 6.3]\n",
    "\n",
    "# Set wrong model weights\n",
    "W = tf.Variable(5.0)\n",
    "\n",
    "# Linear model\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent Magic\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100):\n",
    "      if step % 20 == 0:\n",
    "            print(step, sess.run(W))\n",
    "            sess.run(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [37.333332, 5.0, [(37.333336, 5.0)]]\n",
      "20 [33.84889, 4.6266665, [(33.84889, 4.6266665)]]\n",
      "40 [30.689657, 4.2881775, [(30.689657, 4.2881775)]]\n",
      "60 [27.825287, 3.9812808, [(27.825287, 3.9812808)]]\n",
      "80 [25.228262, 3.703028, [(25.228262, 3.703028)]]\n"
     ]
    }
   ],
   "source": [
    "# lab-03-X-minimizing_cost_tf_gradient.py\n",
    "#\n",
    "# Lab 3 Minimizing Cost\n",
    "# This is optional\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "# tf Graph Input\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "# Set wrong model weights\n",
    "W = tf.Variable(5.)\n",
    "\n",
    "# Linear model\n",
    "hypothesis = X * W\n",
    "\n",
    "# Manual gradient\n",
    "gradient = tf.reduce_mean((W * X - Y) * X) * 2\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent Magic\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Get gradients\n",
    "gvs = optimizer.compute_gradients(cost, [W])\n",
    "# Optional: modify gradient if necessary\n",
    "# gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "# Apply gradients\n",
    "apply_gradients = optimizer.apply_gradients(gvs)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100):\n",
    "     if step % 20 == 0:\n",
    "            print(step, sess.run([gradient, W, gvs]))\n",
    "            sess.run(apply_gradients)\n",
    "    # Same as sess.run(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  43078.645 \n",
      "Prediction:\n",
      " [-24.25063  -42.264275 -34.615715 -40.239582 -33.53014 ]\n",
      "100 Cost:  29.596216 \n",
      "Prediction:\n",
      " [159.63118 179.1522  183.33926 197.1339  135.43394]\n",
      "200 Cost:  28.052479 \n",
      "Prediction:\n",
      " [159.4226  179.29501 183.27512 197.08934 135.61977]\n",
      "300 Cost:  26.589993 \n",
      "Prediction:\n",
      " [159.21956 179.43402 183.21266 197.04605 135.80061]\n",
      "400 Cost:  25.2047 \n",
      "Prediction:\n",
      " [159.02193 179.56932 183.15187 197.00392 135.97658]\n",
      "500 Cost:  23.892467 \n",
      "Prediction:\n",
      " [158.82957 179.701   183.0927  196.963   136.14784]\n",
      "600 Cost:  22.649403 \n",
      "Prediction:\n",
      " [158.64233 179.82918 183.03505 196.92316 136.31447]\n",
      "700 Cost:  21.471952 \n",
      "Prediction:\n",
      " [158.46008 179.95393 182.97899 196.88445 136.47662]\n",
      "800 Cost:  20.356485 \n",
      "Prediction:\n",
      " [158.28267 180.07536 182.92436 196.84679 136.63441]\n",
      "900 Cost:  19.29987 \n",
      "Prediction:\n",
      " [158.10999 180.19356 182.87123 196.8102  136.78798]\n",
      "1000 Cost:  18.298985 \n",
      "Prediction:\n",
      " [157.9419  180.3086  182.81949 196.77461 136.9374 ]\n",
      "1100 Cost:  17.350838 \n",
      "Prediction:\n",
      " [157.77829 180.42056 182.7691  196.74    137.08281]\n",
      "1200 Cost:  16.452702 \n",
      "Prediction:\n",
      " [157.61903 180.52957 182.72008 196.70639 137.22429]\n",
      "1300 Cost:  15.601885 \n",
      "Prediction:\n",
      " [157.464   180.63565 182.67233 196.67368 137.36197]\n",
      "1400 Cost:  14.795889 \n",
      "Prediction:\n",
      " [157.3131  180.73892 182.62585 196.64189 137.49594]\n",
      "1500 Cost:  14.032407 \n",
      "Prediction:\n",
      " [157.16621 180.83943 182.5806  196.611   137.62631]\n",
      "1600 Cost:  13.309143 \n",
      "Prediction:\n",
      " [157.02322 180.93727 182.53656 196.58095 137.75317]\n",
      "1700 Cost:  12.624034 \n",
      "Prediction:\n",
      " [156.88406 181.0325  182.49368 196.55176 137.87663]\n",
      "1800 Cost:  11.975054 \n",
      "Prediction:\n",
      " [156.74858 181.12518 182.45193 196.52338 137.99673]\n",
      "1900 Cost:  11.360227 \n",
      "Prediction:\n",
      " [156.61671 181.21542 182.41129 196.49577 138.11362]\n",
      "2000 Cost:  10.777884 \n",
      "Prediction:\n",
      " [156.48834 181.30322 182.37172 196.46895 138.22733]\n"
     ]
    }
   ],
   "source": [
    "# lab-04-1-multi_variable_linear_regression.py\n",
    "# \n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(123)  # for reproducibility\n",
    "\n",
    "x1_data = [73., 93., 89., 96., 73.]\n",
    "x2_data = [80., 88., 91., 98., 66.]\n",
    "x3_data = [75., 93., 90., 100., 70.]\n",
    "\n",
    "y_data = [152., 185., 180., 196., 142.]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([1]), name='weight1')\n",
    "w2 = tf.Variable(tf.random_normal([1]), name='weight2')\n",
    "w3 = tf.Variable(tf.random_normal([1]), name='weight3')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize. Need a very small learning rate for this data set\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                                   feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})\n",
    "    if step % 200 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  11427.739 \n",
      "Prediction:\n",
      " [[-23.341272]\n",
      " [-25.24855 ]\n",
      " [-27.155828]\n",
      " [-29.063103]\n",
      " [-30.970379]]\n",
      "50 Cost:  0.20513625 \n",
      "Prediction:\n",
      " [[64.30894 ]\n",
      " [71.62681 ]\n",
      " [78.94466 ]\n",
      " [86.262535]\n",
      " [93.58039 ]]\n",
      "100 Cost:  0.1918298 \n",
      "Prediction:\n",
      " [[64.331726]\n",
      " [71.639114]\n",
      " [78.946495]\n",
      " [86.25387 ]\n",
      " [93.56126 ]]\n",
      "150 Cost:  0.17938723 \n",
      "Prediction:\n",
      " [[64.353745]\n",
      " [71.65099 ]\n",
      " [78.948235]\n",
      " [86.24547 ]\n",
      " [93.542725]]\n",
      "200 Cost:  0.16775498 \n",
      "Prediction:\n",
      " [[64.37505]\n",
      " [71.6625 ]\n",
      " [78.94994]\n",
      " [86.23739]\n",
      " [93.52483]]\n",
      "250 Cost:  0.15687156 \n",
      "Prediction:\n",
      " [[64.39568 ]\n",
      " [71.67365 ]\n",
      " [78.951614]\n",
      " [86.229576]\n",
      " [93.50755 ]]\n",
      "300 Cost:  0.14669576 \n",
      "Prediction:\n",
      " [[64.415596]\n",
      " [71.684395]\n",
      " [78.95319 ]\n",
      " [86.221985]\n",
      " [93.49079 ]]\n",
      "350 Cost:  0.13718462 \n",
      "Prediction:\n",
      " [[64.43487 ]\n",
      " [71.6948  ]\n",
      " [78.95474 ]\n",
      " [86.214676]\n",
      " [93.474625]]\n",
      "400 Cost:  0.12828529 \n",
      "Prediction:\n",
      " [[64.45351]\n",
      " [71.70487]\n",
      " [78.95623]\n",
      " [86.20761]\n",
      " [93.45897]]\n",
      "450 Cost:  0.11996198 \n",
      "Prediction:\n",
      " [[64.47153 ]\n",
      " [71.7146  ]\n",
      " [78.95768 ]\n",
      " [86.20075 ]\n",
      " [93.443825]]\n",
      "500 Cost:  0.11218295 \n",
      "Prediction:\n",
      " [[64.48896 ]\n",
      " [71.724014]\n",
      " [78.95908 ]\n",
      " [86.194145]\n",
      " [93.42921 ]]\n",
      "550 Cost:  0.10490751 \n",
      "Prediction:\n",
      " [[64.505806]\n",
      " [71.73312 ]\n",
      " [78.960434]\n",
      " [86.18774 ]\n",
      " [93.41506 ]]\n",
      "600 Cost:  0.09810342 \n",
      "Prediction:\n",
      " [[64.522095]\n",
      " [71.74191 ]\n",
      " [78.96172 ]\n",
      " [86.18155 ]\n",
      " [93.40136 ]]\n",
      "650 Cost:  0.09174249 \n",
      "Prediction:\n",
      " [[64.53784]\n",
      " [71.75041]\n",
      " [78.96298]\n",
      " [86.17555]\n",
      " [93.38812]]\n",
      "700 Cost:  0.08579061 \n",
      "Prediction:\n",
      " [[64.5531  ]\n",
      " [71.75867 ]\n",
      " [78.964226]\n",
      " [86.16978 ]\n",
      " [93.37535 ]]\n",
      "750 Cost:  0.08022527 \n",
      "Prediction:\n",
      " [[64.56783]\n",
      " [71.76661]\n",
      " [78.96539]\n",
      " [86.16418]\n",
      " [93.36295]]\n",
      "800 Cost:  0.07501999 \n",
      "Prediction:\n",
      " [[64.582085]\n",
      " [71.7743  ]\n",
      " [78.96653 ]\n",
      " [86.15875 ]\n",
      " [93.350975]]\n",
      "850 Cost:  0.07015583 \n",
      "Prediction:\n",
      " [[64.59587 ]\n",
      " [71.78176 ]\n",
      " [78.96764 ]\n",
      " [86.15354 ]\n",
      " [93.339424]]\n",
      "900 Cost:  0.06560627 \n",
      "Prediction:\n",
      " [[64.609184]\n",
      " [71.78895 ]\n",
      " [78.9687  ]\n",
      " [86.14846 ]\n",
      " [93.328224]]\n",
      "950 Cost:  0.061350048 \n",
      "Prediction:\n",
      " [[64.62207 ]\n",
      " [71.795906]\n",
      " [78.969734]\n",
      " [86.14357 ]\n",
      " [93.31739 ]]\n",
      "1000 Cost:  0.057371944 \n",
      "Prediction:\n",
      " [[64.634544]\n",
      " [71.80264 ]\n",
      " [78.97075 ]\n",
      " [86.138855]\n",
      " [93.306946]]\n",
      "1050 Cost:  0.053649448 \n",
      "Prediction:\n",
      " [[64.64659 ]\n",
      " [71.80914 ]\n",
      " [78.97171 ]\n",
      " [86.134254]\n",
      " [93.296814]]\n",
      "1100 Cost:  0.050170146 \n",
      "Prediction:\n",
      " [[64.65823 ]\n",
      " [71.81543 ]\n",
      " [78.972626]\n",
      " [86.129814]\n",
      " [93.28702 ]]\n",
      "1150 Cost:  0.04691708 \n",
      "Prediction:\n",
      " [[64.6695  ]\n",
      " [71.821526]\n",
      " [78.97353 ]\n",
      " [86.12554 ]\n",
      " [93.277565]]\n",
      "1200 Cost:  0.043874234 \n",
      "Prediction:\n",
      " [[64.6804 ]\n",
      " [71.8274 ]\n",
      " [78.9744 ]\n",
      " [86.12141]\n",
      " [93.2684 ]]\n",
      "1250 Cost:  0.041027747 \n",
      "Prediction:\n",
      " [[64.69094]\n",
      " [71.83309]\n",
      " [78.97525]\n",
      " [86.1174 ]\n",
      " [93.25955]]\n",
      "1300 Cost:  0.038367614 \n",
      "Prediction:\n",
      " [[64.70113]\n",
      " [71.8386 ]\n",
      " [78.97607]\n",
      " [86.11353]\n",
      " [93.25101]]\n",
      "1350 Cost:  0.035877813 \n",
      "Prediction:\n",
      " [[64.71098]\n",
      " [71.84391]\n",
      " [78.97684]\n",
      " [86.10978]\n",
      " [93.24271]]\n",
      "1400 Cost:  0.033550728 \n",
      "Prediction:\n",
      " [[64.72052]\n",
      " [71.84907]\n",
      " [78.97762]\n",
      " [86.10616]\n",
      " [93.23472]]\n",
      "1450 Cost:  0.031374857 \n",
      "Prediction:\n",
      " [[64.72973 ]\n",
      " [71.85404 ]\n",
      " [78.97835 ]\n",
      " [86.10265 ]\n",
      " [93.226974]]\n",
      "1500 Cost:  0.029340714 \n",
      "Prediction:\n",
      " [[64.73863]\n",
      " [71.85885]\n",
      " [78.97905]\n",
      " [86.09927]\n",
      " [93.21948]]\n",
      "1550 Cost:  0.027436972 \n",
      "Prediction:\n",
      " [[64.74726]\n",
      " [71.86352]\n",
      " [78.97976]\n",
      " [86.09601]\n",
      " [93.21226]]\n",
      "1600 Cost:  0.025656695 \n",
      "Prediction:\n",
      " [[64.75559 ]\n",
      " [71.86801 ]\n",
      " [78.98042 ]\n",
      " [86.092834]\n",
      " [93.205246]]\n",
      "1650 Cost:  0.023993736 \n",
      "Prediction:\n",
      " [[64.76365 ]\n",
      " [71.87236 ]\n",
      " [78.981064]\n",
      " [86.089775]\n",
      " [93.19849 ]]\n",
      "1700 Cost:  0.022437012 \n",
      "Prediction:\n",
      " [[64.771454]\n",
      " [71.87657 ]\n",
      " [78.9817  ]\n",
      " [86.08682 ]\n",
      " [93.19195 ]]\n",
      "1750 Cost:  0.020982143 \n",
      "Prediction:\n",
      " [[64.778984]\n",
      " [71.880646]\n",
      " [78.98231 ]\n",
      " [86.083954]\n",
      " [93.18562 ]]\n",
      "1800 Cost:  0.019622032 \n",
      "Prediction:\n",
      " [[64.78628 ]\n",
      " [71.88458 ]\n",
      " [78.982895]\n",
      " [86.08121 ]\n",
      " [93.17951 ]]\n",
      "1850 Cost:  0.018347418 \n",
      "Prediction:\n",
      " [[64.79332 ]\n",
      " [71.88839 ]\n",
      " [78.983444]\n",
      " [86.07851 ]\n",
      " [93.17357 ]]\n",
      "1900 Cost:  0.017158186 \n",
      "Prediction:\n",
      " [[64.80013 ]\n",
      " [71.89206 ]\n",
      " [78.983986]\n",
      " [86.07592 ]\n",
      " [93.16785 ]]\n",
      "1950 Cost:  0.016046166 \n",
      "Prediction:\n",
      " [[64.80673 ]\n",
      " [71.89563 ]\n",
      " [78.984535]\n",
      " [86.07343 ]\n",
      " [93.16234 ]]\n",
      "2000 Cost:  0.015004417 \n",
      "Prediction:\n",
      " [[64.813095]\n",
      " [71.89906 ]\n",
      " [78.98502 ]\n",
      " [86.07099 ]\n",
      " [93.15696 ]]\n",
      "0 [[-23.341272]\n",
      " [-25.24855 ]\n",
      " [-27.155828]\n",
      " [-29.063103]\n",
      " [-30.970379]] W : [[5.1347623 ]\n",
      " [0.03021693]\n",
      " [2.2316387 ]] b : [-0.507057] Cost:  11427.739 \n",
      "Prediction:\n",
      " [[-23.341272]\n",
      " [-25.24855 ]\n",
      " [-27.155828]\n",
      " [-29.063103]\n",
      " [-30.970379]]\n",
      "20 [[64.29398]\n",
      " [71.61821]\n",
      " [78.94244]\n",
      " [86.26666]\n",
      " [93.59088]] W : [[ 3.2753727]\n",
      " [-0.899478 ]\n",
      " [ 1.6728898]] b : [-0.5812462] Cost:  0.21355729 \n",
      "Prediction:\n",
      " [[64.29398]\n",
      " [71.61821]\n",
      " [78.94244]\n",
      " [86.26666]\n",
      " [93.59088]]\n",
      "40 [[64.30428]\n",
      " [71.62429]\n",
      " [78.94429]\n",
      " [86.26429]\n",
      " [93.58429]] W : [[ 3.2776523]\n",
      " [-0.8983383]\n",
      " [ 1.6628196]] b : [-0.5790042] Cost:  0.20790532 \n",
      "Prediction:\n",
      " [[64.30428]\n",
      " [71.62429]\n",
      " [78.94429]\n",
      " [86.26429]\n",
      " [93.58429]]\n",
      "60 [[64.31355 ]\n",
      " [71.62929 ]\n",
      " [78.94503 ]\n",
      " [86.260765]\n",
      " [93.5765  ]] W : [[ 3.2799206]\n",
      " [-0.8972044]\n",
      " [ 1.6528893]] b : [-0.5767914] Cost:  0.2024019 \n",
      "Prediction:\n",
      " [[64.31355 ]\n",
      " [71.62929 ]\n",
      " [78.94503 ]\n",
      " [86.260765]\n",
      " [93.5765  ]]\n",
      "80 [[64.3227  ]\n",
      " [71.63423 ]\n",
      " [78.945755]\n",
      " [86.257286]\n",
      " [93.56882 ]] W : [[ 3.2821581]\n",
      " [-0.8960857]\n",
      " [ 1.6430911]] b : [-0.574608] Cost:  0.19704254 \n",
      "Prediction:\n",
      " [[64.3227  ]\n",
      " [71.63423 ]\n",
      " [78.945755]\n",
      " [86.257286]\n",
      " [93.56882 ]]\n"
     ]
    }
   ],
   "source": [
    "# lab-04-2-multi_variable_matmul_linear_regression.py\n",
    "# print cost, W, b  depend on learning rate size\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    " \n",
    "x_data = [[20., 10., 5.],\n",
    "          [22, 11., 6.],\n",
    "          [24., 12., 7.],\n",
    "          [26., 13., 8.],\n",
    "          [28., 14., 9.]]\n",
    "y_data = [[65.],\n",
    "          [72.],\n",
    "          [79.],\n",
    "          [86.],\n",
    "          [93.]]\n",
    " \n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis : h = 1 x1 + 2 x2 + 3 x3 + 10\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "... \n",
    "# Minimize : learning_rate=1e-5 : cost 0.349  --> lr=0.001  : cost = 0.006 \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 50 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "        \n",
    "... \n",
    "\n",
    "\n",
    "# Minimize : learning_rate=00.1  : cost 0.349   \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100):\n",
    "    cost_val, hy_val, W_val, b_val, _ = sess.run(\n",
    "        [cost, hypothesis, W, b, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 20 == 0:\n",
    "        print(step, hy_val, \"W :\", W_val, \"b :\", \n",
    "              b_val, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 3) [[ 73.  80.  75.]\n",
      " [ 93.  88.  93.]\n",
      " [ 89.  91.  90.]\n",
      " [ 96.  98. 100.]\n",
      " [ 73.  66.  70.]\n",
      " [ 53.  46.  55.]\n",
      " [ 69.  74.  77.]\n",
      " [ 47.  56.  60.]\n",
      " [ 87.  79.  90.]\n",
      " [ 79.  70.  88.]\n",
      " [ 69.  70.  73.]\n",
      " [ 70.  65.  74.]\n",
      " [ 93.  95.  91.]\n",
      " [ 79.  80.  73.]\n",
      " [ 70.  73.  78.]\n",
      " [ 93.  89.  96.]\n",
      " [ 78.  75.  68.]\n",
      " [ 81.  90.  93.]\n",
      " [ 88.  92.  86.]\n",
      " [ 78.  83.  77.]\n",
      " [ 82.  86.  90.]\n",
      " [ 86.  82.  89.]\n",
      " [ 78.  83.  85.]\n",
      " [ 76.  83.  71.]\n",
      " [ 96.  93.  95.]] 25\n",
      "(25, 1) [[152.]\n",
      " [185.]\n",
      " [180.]\n",
      " [196.]\n",
      " [142.]\n",
      " [101.]\n",
      " [149.]\n",
      " [115.]\n",
      " [175.]\n",
      " [164.]\n",
      " [141.]\n",
      " [141.]\n",
      " [184.]\n",
      " [152.]\n",
      " [148.]\n",
      " [192.]\n",
      " [147.]\n",
      " [183.]\n",
      " [177.]\n",
      " [159.]\n",
      " [177.]\n",
      " [175.]\n",
      " [175.]\n",
      " [149.]\n",
      " [192.]]\n",
      "0 Cost:  144938.64 \n",
      "Prediction:\n",
      " [[-205.3319 ]\n",
      " [-245.69765]\n",
      " [-242.9623 ]\n",
      " [-263.2252 ]\n",
      " [-187.8165 ]\n",
      " [-135.35274]\n",
      " [-194.62337]\n",
      " [-141.14595]\n",
      " [-227.23412]\n",
      " [-206.74092]\n",
      " [-188.59007]\n",
      " [-184.66411]\n",
      " [-252.6548 ]\n",
      " [-211.82103]\n",
      " [-195.13019]\n",
      " [-248.00623]\n",
      " [-203.06453]\n",
      " [-233.33528]\n",
      " [-241.25508]\n",
      " [-215.53123]\n",
      " [-228.917  ]\n",
      " [-228.94368]\n",
      " [-218.73222]\n",
      " [-210.70068]\n",
      " [-255.68358]]\n",
      "50 Cost:  6.8049207 \n",
      "Prediction:\n",
      " [[152.62976 ]\n",
      " [184.52243 ]\n",
      " [180.94975 ]\n",
      " [198.44095 ]\n",
      " [140.29733 ]\n",
      " [106.540596]\n",
      " [150.94534 ]\n",
      " [115.03181 ]\n",
      " [174.82565 ]\n",
      " [165.64896 ]\n",
      " [144.34888 ]\n",
      " [143.6109  ]\n",
      " [185.31107 ]\n",
      " [152.26459 ]\n",
      " [152.02374 ]\n",
      " [188.56158 ]\n",
      " [143.68152 ]\n",
      " [181.38092 ]\n",
      " [176.28629 ]\n",
      " [158.06949 ]\n",
      " [176.31694 ]\n",
      " [174.65544 ]\n",
      " [167.62965 ]\n",
      " [150.2244  ]\n",
      " [190.19537 ]]\n",
      "100 Cost:  6.7926373 \n",
      "Prediction:\n",
      " [[152.64069]\n",
      " [184.51852]\n",
      " [180.95514]\n",
      " [198.44113]\n",
      " [140.2948 ]\n",
      " [106.52788]\n",
      " [150.94171]\n",
      " [115.0244 ]\n",
      " [174.81105]\n",
      " [165.62125]\n",
      " [144.34618]\n",
      " [143.59851]\n",
      " [185.32217]\n",
      " [152.28084]\n",
      " [152.01694]\n",
      " [188.55374]\n",
      " [143.69865]\n",
      " [181.37694]\n",
      " [176.30042]\n",
      " [158.08292]\n",
      " [176.31232]\n",
      " [174.64728]\n",
      " [167.6283 ]\n",
      " [150.2484 ]\n",
      " [190.19669]]\n",
      "150 Cost:  6.7808824 \n",
      "Prediction:\n",
      " [[152.65132]\n",
      " [184.51466]\n",
      " [180.9604 ]\n",
      " [198.44127]\n",
      " [140.29228]\n",
      " [106.51543]\n",
      " [150.93816]\n",
      " [115.01712]\n",
      " [174.79674]\n",
      " [165.59413]\n",
      " [144.34349]\n",
      " [143.58638]\n",
      " [185.33304]\n",
      " [152.29672]\n",
      " [152.01024]\n",
      " [188.54604]\n",
      " [143.71538]\n",
      " [181.373  ]\n",
      " [176.31422]\n",
      " [158.09605]\n",
      " [176.30775]\n",
      " [174.63927]\n",
      " [167.62695]\n",
      " [150.27184]\n",
      " [190.19795]]\n",
      "200 Cost:  6.7696333 \n",
      "Prediction:\n",
      " [[152.66174]\n",
      " [184.51093]\n",
      " [180.96556]\n",
      " [198.44142]\n",
      " [140.28989]\n",
      " [106.50327]\n",
      " [150.93468]\n",
      " [115.01   ]\n",
      " [174.78278]\n",
      " [165.56766]\n",
      " [144.34091]\n",
      " [143.57454]\n",
      " [185.34369]\n",
      " [152.31227]\n",
      " [152.00371]\n",
      " [188.53856]\n",
      " [143.7318 ]\n",
      " [181.36919]\n",
      " [176.32774]\n",
      " [158.1089 ]\n",
      " [176.3033 ]\n",
      " [174.63145]\n",
      " [167.62564]\n",
      " [150.29478]\n",
      " [190.19922]]\n",
      "250 Cost:  6.758875 \n",
      "Prediction:\n",
      " [[152.6719 ]\n",
      " [184.50725]\n",
      " [180.97057]\n",
      " [198.44154]\n",
      " [140.2875 ]\n",
      " [106.49137]\n",
      " [150.93124]\n",
      " [115.00302]\n",
      " [174.7691 ]\n",
      " [165.54172]\n",
      " [144.33835]\n",
      " [143.56293]\n",
      " [185.35408]\n",
      " [152.32747]\n",
      " [151.99728]\n",
      " [188.53119]\n",
      " [143.7478 ]\n",
      " [181.3654 ]\n",
      " [176.34094]\n",
      " [158.12146]\n",
      " [176.2989 ]\n",
      " [174.62378]\n",
      " [167.62433]\n",
      " [150.3172 ]\n",
      " [190.20044]]\n",
      "300 Cost:  6.748588 \n",
      "Prediction:\n",
      " [[152.68184]\n",
      " [184.50368]\n",
      " [180.97548]\n",
      " [198.44168]\n",
      " [140.2852 ]\n",
      " [106.47976]\n",
      " [150.92787]\n",
      " [114.99618]\n",
      " [174.75574]\n",
      " [165.51637]\n",
      " [144.33585]\n",
      " [143.55159]\n",
      " [185.36424]\n",
      " [152.34235]\n",
      " [151.99101]\n",
      " [188.524  ]\n",
      " [143.7635 ]\n",
      " [181.36168]\n",
      " [176.35387]\n",
      " [158.13373]\n",
      " [176.29463]\n",
      " [174.6163 ]\n",
      " [167.62305]\n",
      " [150.33913]\n",
      " [190.20164]]\n",
      "350 Cost:  6.7387433 \n",
      "Prediction:\n",
      " [[152.69157]\n",
      " [184.5002 ]\n",
      " [180.98033]\n",
      " [198.44183]\n",
      " [140.28297]\n",
      " [106.4684 ]\n",
      " [150.9246 ]\n",
      " [114.98947]\n",
      " [174.74272]\n",
      " [165.49162]\n",
      " [144.33342]\n",
      " [143.54054]\n",
      " [185.37422]\n",
      " [152.35692]\n",
      " [151.9849 ]\n",
      " [188.51701]\n",
      " [143.77888]\n",
      " [181.35808]\n",
      " [176.36653]\n",
      " [158.14577]\n",
      " [176.29045]\n",
      " [174.60902]\n",
      " [167.62181]\n",
      " [150.36061]\n",
      " [190.20285]]\n",
      "400 Cost:  6.7293 \n",
      "Prediction:\n",
      " [[152.70108 ]\n",
      " [184.4968  ]\n",
      " [180.98502 ]\n",
      " [198.44194 ]\n",
      " [140.28079 ]\n",
      " [106.457275]\n",
      " [150.92139 ]\n",
      " [114.98288 ]\n",
      " [174.72995 ]\n",
      " [165.46736 ]\n",
      " [144.33102 ]\n",
      " [143.5297  ]\n",
      " [185.38394 ]\n",
      " [152.37115 ]\n",
      " [151.97885 ]\n",
      " [188.51015 ]\n",
      " [143.79391 ]\n",
      " [181.35449 ]\n",
      " [176.37888 ]\n",
      " [158.1575  ]\n",
      " [176.28633 ]\n",
      " [174.60187 ]\n",
      " [167.62057 ]\n",
      " [150.38158 ]\n",
      " [190.20401 ]]\n",
      "450 Cost:  6.7202992 \n",
      "Prediction:\n",
      " [[152.71036]\n",
      " [184.49347]\n",
      " [180.98962]\n",
      " [198.44206]\n",
      " [140.27867]\n",
      " [106.44641]\n",
      " [150.91821]\n",
      " [114.97643]\n",
      " [174.71747]\n",
      " [165.44366]\n",
      " [144.32869]\n",
      " [143.5191 ]\n",
      " [185.39348]\n",
      " [152.3851 ]\n",
      " [151.97298]\n",
      " [188.50343]\n",
      " [143.80862]\n",
      " [181.35098]\n",
      " [176.39096]\n",
      " [158.16898]\n",
      " [176.28232]\n",
      " [174.59488]\n",
      " [167.61935]\n",
      " [150.4021 ]\n",
      " [190.20515]]\n",
      "500 Cost:  6.711673 \n",
      "Prediction:\n",
      " [[152.71944]\n",
      " [184.49022]\n",
      " [180.99413]\n",
      " [198.44218]\n",
      " [140.2766 ]\n",
      " [106.4358 ]\n",
      " [150.91512]\n",
      " [114.97011]\n",
      " [174.70528]\n",
      " [165.42049]\n",
      " [144.3264 ]\n",
      " [143.50874]\n",
      " [185.40279]\n",
      " [152.39873]\n",
      " [151.96722]\n",
      " [188.4969 ]\n",
      " [143.82304]\n",
      " [181.34755]\n",
      " [176.40279]\n",
      " [158.18022]\n",
      " [176.27837]\n",
      " [174.58806]\n",
      " [167.61815]\n",
      " [150.42216]\n",
      " [190.20628]]\n",
      "550 Cost:  6.7034297 \n",
      "Prediction:\n",
      " [[152.7283  ]\n",
      " [184.48706 ]\n",
      " [180.99852 ]\n",
      " [198.44229 ]\n",
      " [140.27458 ]\n",
      " [106.425415]\n",
      " [150.91208 ]\n",
      " [114.963905]\n",
      " [174.69336 ]\n",
      " [165.39783 ]\n",
      " [144.32416 ]\n",
      " [143.49863 ]\n",
      " [185.41191 ]\n",
      " [152.41206 ]\n",
      " [151.96158 ]\n",
      " [188.49048 ]\n",
      " [143.83713 ]\n",
      " [181.34416 ]\n",
      " [176.41435 ]\n",
      " [158.1912  ]\n",
      " [176.2745  ]\n",
      " [174.58138 ]\n",
      " [167.61697 ]\n",
      " [150.4418  ]\n",
      " [190.2074  ]]\n",
      "600 Cost:  6.695531 \n",
      "Prediction:\n",
      " [[152.737   ]\n",
      " [184.48398 ]\n",
      " [181.00284 ]\n",
      " [198.44241 ]\n",
      " [140.27263 ]\n",
      " [106.415276]\n",
      " [150.90909 ]\n",
      " [114.957825]\n",
      " [174.68172 ]\n",
      " [165.37567 ]\n",
      " [144.32196 ]\n",
      " [143.48871 ]\n",
      " [185.42084 ]\n",
      " [152.42513 ]\n",
      " [151.95605 ]\n",
      " [188.48422 ]\n",
      " [143.85094 ]\n",
      " [181.34085 ]\n",
      " [176.42569 ]\n",
      " [158.20193 ]\n",
      " [176.27072 ]\n",
      " [174.57486 ]\n",
      " [167.61581 ]\n",
      " [150.461   ]\n",
      " [190.2085  ]]\n",
      "650 Cost:  6.6879663 \n",
      "Prediction:\n",
      " [[152.74547 ]\n",
      " [184.48094 ]\n",
      " [181.00705 ]\n",
      " [198.44252 ]\n",
      " [140.2707  ]\n",
      " [106.40535 ]\n",
      " [150.90616 ]\n",
      " [114.951866]\n",
      " [174.67033 ]\n",
      " [165.35399 ]\n",
      " [144.31981 ]\n",
      " [143.47903 ]\n",
      " [185.42957 ]\n",
      " [152.43788 ]\n",
      " [151.95064 ]\n",
      " [188.4781  ]\n",
      " [143.86444 ]\n",
      " [181.3376  ]\n",
      " [176.43674 ]\n",
      " [158.21245 ]\n",
      " [176.26701 ]\n",
      " [174.56848 ]\n",
      " [167.61469 ]\n",
      " [150.47975 ]\n",
      " [190.20955 ]]\n",
      "700 Cost:  6.6807256 \n",
      "Prediction:\n",
      " [[152.75375]\n",
      " [184.47801]\n",
      " [181.01118]\n",
      " [198.44261]\n",
      " [140.26888]\n",
      " [106.39565]\n",
      " [150.9033 ]\n",
      " [114.94601]\n",
      " [174.65923]\n",
      " [165.33281]\n",
      " [144.3177 ]\n",
      " [143.46959]\n",
      " [185.4381 ]\n",
      " [152.4504 ]\n",
      " [151.94534]\n",
      " [188.47214]\n",
      " [143.87767]\n",
      " [181.3344 ]\n",
      " [176.44757]\n",
      " [158.22273]\n",
      " [176.2634 ]\n",
      " [174.56227]\n",
      " [167.61357]\n",
      " [150.49812]\n",
      " [190.21062]]\n",
      "750 Cost:  6.673807 \n",
      "Prediction:\n",
      " [[152.76184]\n",
      " [184.47514]\n",
      " [181.0152 ]\n",
      " [198.44272]\n",
      " [140.26706]\n",
      " [106.38617]\n",
      " [150.90048]\n",
      " [114.94028]\n",
      " [174.64835]\n",
      " [165.31207]\n",
      " [144.31564]\n",
      " [143.46033]\n",
      " [185.44646]\n",
      " [152.46262]\n",
      " [151.94016]\n",
      " [188.46628]\n",
      " [143.8906 ]\n",
      " [181.33127]\n",
      " [176.45816]\n",
      " [158.23276]\n",
      " [176.25983]\n",
      " [174.55617]\n",
      " [167.61247]\n",
      " [150.51608]\n",
      " [190.21167]]\n",
      "800 Cost:  6.6671886 \n",
      "Prediction:\n",
      " [[152.76976]\n",
      " [184.47234]\n",
      " [181.01915]\n",
      " [198.44281]\n",
      " [140.26529]\n",
      " [106.37691]\n",
      " [150.89772]\n",
      " [114.93467]\n",
      " [174.6377 ]\n",
      " [165.29182]\n",
      " [144.31361]\n",
      " [143.45126]\n",
      " [185.4546 ]\n",
      " [152.47456]\n",
      " [151.93507]\n",
      " [188.46057]\n",
      " [143.90326]\n",
      " [181.32817]\n",
      " [176.46849]\n",
      " [158.24257]\n",
      " [176.25633]\n",
      " [174.5502 ]\n",
      " [167.61139]\n",
      " [150.53363]\n",
      " [190.21266]]\n",
      "850 Cost:  6.66085 \n",
      "Prediction:\n",
      " [[152.7775 ]\n",
      " [184.46962]\n",
      " [181.02301]\n",
      " [198.44292]\n",
      " [140.26358]\n",
      " [106.36784]\n",
      " [150.89503]\n",
      " [114.92915]\n",
      " [174.6273 ]\n",
      " [165.272  ]\n",
      " [144.31166]\n",
      " [143.44243]\n",
      " [185.4626 ]\n",
      " [152.48627]\n",
      " [151.9301 ]\n",
      " [188.45499]\n",
      " [143.91566]\n",
      " [181.32515]\n",
      " [176.47862]\n",
      " [158.25218]\n",
      " [176.25293]\n",
      " [174.54439]\n",
      " [167.61034]\n",
      " [150.55081]\n",
      " [190.21368]]\n",
      "900 Cost:  6.6548 \n",
      "Prediction:\n",
      " [[152.78506 ]\n",
      " [184.46693 ]\n",
      " [181.02676 ]\n",
      " [198.443   ]\n",
      " [140.2619  ]\n",
      " [106.35899 ]\n",
      " [150.89236 ]\n",
      " [114.923744]\n",
      " [174.61716 ]\n",
      " [165.25262 ]\n",
      " [144.30972 ]\n",
      " [143.43376 ]\n",
      " [185.47043 ]\n",
      " [152.49771 ]\n",
      " [151.92523 ]\n",
      " [188.44952 ]\n",
      " [143.9278  ]\n",
      " [181.32217 ]\n",
      " [176.48853 ]\n",
      " [158.26157 ]\n",
      " [176.24957 ]\n",
      " [174.5387  ]\n",
      " [167.60927 ]\n",
      " [150.5676  ]\n",
      " [190.21466 ]]\n",
      "950 Cost:  6.648995 \n",
      "Prediction:\n",
      " [[152.79245 ]\n",
      " [184.46434 ]\n",
      " [181.03046 ]\n",
      " [198.44308 ]\n",
      " [140.26028 ]\n",
      " [106.35033 ]\n",
      " [150.88979 ]\n",
      " [114.918434]\n",
      " [174.60722 ]\n",
      " [165.23367 ]\n",
      " [144.30783 ]\n",
      " [143.42532 ]\n",
      " [185.47809 ]\n",
      " [152.50891 ]\n",
      " [151.92047 ]\n",
      " [188.4442  ]\n",
      " [143.93967 ]\n",
      " [181.31927 ]\n",
      " [176.49821 ]\n",
      " [158.27075 ]\n",
      " [176.2463  ]\n",
      " [174.53314 ]\n",
      " [167.60825 ]\n",
      " [150.58403 ]\n",
      " [190.21564 ]]\n",
      "1000 Cost:  6.64344 \n",
      "Prediction:\n",
      " [[152.79967]\n",
      " [184.4618 ]\n",
      " [181.03407]\n",
      " [198.44316]\n",
      " [140.25871]\n",
      " [106.34186]\n",
      " [150.88722]\n",
      " [114.91324]\n",
      " [174.59752]\n",
      " [165.21513]\n",
      " [144.30598]\n",
      " [143.41704]\n",
      " [185.48555]\n",
      " [152.51988]\n",
      " [151.9158 ]\n",
      " [188.43898]\n",
      " [143.9513 ]\n",
      " [181.31639]\n",
      " [176.50768]\n",
      " [158.27974]\n",
      " [176.24309]\n",
      " [174.5277 ]\n",
      " [167.60722]\n",
      " [150.60008]\n",
      " [190.2166 ]]\n",
      "1050 Cost:  6.638135 \n",
      "Prediction:\n",
      " [[152.80673]\n",
      " [184.45932]\n",
      " [181.0376 ]\n",
      " [198.44325]\n",
      " [140.25717]\n",
      " [106.33359]\n",
      " [150.88474]\n",
      " [114.90814]\n",
      " [174.58803]\n",
      " [165.19704]\n",
      " [144.30417]\n",
      " [143.40895]\n",
      " [185.49289]\n",
      " [152.5306 ]\n",
      " [151.91124]\n",
      " [188.43388]\n",
      " [143.96268]\n",
      " [181.31358]\n",
      " [176.51695]\n",
      " [158.28853]\n",
      " [176.23994]\n",
      " [174.52238]\n",
      " [167.60623]\n",
      " [150.6158 ]\n",
      " [190.21754]]\n",
      "1100 Cost:  6.633045 \n",
      "Prediction:\n",
      " [[152.81363 ]\n",
      " [184.4569  ]\n",
      " [181.04105 ]\n",
      " [198.44331 ]\n",
      " [140.25568 ]\n",
      " [106.3255  ]\n",
      " [150.88228 ]\n",
      " [114.903145]\n",
      " [174.57877 ]\n",
      " [165.17932 ]\n",
      " [144.30238 ]\n",
      " [143.40105 ]\n",
      " [185.50005 ]\n",
      " [152.54108 ]\n",
      " [151.90675 ]\n",
      " [188.4289  ]\n",
      " [143.97382 ]\n",
      " [181.31082 ]\n",
      " [176.526   ]\n",
      " [158.29712 ]\n",
      " [176.23686 ]\n",
      " [174.51718 ]\n",
      " [167.60526 ]\n",
      " [150.63116 ]\n",
      " [190.21844 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1150 Cost:  6.6281724 \n",
      "Prediction:\n",
      " [[152.82036]\n",
      " [184.45453]\n",
      " [181.04442]\n",
      " [198.44337]\n",
      " [140.25421]\n",
      " [106.31759]\n",
      " [150.87985]\n",
      " [114.89825]\n",
      " [174.56969]\n",
      " [165.16197]\n",
      " [144.30063]\n",
      " [143.39331]\n",
      " [185.50703]\n",
      " [152.55133]\n",
      " [151.90236]\n",
      " [188.42403]\n",
      " [143.9847 ]\n",
      " [181.3081 ]\n",
      " [176.53487]\n",
      " [158.3055 ]\n",
      " [176.23384]\n",
      " [174.5121 ]\n",
      " [167.60428]\n",
      " [150.64618]\n",
      " [190.21935]]\n",
      "1200 Cost:  6.623516 \n",
      "Prediction:\n",
      " [[152.82697]\n",
      " [184.45224]\n",
      " [181.04771]\n",
      " [198.44347]\n",
      " [140.2528 ]\n",
      " [106.30986]\n",
      " [150.87752]\n",
      " [114.89343]\n",
      " [174.56084]\n",
      " [165.14503]\n",
      " [144.29895]\n",
      " [143.38576]\n",
      " [185.5139 ]\n",
      " [152.5614 ]\n",
      " [151.89809]\n",
      " [188.41928]\n",
      " [143.99538]\n",
      " [181.30547]\n",
      " [176.54356]\n",
      " [158.3137 ]\n",
      " [176.23088]\n",
      " [174.50716]\n",
      " [167.60335]\n",
      " [150.66089]\n",
      " [190.22026]]\n",
      "1250 Cost:  6.6190667 \n",
      "Prediction:\n",
      " [[152.8334 ]\n",
      " [184.45   ]\n",
      " [181.05096]\n",
      " [198.44354]\n",
      " [140.25142]\n",
      " [106.30231]\n",
      " [150.8752 ]\n",
      " [114.88872]\n",
      " [174.55219]\n",
      " [165.12848]\n",
      " [144.29727]\n",
      " [143.37836]\n",
      " [185.52061]\n",
      " [152.57123]\n",
      " [151.89388]\n",
      " [188.41464]\n",
      " [144.00583]\n",
      " [181.30283]\n",
      " [176.55203]\n",
      " [158.32175]\n",
      " [176.228  ]\n",
      " [174.50229]\n",
      " [167.6024 ]\n",
      " [150.67526]\n",
      " [190.22113]]\n",
      "1300 Cost:  6.61481 \n",
      "Prediction:\n",
      " [[152.8397  ]\n",
      " [184.4478  ]\n",
      " [181.05411 ]\n",
      " [198.4436  ]\n",
      " [140.25009 ]\n",
      " [106.294914]\n",
      " [150.87292 ]\n",
      " [114.884094]\n",
      " [174.54373 ]\n",
      " [165.11227 ]\n",
      " [144.29565 ]\n",
      " [143.37114 ]\n",
      " [185.52718 ]\n",
      " [152.58084 ]\n",
      " [151.88977 ]\n",
      " [188.4101  ]\n",
      " [144.01605 ]\n",
      " [181.30026 ]\n",
      " [176.56032 ]\n",
      " [158.32959 ]\n",
      " [176.22516 ]\n",
      " [174.49756 ]\n",
      " [167.60147 ]\n",
      " [150.68932 ]\n",
      " [190.222   ]]\n",
      "1350 Cost:  6.6107206 \n",
      "Prediction:\n",
      " [[152.84584 ]\n",
      " [184.44566 ]\n",
      " [181.0572  ]\n",
      " [198.44365 ]\n",
      " [140.24878 ]\n",
      " [106.2877  ]\n",
      " [150.8707  ]\n",
      " [114.879555]\n",
      " [174.53545 ]\n",
      " [165.09642 ]\n",
      " [144.29405 ]\n",
      " [143.36407 ]\n",
      " [185.53358 ]\n",
      " [152.59023 ]\n",
      " [151.88573 ]\n",
      " [188.40564 ]\n",
      " [144.02605 ]\n",
      " [181.29774 ]\n",
      " [176.56844 ]\n",
      " [158.33727 ]\n",
      " [176.22237 ]\n",
      " [174.49294 ]\n",
      " [167.60057 ]\n",
      " [150.70305 ]\n",
      " [190.22285 ]]\n",
      "1400 Cost:  6.606809 \n",
      "Prediction:\n",
      " [[152.85184]\n",
      " [184.44356]\n",
      " [181.0602 ]\n",
      " [198.4437 ]\n",
      " [140.2475 ]\n",
      " [106.28063]\n",
      " [150.8685 ]\n",
      " [114.8751 ]\n",
      " [174.52734]\n",
      " [165.08092]\n",
      " [144.29247]\n",
      " [143.35715]\n",
      " [185.53986]\n",
      " [152.59943]\n",
      " [151.88176]\n",
      " [188.4013 ]\n",
      " [144.03583]\n",
      " [181.29524]\n",
      " [176.57634]\n",
      " [158.34476]\n",
      " [176.21962]\n",
      " [174.48837]\n",
      " [167.59967]\n",
      " [150.71649]\n",
      " [190.22366]]\n",
      "1450 Cost:  6.603066 \n",
      "Prediction:\n",
      " [[152.85773 ]\n",
      " [184.44153 ]\n",
      " [181.06317 ]\n",
      " [198.44379 ]\n",
      " [140.24628 ]\n",
      " [106.273735]\n",
      " [150.86636 ]\n",
      " [114.87074 ]\n",
      " [174.51947 ]\n",
      " [165.06578 ]\n",
      " [144.29094 ]\n",
      " [143.3504  ]\n",
      " [185.54599 ]\n",
      " [152.60843 ]\n",
      " [151.8779  ]\n",
      " [188.39706 ]\n",
      " [144.04543 ]\n",
      " [181.29282 ]\n",
      " [176.5841  ]\n",
      " [158.35211 ]\n",
      " [176.21696 ]\n",
      " [174.48396 ]\n",
      " [167.59882 ]\n",
      " [150.72964 ]\n",
      " [190.22449 ]]\n",
      "1500 Cost:  6.5994873 \n",
      "Prediction:\n",
      " [[152.86346 ]\n",
      " [184.43954 ]\n",
      " [181.06606 ]\n",
      " [198.44383 ]\n",
      " [140.24507 ]\n",
      " [106.26698 ]\n",
      " [150.86426 ]\n",
      " [114.866455]\n",
      " [174.51173 ]\n",
      " [165.05096 ]\n",
      " [144.28943 ]\n",
      " [143.3438  ]\n",
      " [185.55202 ]\n",
      " [152.61723 ]\n",
      " [151.87411 ]\n",
      " [188.39291 ]\n",
      " [144.05481 ]\n",
      " [181.29042 ]\n",
      " [176.59169 ]\n",
      " [158.35928 ]\n",
      " [176.21434 ]\n",
      " [174.47963 ]\n",
      " [167.59795 ]\n",
      " [150.74248 ]\n",
      " [190.2253  ]]\n",
      "1550 Cost:  6.596079 \n",
      "Prediction:\n",
      " [[152.8691 ]\n",
      " [184.43764]\n",
      " [181.0689 ]\n",
      " [198.4439 ]\n",
      " [140.24391]\n",
      " [106.2604 ]\n",
      " [150.86221]\n",
      " [114.86227]\n",
      " [174.50421]\n",
      " [165.03648]\n",
      " [144.28798]\n",
      " [143.33736]\n",
      " [185.55789]\n",
      " [152.62587]\n",
      " [151.87044]\n",
      " [188.38887]\n",
      " [144.06401]\n",
      " [181.2881 ]\n",
      " [176.59914]\n",
      " [158.36632]\n",
      " [176.21178]\n",
      " [174.47542]\n",
      " [167.59709]\n",
      " [150.75508]\n",
      " [190.22612]]\n",
      "1600 Cost:  6.5927997 \n",
      "Prediction:\n",
      " [[152.87457]\n",
      " [184.43571]\n",
      " [181.07164]\n",
      " [198.44394]\n",
      " [140.24277]\n",
      " [106.25395]\n",
      " [150.86017]\n",
      " [114.85813]\n",
      " [174.49683]\n",
      " [165.02231]\n",
      " [144.28653]\n",
      " [143.33102]\n",
      " [185.56363]\n",
      " [152.63431]\n",
      " [151.86679]\n",
      " [188.3849 ]\n",
      " [144.073  ]\n",
      " [181.28577]\n",
      " [176.60637]\n",
      " [158.37317]\n",
      " [176.20926]\n",
      " [174.47127]\n",
      " [167.59625]\n",
      " [150.76736]\n",
      " [190.22687]]\n",
      "1650 Cost:  6.589663 \n",
      "Prediction:\n",
      " [[152.87991 ]\n",
      " [184.43387 ]\n",
      " [181.07433 ]\n",
      " [198.44398 ]\n",
      " [140.24165 ]\n",
      " [106.24765 ]\n",
      " [150.85817 ]\n",
      " [114.854095]\n",
      " [174.48961 ]\n",
      " [165.00845 ]\n",
      " [144.28511 ]\n",
      " [143.32484 ]\n",
      " [185.56926 ]\n",
      " [152.64253 ]\n",
      " [151.86322 ]\n",
      " [188.38103 ]\n",
      " [144.08179 ]\n",
      " [181.2835  ]\n",
      " [176.61348 ]\n",
      " [158.37987 ]\n",
      " [176.20677 ]\n",
      " [174.46722 ]\n",
      " [167.59541 ]\n",
      " [150.77937 ]\n",
      " [190.22765 ]]\n",
      "1700 Cost:  6.586662 \n",
      "Prediction:\n",
      " [[152.88518]\n",
      " [184.43208]\n",
      " [181.077  ]\n",
      " [198.44405]\n",
      " [140.2406 ]\n",
      " [106.2415 ]\n",
      " [150.85625]\n",
      " [114.85014]\n",
      " [174.48257]\n",
      " [164.99492]\n",
      " [144.28374]\n",
      " [143.31883]\n",
      " [185.57478]\n",
      " [152.65062]\n",
      " [151.85976]\n",
      " [188.37727]\n",
      " [144.09041]\n",
      " [181.2813 ]\n",
      " [176.62044]\n",
      " [158.38644]\n",
      " [176.20439]\n",
      " [174.46329]\n",
      " [167.59462]\n",
      " [150.79115]\n",
      " [190.22841]]\n",
      "1750 Cost:  6.583783 \n",
      "Prediction:\n",
      " [[152.89027]\n",
      " [184.43033]\n",
      " [181.07956]\n",
      " [198.44406]\n",
      " [140.23955]\n",
      " [106.23546]\n",
      " [150.85434]\n",
      " [114.84624]\n",
      " [174.47568]\n",
      " [164.98166]\n",
      " [144.28238]\n",
      " [143.31293]\n",
      " [185.58014]\n",
      " [152.65851]\n",
      " [151.85632]\n",
      " [188.37355]\n",
      " [144.09883]\n",
      " [181.27908]\n",
      " [176.6272 ]\n",
      " [158.39285]\n",
      " [176.202  ]\n",
      " [174.45941]\n",
      " [167.59381]\n",
      " [150.80263]\n",
      " [190.22913]]\n",
      "1800 Cost:  6.5810227 \n",
      "Prediction:\n",
      " [[152.89528 ]\n",
      " [184.42862 ]\n",
      " [181.0821  ]\n",
      " [198.44412 ]\n",
      " [140.23854 ]\n",
      " [106.229576]\n",
      " [150.85246 ]\n",
      " [114.84242 ]\n",
      " [174.46896 ]\n",
      " [164.96872 ]\n",
      " [144.28107 ]\n",
      " [143.30716 ]\n",
      " [185.58542 ]\n",
      " [152.66626 ]\n",
      " [151.853   ]\n",
      " [188.36995 ]\n",
      " [144.1071  ]\n",
      " [181.27695 ]\n",
      " [176.63385 ]\n",
      " [158.39914 ]\n",
      " [176.19969 ]\n",
      " [174.45564 ]\n",
      " [167.59305 ]\n",
      " [150.81389 ]\n",
      " [190.22989 ]]\n",
      "1850 Cost:  6.5783954 \n",
      "Prediction:\n",
      " [[152.90018]\n",
      " [184.42696]\n",
      " [181.08458]\n",
      " [198.44417]\n",
      " [140.23756]\n",
      " [106.22383]\n",
      " [150.85062]\n",
      " [114.83869]\n",
      " [174.46239]\n",
      " [164.95605]\n",
      " [144.27977]\n",
      " [143.30151]\n",
      " [185.59058]\n",
      " [152.67381]\n",
      " [151.84973]\n",
      " [188.36641]\n",
      " [144.11519]\n",
      " [181.27484]\n",
      " [176.64037]\n",
      " [158.40526]\n",
      " [176.19742]\n",
      " [174.45197]\n",
      " [167.59227]\n",
      " [150.82487]\n",
      " [190.2306 ]]\n",
      "1900 Cost:  6.575891 \n",
      "Prediction:\n",
      " [[152.90494]\n",
      " [184.42532]\n",
      " [181.087  ]\n",
      " [198.44421]\n",
      " [140.2366 ]\n",
      " [106.21821]\n",
      " [150.84882]\n",
      " [114.83502]\n",
      " [174.45596]\n",
      " [164.94366]\n",
      " [144.2785 ]\n",
      " [143.29599]\n",
      " [185.59561]\n",
      " [152.68121]\n",
      " [151.84653]\n",
      " [188.36296]\n",
      " [144.1231 ]\n",
      " [181.27277]\n",
      " [176.6467 ]\n",
      " [158.41125]\n",
      " [176.19519]\n",
      " [174.44836]\n",
      " [167.59149]\n",
      " [150.83563]\n",
      " [190.2313 ]]\n",
      "1950 Cost:  6.5734572 \n",
      "Prediction:\n",
      " [[152.90962 ]\n",
      " [184.42375 ]\n",
      " [181.08939 ]\n",
      " [198.44426 ]\n",
      " [140.23567 ]\n",
      " [106.212715]\n",
      " [150.84705 ]\n",
      " [114.83142 ]\n",
      " [174.44968 ]\n",
      " [164.93156 ]\n",
      " [144.27725 ]\n",
      " [143.29062 ]\n",
      " [185.60056 ]\n",
      " [152.68848 ]\n",
      " [151.84338 ]\n",
      " [188.35962 ]\n",
      " [144.13086 ]\n",
      " [181.27074 ]\n",
      " [176.65292 ]\n",
      " [158.41714 ]\n",
      " [176.19302 ]\n",
      " [174.44484 ]\n",
      " [167.59076 ]\n",
      " [150.84616 ]\n",
      " [190.23203 ]]\n",
      "2000 Cost:  6.57116 \n",
      "Prediction:\n",
      " [[152.91417]\n",
      " [184.4222 ]\n",
      " [181.09169]\n",
      " [198.44429]\n",
      " [140.23477]\n",
      " [106.20734]\n",
      " [150.84532]\n",
      " [114.82788]\n",
      " [174.44356]\n",
      " [164.91972]\n",
      " [144.27605]\n",
      " [143.28535]\n",
      " [185.6054 ]\n",
      " [152.69554]\n",
      " [151.84032]\n",
      " [188.35631]\n",
      " [144.13844]\n",
      " [181.26872]\n",
      " [176.65901]\n",
      " [158.42287]\n",
      " [176.19087]\n",
      " [174.44139]\n",
      " [167.59003]\n",
      " [150.85646]\n",
      " [190.2327 ]]\n",
      "2050 Cost:  6.568932 \n",
      "Prediction:\n",
      " [[152.91864]\n",
      " [184.4207 ]\n",
      " [181.09395]\n",
      " [198.44432]\n",
      " [140.2339 ]\n",
      " [106.20208]\n",
      " [150.84361]\n",
      " [114.82441]\n",
      " [174.43755]\n",
      " [164.90816]\n",
      " [144.27486]\n",
      " [143.2802 ]\n",
      " [185.61012]\n",
      " [152.7025 ]\n",
      " [151.8373 ]\n",
      " [188.3531 ]\n",
      " [144.14586]\n",
      " [181.26677]\n",
      " [176.66496]\n",
      " [158.42848]\n",
      " [176.18878]\n",
      " [174.43803]\n",
      " [167.58931]\n",
      " [150.86652]\n",
      " [190.23338]]\n",
      "2100 Cost:  6.5668178 \n",
      "Prediction:\n",
      " [[152.92297 ]\n",
      " [184.41922 ]\n",
      " [181.09616 ]\n",
      " [198.44435 ]\n",
      " [140.23303 ]\n",
      " [106.196945]\n",
      " [150.84193 ]\n",
      " [114.821014]\n",
      " [174.43167 ]\n",
      " [164.89682 ]\n",
      " [144.27368 ]\n",
      " [143.27515 ]\n",
      " [185.61472 ]\n",
      " [152.70926 ]\n",
      " [151.83435 ]\n",
      " [188.34995 ]\n",
      " [144.1531  ]\n",
      " [181.26483 ]\n",
      " [176.67076 ]\n",
      " [158.43396 ]\n",
      " [176.18672 ]\n",
      " [174.43474 ]\n",
      " [167.5886  ]\n",
      " [150.87634 ]\n",
      " [190.23404 ]]\n",
      "2150 Cost:  6.5647955 \n",
      "Prediction:\n",
      " [[152.92723 ]\n",
      " [184.41779 ]\n",
      " [181.09831 ]\n",
      " [198.44438 ]\n",
      " [140.2322  ]\n",
      " [106.191925]\n",
      " [150.84029 ]\n",
      " [114.81767 ]\n",
      " [174.42595 ]\n",
      " [164.88574 ]\n",
      " [144.27252 ]\n",
      " [143.27022 ]\n",
      " [185.61925 ]\n",
      " [152.7159  ]\n",
      " [151.83145 ]\n",
      " [188.34686 ]\n",
      " [144.16023 ]\n",
      " [181.26292 ]\n",
      " [176.67644 ]\n",
      " [158.43932 ]\n",
      " [176.18471 ]\n",
      " [174.43152 ]\n",
      " [167.58788 ]\n",
      " [150.88597 ]\n",
      " [190.23468 ]]\n",
      "2200 Cost:  6.562854 \n",
      "Prediction:\n",
      " [[152.93141]\n",
      " [184.4164 ]\n",
      " [181.10045]\n",
      " [198.44444]\n",
      " [140.23141]\n",
      " [106.18703]\n",
      " [150.8387 ]\n",
      " [114.81441]\n",
      " [174.42036]\n",
      " [164.87492]\n",
      " [144.27142]\n",
      " [143.26541]\n",
      " [185.62369]\n",
      " [152.72241]\n",
      " [151.82864]\n",
      " [188.34389]\n",
      " [144.1672 ]\n",
      " [181.26108]\n",
      " [176.68202]\n",
      " [158.44458]\n",
      " [176.18275]\n",
      " [174.42839]\n",
      " [167.5872 ]\n",
      " [150.89539]\n",
      " [190.23534]]\n",
      "2250 Cost:  6.560989 \n",
      "Prediction:\n",
      " [[152.93546 ]\n",
      " [184.41504 ]\n",
      " [181.10251 ]\n",
      " [198.44444 ]\n",
      " [140.23062 ]\n",
      " [106.18223 ]\n",
      " [150.8371  ]\n",
      " [114.811195]\n",
      " [174.41489 ]\n",
      " [164.86433 ]\n",
      " [144.27031 ]\n",
      " [143.26068 ]\n",
      " [185.62799 ]\n",
      " [152.72876 ]\n",
      " [151.82585 ]\n",
      " [188.34094 ]\n",
      " [144.17401 ]\n",
      " [181.25925 ]\n",
      " [176.68745 ]\n",
      " [158.44969 ]\n",
      " [176.1808  ]\n",
      " [174.42531 ]\n",
      " [167.5865  ]\n",
      " [150.90459 ]\n",
      " [190.23596 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2300 Cost:  6.559206 \n",
      "Prediction:\n",
      " [[152.93944]\n",
      " [184.41373]\n",
      " [181.10455]\n",
      " [198.44447]\n",
      " [140.22987]\n",
      " [106.17753]\n",
      " [150.83556]\n",
      " [114.80804]\n",
      " [174.40955]\n",
      " [164.85399]\n",
      " [144.26924]\n",
      " [143.2561 ]\n",
      " [185.63223]\n",
      " [152.73499]\n",
      " [151.82315]\n",
      " [188.33807]\n",
      " [144.1807 ]\n",
      " [181.25745]\n",
      " [176.69278]\n",
      " [158.45471]\n",
      " [176.17892]\n",
      " [174.42233]\n",
      " [167.58585]\n",
      " [150.91359]\n",
      " [190.23659]]\n",
      "2350 Cost:  6.557504 \n",
      "Prediction:\n",
      " [[152.94331]\n",
      " [184.41241]\n",
      " [181.10652]\n",
      " [198.4445 ]\n",
      " [140.22914]\n",
      " [106.17296]\n",
      " [150.83403]\n",
      " [114.80495]\n",
      " [174.40431]\n",
      " [164.84386]\n",
      " [144.26819]\n",
      " [143.2516 ]\n",
      " [185.63638]\n",
      " [152.74106]\n",
      " [151.82048]\n",
      " [188.33527]\n",
      " [144.18724]\n",
      " [181.25568]\n",
      " [176.69798]\n",
      " [158.45961]\n",
      " [176.17705]\n",
      " [174.41939]\n",
      " [167.58519]\n",
      " [150.92241]\n",
      " [190.2372 ]]\n",
      "2400 Cost:  6.5558696 \n",
      "Prediction:\n",
      " [[152.9471 ]\n",
      " [184.41115]\n",
      " [181.10846]\n",
      " [198.44452]\n",
      " [140.22841]\n",
      " [106.16847]\n",
      " [150.83253]\n",
      " [114.8019 ]\n",
      " [174.3992 ]\n",
      " [164.83395]\n",
      " [144.26715]\n",
      " [143.24718]\n",
      " [185.64041]\n",
      " [152.74701]\n",
      " [151.81787]\n",
      " [188.33252]\n",
      " [144.19362]\n",
      " [181.25394]\n",
      " [176.70306]\n",
      " [158.4644 ]\n",
      " [176.17523]\n",
      " [174.4165 ]\n",
      " [167.58453]\n",
      " [150.931  ]\n",
      " [190.2378 ]]\n",
      "2450 Cost:  6.5543013 \n",
      "Prediction:\n",
      " [[152.9508 ]\n",
      " [184.40991]\n",
      " [181.11035]\n",
      " [198.44455]\n",
      " [140.22772]\n",
      " [106.16409]\n",
      " [150.83107]\n",
      " [114.79892]\n",
      " [174.39423]\n",
      " [164.82428]\n",
      " [144.26614]\n",
      " [143.24289]\n",
      " [185.64438]\n",
      " [152.75284]\n",
      " [151.81532]\n",
      " [188.32986]\n",
      " [144.19989]\n",
      " [181.25223]\n",
      " [176.70804]\n",
      " [158.46909]\n",
      " [176.17343]\n",
      " [174.41373]\n",
      " [167.5839 ]\n",
      " [150.9394 ]\n",
      " [190.2384 ]]\n",
      "2500 Cost:  6.552817 \n",
      "Prediction:\n",
      " [[152.95444]\n",
      " [184.40874]\n",
      " [181.11223]\n",
      " [198.44458]\n",
      " [140.22707]\n",
      " [106.15983]\n",
      " [150.82965]\n",
      " [114.79601]\n",
      " [174.38936]\n",
      " [164.81483]\n",
      " [144.26517]\n",
      " [143.2387 ]\n",
      " [185.64828]\n",
      " [152.75854]\n",
      " [151.81284]\n",
      " [188.32726]\n",
      " [144.20604]\n",
      " [181.25056]\n",
      " [176.71294]\n",
      " [158.47368]\n",
      " [176.1717 ]\n",
      " [174.411  ]\n",
      " [167.58327]\n",
      " [150.94765]\n",
      " [190.23898]]\n",
      "2550 Cost:  6.551367 \n",
      "Prediction:\n",
      " [[152.95796 ]\n",
      " [184.40758 ]\n",
      " [181.11403 ]\n",
      " [198.4446  ]\n",
      " [140.22641 ]\n",
      " [106.155624]\n",
      " [150.82823 ]\n",
      " [114.79313 ]\n",
      " [174.3846  ]\n",
      " [164.80557 ]\n",
      " [144.26419 ]\n",
      " [143.23457 ]\n",
      " [185.65208 ]\n",
      " [152.76411 ]\n",
      " [151.81038 ]\n",
      " [188.32469 ]\n",
      " [144.21204 ]\n",
      " [181.24892 ]\n",
      " [176.7177  ]\n",
      " [158.47816 ]\n",
      " [176.16998 ]\n",
      " [174.40831 ]\n",
      " [167.58266 ]\n",
      " [150.9557  ]\n",
      " [190.23955 ]]\n",
      "2600 Cost:  6.550001 \n",
      "Prediction:\n",
      " [[152.96143 ]\n",
      " [184.40642 ]\n",
      " [181.11578 ]\n",
      " [198.44461 ]\n",
      " [140.22577 ]\n",
      " [106.151535]\n",
      " [150.82683 ]\n",
      " [114.790306]\n",
      " [174.37991 ]\n",
      " [164.79651 ]\n",
      " [144.26324 ]\n",
      " [143.23053 ]\n",
      " [185.65578 ]\n",
      " [152.76956 ]\n",
      " [151.80798 ]\n",
      " [188.32219 ]\n",
      " [144.2179  ]\n",
      " [181.24728 ]\n",
      " [176.72235 ]\n",
      " [158.48253 ]\n",
      " [176.16827 ]\n",
      " [174.4057  ]\n",
      " [167.58203 ]\n",
      " [150.96356 ]\n",
      " [190.24011 ]]\n",
      "2650 Cost:  6.54868 \n",
      "Prediction:\n",
      " [[152.96481 ]\n",
      " [184.40532 ]\n",
      " [181.11755 ]\n",
      " [198.44464 ]\n",
      " [140.22516 ]\n",
      " [106.147545]\n",
      " [150.82549 ]\n",
      " [114.787544]\n",
      " [174.37538 ]\n",
      " [164.78767 ]\n",
      " [144.26231 ]\n",
      " [143.22661 ]\n",
      " [185.65942 ]\n",
      " [152.77493 ]\n",
      " [151.80565 ]\n",
      " [188.31976 ]\n",
      " [144.22368 ]\n",
      " [181.2457  ]\n",
      " [176.72691 ]\n",
      " [158.48683 ]\n",
      " [176.16666 ]\n",
      " [174.40317 ]\n",
      " [167.58145 ]\n",
      " [150.97128 ]\n",
      " [190.24069 ]]\n",
      "2700 Cost:  6.547417 \n",
      "Prediction:\n",
      " [[152.9681 ]\n",
      " [184.40424]\n",
      " [181.11925]\n",
      " [198.44466]\n",
      " [140.22456]\n",
      " [106.14364]\n",
      " [150.82414]\n",
      " [114.78484]\n",
      " [174.37094]\n",
      " [164.77904]\n",
      " [144.2614 ]\n",
      " [143.22276]\n",
      " [185.66298]\n",
      " [152.78015]\n",
      " [151.80334]\n",
      " [188.31738]\n",
      " [144.22931]\n",
      " [181.24413]\n",
      " [176.73137]\n",
      " [158.49101]\n",
      " [176.16505]\n",
      " [174.40065]\n",
      " [167.58086]\n",
      " [150.9788 ]\n",
      " [190.24124]]\n",
      "2750 Cost:  6.5462217 \n",
      "Prediction:\n",
      " [[152.97131 ]\n",
      " [184.40317 ]\n",
      " [181.12091 ]\n",
      " [198.44467 ]\n",
      " [140.22397 ]\n",
      " [106.139824]\n",
      " [150.82281 ]\n",
      " [114.78217 ]\n",
      " [174.36658 ]\n",
      " [164.77055 ]\n",
      " [144.2605  ]\n",
      " [143.219   ]\n",
      " [185.66644 ]\n",
      " [152.78525 ]\n",
      " [151.80109 ]\n",
      " [188.31505 ]\n",
      " [144.2348  ]\n",
      " [181.24258 ]\n",
      " [176.73572 ]\n",
      " [158.4951  ]\n",
      " [176.16345 ]\n",
      " [174.39821 ]\n",
      " [167.58026 ]\n",
      " [150.98616 ]\n",
      " [190.24176 ]]\n",
      "2800 Cost:  6.545061 \n",
      "Prediction:\n",
      " [[152.97449 ]\n",
      " [184.40216 ]\n",
      " [181.12253 ]\n",
      " [198.4447  ]\n",
      " [140.22343 ]\n",
      " [106.136086]\n",
      " [150.82156 ]\n",
      " [114.779564]\n",
      " [174.36235 ]\n",
      " [164.7623  ]\n",
      " [144.25964 ]\n",
      " [143.21535 ]\n",
      " [185.66986 ]\n",
      " [152.79028 ]\n",
      " [151.79889 ]\n",
      " [188.31277 ]\n",
      " [144.24023 ]\n",
      " [181.24109 ]\n",
      " [176.74    ]\n",
      " [158.49911 ]\n",
      " [176.1619  ]\n",
      " [174.39584 ]\n",
      " [167.57971 ]\n",
      " [150.99338 ]\n",
      " [190.24231 ]]\n",
      "2850 Cost:  6.5439477 \n",
      "Prediction:\n",
      " [[152.97755]\n",
      " [184.40115]\n",
      " [181.12411]\n",
      " [198.44472]\n",
      " [140.22289]\n",
      " [106.13243]\n",
      " [150.8203 ]\n",
      " [114.77699]\n",
      " [174.3582 ]\n",
      " [164.7542 ]\n",
      " [144.25877]\n",
      " [143.21175]\n",
      " [185.67317]\n",
      " [152.79517]\n",
      " [151.7967 ]\n",
      " [188.31056]\n",
      " [144.2455 ]\n",
      " [181.23961]\n",
      " [176.74416]\n",
      " [158.50304]\n",
      " [176.16039]\n",
      " [174.39351]\n",
      " [167.57913]\n",
      " [151.00041]\n",
      " [190.24281]]\n",
      "2900 Cost:  6.5428853 \n",
      "Prediction:\n",
      " [[152.98058]\n",
      " [184.40019]\n",
      " [181.12569]\n",
      " [198.44473]\n",
      " [140.22237]\n",
      " [106.12887]\n",
      " [150.81906]\n",
      " [114.77448]\n",
      " [174.35414]\n",
      " [164.74629]\n",
      " [144.25795]\n",
      " [143.20824]\n",
      " [185.67645]\n",
      " [152.79997]\n",
      " [151.7946 ]\n",
      " [188.3084 ]\n",
      " [144.2507 ]\n",
      " [181.23816]\n",
      " [176.74825]\n",
      " [158.50688]\n",
      " [176.15889]\n",
      " [174.39125]\n",
      " [167.57858]\n",
      " [151.00731]\n",
      " [190.24336]]\n",
      "2950 Cost:  6.5418606 \n",
      "Prediction:\n",
      " [[152.98349]\n",
      " [184.39923]\n",
      " [181.1272 ]\n",
      " [198.44473]\n",
      " [140.22185]\n",
      " [106.12538]\n",
      " [150.81784]\n",
      " [114.77202]\n",
      " [174.35019]\n",
      " [164.73856]\n",
      " [144.25713]\n",
      " [143.2048 ]\n",
      " [185.67963]\n",
      " [152.80466]\n",
      " [151.79251]\n",
      " [188.30627]\n",
      " [144.25577]\n",
      " [181.23672]\n",
      " [176.75223]\n",
      " [158.51062]\n",
      " [176.15744]\n",
      " [174.389  ]\n",
      " [167.57803]\n",
      " [151.01404]\n",
      " [190.24385]]\n",
      "3000 Cost:  6.5408936 \n",
      "Prediction:\n",
      " [[152.98637]\n",
      " [184.39832]\n",
      " [181.12868]\n",
      " [198.44475]\n",
      " [140.22136]\n",
      " [106.12198]\n",
      " [150.81664]\n",
      " [114.76959]\n",
      " [174.34631]\n",
      " [164.731  ]\n",
      " [144.25632]\n",
      " [143.20145]\n",
      " [185.68275]\n",
      " [152.80925]\n",
      " [151.79048]\n",
      " [188.3042 ]\n",
      " [144.26073]\n",
      " [181.2353 ]\n",
      " [176.75615]\n",
      " [158.51427]\n",
      " [176.15599]\n",
      " [174.38683]\n",
      " [167.5775 ]\n",
      " [151.02063]\n",
      " [190.24434]]\n",
      "Your score will be  [[186.76888]]\n",
      "Other scores will be  [[183.04027]\n",
      " [174.7126 ]]\n"
     ]
    }
   ],
   "source": [
    "# Multi-variable linear regression\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(123)  # for reproducibility\n",
    "\n",
    "xy = np.loadtxt('data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# Make sure the shape and data are OK\n",
    "print(x_data.shape, x_data, len(x_data))\n",
    "print(y_data.shape, y_data)\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize : learning_rate=1e-5, 2000 Cost = 64.67786   /  lr = 0.001  : 2000 cost = nan  / .00001 : 3000 cost = 6.54\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.00001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(3001):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 50 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "\n",
    "# Ask my score\n",
    "print(\"Your score will be \", sess.run(\n",
    "    hypothesis, feed_dict={X: [[100, 70, 101]]}))\n",
    "\n",
    "print(\"Other scores will be \", sess.run(hypothesis,\n",
    "                                        feed_dict={X: [[60, 70, 110], [90, 100, 80]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-0711e24e32ce>:10: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From c:\\users\\beomc\\anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From c:\\users\\beomc\\anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From c:\\users\\beomc\\anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From c:\\users\\beomc\\anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-22-0711e24e32ce>:12: TextLineReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TextLineDataset`.\n",
      "WARNING:tensorflow:From <ipython-input-22-0711e24e32ce>:22: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From <ipython-input-22-0711e24e32ce>:48: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "0 Cost:  6664.8486 \n",
      "Prediction:\n",
      " [[58.513256]\n",
      " [98.22161 ]\n",
      " [82.36636 ]\n",
      " [90.40746 ]\n",
      " [81.220024]\n",
      " [64.13798 ]\n",
      " [60.82174 ]\n",
      " [34.706482]\n",
      " [99.07922 ]\n",
      " [95.76013 ]]\n",
      "10 Cost:  192.38693 \n",
      "Prediction:\n",
      " [[140.66318]\n",
      " [196.8898 ]\n",
      " [179.621  ]\n",
      " [196.32507]\n",
      " [156.45364]\n",
      " [119.59655]\n",
      " [140.1239 ]\n",
      " [ 93.525  ]\n",
      " [191.27571]\n",
      " [181.15022]]\n",
      "20 Cost:  196.09026 \n",
      "Prediction:\n",
      " [[141.25703 ]\n",
      " [197.51962 ]\n",
      " [180.28508 ]\n",
      " [197.04503 ]\n",
      " [156.91583 ]\n",
      " [119.922455]\n",
      " [140.6815  ]\n",
      " [ 93.967026]\n",
      " [191.84084 ]\n",
      " [181.65895 ]]\n",
      "30 Cost:  194.97357 \n",
      "Prediction:\n",
      " [[141.30043]\n",
      " [197.48802]\n",
      " [180.29738]\n",
      " [197.05516]\n",
      " [156.87361]\n",
      " [119.87651]\n",
      " [140.70775]\n",
      " [ 94.01502]\n",
      " [191.7879 ]\n",
      " [181.5952 ]]\n",
      "40 Cost:  193.82901 \n",
      "Prediction:\n",
      " [[141.3399 ]\n",
      " [197.452  ]\n",
      " [180.30518]\n",
      " [197.0604 ]\n",
      " [156.82806]\n",
      " [119.8282 ]\n",
      " [140.73029]\n",
      " [ 94.06025]\n",
      " [191.73093]\n",
      " [181.52783]]\n",
      "50 Cost:  192.69162 \n",
      "Prediction:\n",
      " [[141.37926]\n",
      " [197.41609]\n",
      " [180.31293]\n",
      " [197.06564]\n",
      " [156.78265]\n",
      " [119.78006]\n",
      " [140.7528 ]\n",
      " [ 94.1054 ]\n",
      " [191.67416]\n",
      " [181.46074]]\n",
      "60 Cost:  191.5615 \n",
      "Prediction:\n",
      " [[141.41846]\n",
      " [197.3803 ]\n",
      " [180.32063]\n",
      " [197.07085]\n",
      " [156.73738]\n",
      " [119.7321 ]\n",
      " [140.77528]\n",
      " [ 94.15045]\n",
      " [191.6176 ]\n",
      " [181.39392]]\n",
      "70 Cost:  190.43863 \n",
      "Prediction:\n",
      " [[141.45753]\n",
      " [197.34462]\n",
      " [180.32832]\n",
      " [197.07608]\n",
      " [156.69223]\n",
      " [119.6843 ]\n",
      " [140.79771]\n",
      " [ 94.19541]\n",
      " [191.56126]\n",
      " [181.3274 ]]\n",
      "80 Cost:  189.3227 \n",
      "Prediction:\n",
      " [[141.49643]\n",
      " [197.30904]\n",
      " [180.33597]\n",
      " [197.08128]\n",
      " [156.64719]\n",
      " [119.63669]\n",
      " [140.82008]\n",
      " [ 94.24029]\n",
      " [191.5051 ]\n",
      " [181.26112]]\n",
      "90 Cost:  188.21394 \n",
      "Prediction:\n",
      " [[141.53522 ]\n",
      " [197.27359 ]\n",
      " [180.34358 ]\n",
      " [197.08646 ]\n",
      " [156.6023  ]\n",
      " [119.589226]\n",
      " [140.84244 ]\n",
      " [ 94.28506 ]\n",
      " [191.44914 ]\n",
      " [181.19514 ]]\n",
      "100 Cost:  187.11209 \n",
      "Prediction:\n",
      " [[141.57385 ]\n",
      " [197.23822 ]\n",
      " [180.35114 ]\n",
      " [197.09164 ]\n",
      " [156.55753 ]\n",
      " [119.541954]\n",
      " [140.86473 ]\n",
      " [ 94.32977 ]\n",
      " [191.39339 ]\n",
      " [181.12944 ]]\n",
      "110 Cost:  186.01746 \n",
      "Prediction:\n",
      " [[141.61237]\n",
      " [197.20299]\n",
      " [180.35866]\n",
      " [197.09683]\n",
      " [156.5129 ]\n",
      " [119.49485]\n",
      " [140.88698]\n",
      " [ 94.37437]\n",
      " [191.33784]\n",
      " [181.06401]]\n",
      "120 Cost:  184.92935 \n",
      "Prediction:\n",
      " [[141.65073]\n",
      " [197.16783]\n",
      " [180.36617]\n",
      " [197.10196]\n",
      " [156.46837]\n",
      " [119.44791]\n",
      " [140.9092 ]\n",
      " [ 94.41889]\n",
      " [191.28247]\n",
      " [180.99884]]\n",
      "130 Cost:  183.84866 \n",
      "Prediction:\n",
      " [[141.68896]\n",
      " [197.13283]\n",
      " [180.37364]\n",
      " [197.10713]\n",
      " [156.424  ]\n",
      " [119.40116]\n",
      " [140.93138]\n",
      " [ 94.46333]\n",
      " [191.22736]\n",
      " [180.93398]]\n",
      "140 Cost:  182.77469 \n",
      "Prediction:\n",
      " [[141.72707]\n",
      " [197.09792]\n",
      " [180.38106]\n",
      " [197.11229]\n",
      " [156.37975]\n",
      " [119.35458]\n",
      " [140.9535 ]\n",
      " [ 94.50768]\n",
      " [191.17242]\n",
      " [180.86937]]\n",
      "150 Cost:  181.7072 \n",
      "Prediction:\n",
      " [[141.76505 ]\n",
      " [197.0631  ]\n",
      " [180.38847 ]\n",
      " [197.1174  ]\n",
      " [156.33562 ]\n",
      " [119.308136]\n",
      " [140.9756  ]\n",
      " [ 94.55193 ]\n",
      " [191.11766 ]\n",
      " [180.80502 ]]\n",
      "160 Cost:  180.6467 \n",
      "Prediction:\n",
      " [[141.80286]\n",
      " [197.0284 ]\n",
      " [180.39581]\n",
      " [197.1225 ]\n",
      " [156.29161]\n",
      " [119.26189]\n",
      " [140.99762]\n",
      " [ 94.5961 ]\n",
      " [191.06311]\n",
      " [180.74094]]\n",
      "170 Cost:  179.59296 \n",
      "Prediction:\n",
      " [[141.84058]\n",
      " [196.99382]\n",
      " [180.40314]\n",
      " [197.12764]\n",
      " [156.24776]\n",
      " [119.2158 ]\n",
      " [141.01964]\n",
      " [ 94.6402 ]\n",
      " [191.00877]\n",
      " [180.67715]]\n",
      "180 Cost:  178.54584 \n",
      "Prediction:\n",
      " [[141.87813 ]\n",
      " [196.95935 ]\n",
      " [180.41045 ]\n",
      " [197.13275 ]\n",
      " [156.20401 ]\n",
      " [119.16988 ]\n",
      " [141.0416  ]\n",
      " [ 94.684204]\n",
      " [190.95464 ]\n",
      " [180.6136  ]]\n",
      "190 Cost:  177.50537 \n",
      "Prediction:\n",
      " [[141.91556]\n",
      " [196.92497]\n",
      " [180.41771]\n",
      " [197.13783]\n",
      " [156.16039]\n",
      " [119.12413]\n",
      " [141.06352]\n",
      " [ 94.72811]\n",
      " [190.90068]\n",
      " [180.55035]]\n",
      "200 Cost:  176.47122 \n",
      "Prediction:\n",
      " [[141.95285 ]\n",
      " [196.89069 ]\n",
      " [180.42491 ]\n",
      " [197.14291 ]\n",
      " [156.11688 ]\n",
      " [119.07854 ]\n",
      " [141.08539 ]\n",
      " [ 94.771935]\n",
      " [190.8469  ]\n",
      " [180.48734 ]]\n",
      "210 Cost:  175.44382 \n",
      "Prediction:\n",
      " [[141.98999 ]\n",
      " [196.85654 ]\n",
      " [180.4321  ]\n",
      " [197.14798 ]\n",
      " [156.0735  ]\n",
      " [119.033104]\n",
      " [141.10724 ]\n",
      " [ 94.81567 ]\n",
      " [190.79333 ]\n",
      " [180.42459 ]]\n",
      "220 Cost:  174.42278 \n",
      "Prediction:\n",
      " [[142.02702 ]\n",
      " [196.82246 ]\n",
      " [180.43925 ]\n",
      " [197.15303 ]\n",
      " [156.03026 ]\n",
      " [118.987854]\n",
      " [141.12903 ]\n",
      " [ 94.85932 ]\n",
      " [190.73994 ]\n",
      " [180.3621  ]]\n",
      "230 Cost:  173.40836 \n",
      "Prediction:\n",
      " [[142.06393 ]\n",
      " [196.78853 ]\n",
      " [180.44637 ]\n",
      " [197.1581  ]\n",
      " [155.98715 ]\n",
      " [118.94276 ]\n",
      " [141.15079 ]\n",
      " [ 94.902885]\n",
      " [190.68677 ]\n",
      " [180.2999  ]]\n",
      "240 Cost:  172.40018 \n",
      "Prediction:\n",
      " [[142.10071 ]\n",
      " [196.75468 ]\n",
      " [180.45346 ]\n",
      " [197.16315 ]\n",
      " [155.94414 ]\n",
      " [118.89781 ]\n",
      " [141.17249 ]\n",
      " [ 94.946365]\n",
      " [190.63379 ]\n",
      " [180.23793 ]]\n",
      "250 Cost:  171.39859 \n",
      "Prediction:\n",
      " [[142.13736]\n",
      " [196.72098]\n",
      " [180.46054]\n",
      " [197.16818]\n",
      " [155.90128]\n",
      " [118.85305]\n",
      " [141.19418]\n",
      " [ 94.98977]\n",
      " [190.58101]\n",
      " [180.17625]]\n",
      "260 Cost:  170.40315 \n",
      "Prediction:\n",
      " [[142.17386 ]\n",
      " [196.68733 ]\n",
      " [180.46754 ]\n",
      " [197.1732  ]\n",
      " [155.85852 ]\n",
      " [118.80846 ]\n",
      " [141.21579 ]\n",
      " [ 95.033066]\n",
      " [190.52838 ]\n",
      " [180.1148  ]]\n",
      "270 Cost:  169.41379 \n",
      "Prediction:\n",
      " [[142.21024 ]\n",
      " [196.6538  ]\n",
      " [180.47452 ]\n",
      " [197.1782  ]\n",
      " [155.81589 ]\n",
      " [118.76399 ]\n",
      " [141.23738 ]\n",
      " [ 95.076294]\n",
      " [190.47597 ]\n",
      " [180.0536  ]]\n",
      "280 Cost:  168.43085 \n",
      "Prediction:\n",
      " [[142.24648]\n",
      " [196.62036]\n",
      " [180.48148]\n",
      " [197.1832 ]\n",
      " [155.77338]\n",
      " [118.71971]\n",
      " [141.25893]\n",
      " [ 95.11943]\n",
      " [190.42374]\n",
      " [179.99268]]\n",
      "290 Cost:  167.45439 \n",
      "Prediction:\n",
      " [[142.28262]\n",
      " [196.58707]\n",
      " [180.4884 ]\n",
      " [197.18823]\n",
      " [155.73102]\n",
      " [118.67559]\n",
      " [141.28046]\n",
      " [ 95.16248]\n",
      " [190.37173]\n",
      " [179.93204]]\n",
      "300 Cost:  166.48363 \n",
      "Prediction:\n",
      " [[142.31859]\n",
      " [196.55383]\n",
      " [180.49529]\n",
      " [197.19319]\n",
      " [155.68874]\n",
      " [118.63162]\n",
      " [141.3019 ]\n",
      " [ 95.20543]\n",
      " [190.31984]\n",
      " [179.8716 ]]\n",
      "310 Cost:  165.51921 \n",
      "Prediction:\n",
      " [[142.35448 ]\n",
      " [196.52074 ]\n",
      " [180.50215 ]\n",
      " [197.19817 ]\n",
      " [155.6466  ]\n",
      " [118.587814]\n",
      " [141.32332 ]\n",
      " [ 95.24832 ]\n",
      " [190.26819 ]\n",
      " [179.81145 ]]\n",
      "320 Cost:  164.56067 \n",
      "Prediction:\n",
      " [[142.39021]\n",
      " [196.48772]\n",
      " [180.50897]\n",
      " [197.20312]\n",
      " [155.60458]\n",
      " [118.54417]\n",
      " [141.34471]\n",
      " [ 95.29112]\n",
      " [190.2167 ]\n",
      " [179.75153]]\n",
      "330 Cost:  163.60812 \n",
      "Prediction:\n",
      " [[142.42583 ]\n",
      " [196.4548  ]\n",
      " [180.51575 ]\n",
      " [197.20807 ]\n",
      " [155.56267 ]\n",
      " [118.500656]\n",
      " [141.36604 ]\n",
      " [ 95.33381 ]\n",
      " [190.1654  ]\n",
      " [179.69183 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340 Cost:  162.66173 \n",
      "Prediction:\n",
      " [[142.4613 ]\n",
      " [196.422  ]\n",
      " [180.52252]\n",
      " [197.21303]\n",
      " [155.52089]\n",
      " [118.45733]\n",
      " [141.38734]\n",
      " [ 95.37645]\n",
      " [190.1143 ]\n",
      " [179.63243]]\n",
      "350 Cost:  161.7214 \n",
      "Prediction:\n",
      " [[142.49669 ]\n",
      " [196.3893  ]\n",
      " [180.52925 ]\n",
      " [197.21797 ]\n",
      " [155.47925 ]\n",
      " [118.414154]\n",
      " [141.40862 ]\n",
      " [ 95.41898 ]\n",
      " [190.0634  ]\n",
      " [179.57329 ]]\n",
      "360 Cost:  160.7866 \n",
      "Prediction:\n",
      " [[142.53192]\n",
      " [196.35669]\n",
      " [180.53595]\n",
      " [197.2229 ]\n",
      " [155.4377 ]\n",
      " [118.37112]\n",
      " [141.42982]\n",
      " [ 95.46145]\n",
      " [190.01265]\n",
      " [179.51434]]\n",
      "370 Cost:  159.85803 \n",
      "Prediction:\n",
      " [[142.56705 ]\n",
      " [196.3242  ]\n",
      " [180.5426  ]\n",
      " [197.2278  ]\n",
      " [155.39629 ]\n",
      " [118.328255]\n",
      " [141.451   ]\n",
      " [ 95.5038  ]\n",
      " [189.9621  ]\n",
      " [179.45567 ]]\n",
      "380 Cost:  158.93509 \n",
      "Prediction:\n",
      " [[142.60204 ]\n",
      " [196.2918  ]\n",
      " [180.54924 ]\n",
      " [197.23271 ]\n",
      " [155.35498 ]\n",
      " [118.285545]\n",
      " [141.47214 ]\n",
      " [ 95.54609 ]\n",
      " [189.9117  ]\n",
      " [179.39726 ]]\n",
      "390 Cost:  158.01804 \n",
      "Prediction:\n",
      " [[142.6369 ]\n",
      " [196.2595 ]\n",
      " [180.55586]\n",
      " [197.23763]\n",
      " [155.3138 ]\n",
      " [118.24297]\n",
      " [141.49324]\n",
      " [ 95.58828]\n",
      " [189.86153]\n",
      " [179.33907]]\n",
      "400 Cost:  157.10684 \n",
      "Prediction:\n",
      " [[142.67166 ]\n",
      " [196.22733 ]\n",
      " [180.56245 ]\n",
      " [197.24251 ]\n",
      " [155.27275 ]\n",
      " [118.20057 ]\n",
      " [141.5143  ]\n",
      " [ 95.630394]\n",
      " [189.81152 ]\n",
      " [179.28114 ]]\n",
      "410 Cost:  156.20093 \n",
      "Prediction:\n",
      " [[142.70625 ]\n",
      " [196.19519 ]\n",
      " [180.56895 ]\n",
      " [197.24736 ]\n",
      " [155.23177 ]\n",
      " [118.158295]\n",
      " [141.53528 ]\n",
      " [ 95.67241 ]\n",
      " [189.76166 ]\n",
      " [179.22342 ]]\n",
      "420 Cost:  155.30095 \n",
      "Prediction:\n",
      " [[142.74078 ]\n",
      " [196.16321 ]\n",
      " [180.57547 ]\n",
      " [197.25226 ]\n",
      " [155.19095 ]\n",
      " [118.1162  ]\n",
      " [141.55627 ]\n",
      " [ 95.714355]\n",
      " [189.712   ]\n",
      " [179.16595 ]]\n",
      "430 Cost:  154.40672 \n",
      "Prediction:\n",
      " [[142.77516 ]\n",
      " [196.13132 ]\n",
      " [180.58197 ]\n",
      " [197.25713 ]\n",
      " [155.15024 ]\n",
      " [118.074265]\n",
      " [141.57721 ]\n",
      " [ 95.75623 ]\n",
      " [189.66255 ]\n",
      " [179.10875 ]]\n",
      "440 Cost:  153.51782 \n",
      "Prediction:\n",
      " [[142.8094 ]\n",
      " [196.0995 ]\n",
      " [180.5884 ]\n",
      " [197.26195]\n",
      " [155.10963]\n",
      " [118.03245]\n",
      " [141.5981 ]\n",
      " [ 95.798  ]\n",
      " [189.61324]\n",
      " [179.05174]]\n",
      "450 Cost:  152.63464 \n",
      "Prediction:\n",
      " [[142.84354]\n",
      " [196.06781]\n",
      " [180.59483]\n",
      " [197.2668 ]\n",
      " [155.06914]\n",
      " [117.9908 ]\n",
      " [141.61896]\n",
      " [ 95.83971]\n",
      " [189.56412]\n",
      " [178.99503]]\n",
      "460 Cost:  151.757 \n",
      "Prediction:\n",
      " [[142.87756 ]\n",
      " [196.0362  ]\n",
      " [180.60121 ]\n",
      " [197.27162 ]\n",
      " [155.02878 ]\n",
      " [117.9493  ]\n",
      " [141.63976 ]\n",
      " [ 95.881294]\n",
      " [189.51517 ]\n",
      " [178.9385  ]]\n",
      "470 Cost:  150.88464 \n",
      "Prediction:\n",
      " [[142.91145 ]\n",
      " [196.00468 ]\n",
      " [180.60757 ]\n",
      " [197.27644 ]\n",
      " [154.98853 ]\n",
      " [117.907936]\n",
      " [141.66052 ]\n",
      " [ 95.92283 ]\n",
      " [189.46638 ]\n",
      " [178.88222 ]]\n",
      "480 Cost:  150.01799 \n",
      "Prediction:\n",
      " [[142.94525 ]\n",
      " [195.97327 ]\n",
      " [180.6139  ]\n",
      " [197.28127 ]\n",
      " [154.9484  ]\n",
      " [117.866745]\n",
      " [141.68126 ]\n",
      " [ 95.964264]\n",
      " [189.4178  ]\n",
      " [178.8262  ]]\n",
      "490 Cost:  149.15657 \n",
      "Prediction:\n",
      " [[142.97888 ]\n",
      " [195.94194 ]\n",
      " [180.6202  ]\n",
      " [197.28604 ]\n",
      " [154.90836 ]\n",
      " [117.825676]\n",
      " [141.70193 ]\n",
      " [ 96.00562 ]\n",
      " [189.36938 ]\n",
      " [178.77039 ]]\n",
      "500 Cost:  148.30061 \n",
      "Prediction:\n",
      " [[143.01244 ]\n",
      " [195.91074 ]\n",
      " [180.62646 ]\n",
      " [197.29083 ]\n",
      " [154.86847 ]\n",
      " [117.78477 ]\n",
      " [141.7226  ]\n",
      " [ 96.046906]\n",
      " [189.32114 ]\n",
      " [178.71481 ]]\n",
      "510 Cost:  147.44992 \n",
      "Prediction:\n",
      " [[143.04584]\n",
      " [195.8796 ]\n",
      " [180.6327 ]\n",
      " [197.29561]\n",
      " [154.82864]\n",
      " [117.74401]\n",
      " [141.74318]\n",
      " [ 96.0881 ]\n",
      " [189.27306]\n",
      " [178.65947]]\n",
      "520 Cost:  146.60452 \n",
      "Prediction:\n",
      " [[143.07916 ]\n",
      " [195.84857 ]\n",
      " [180.63892 ]\n",
      " [197.3004  ]\n",
      " [154.78899 ]\n",
      " [117.703384]\n",
      " [141.76378 ]\n",
      " [ 96.1292  ]\n",
      " [189.22514 ]\n",
      " [178.60435 ]]\n",
      "530 Cost:  145.76456 \n",
      "Prediction:\n",
      " [[143.11237 ]\n",
      " [195.81767 ]\n",
      " [180.64513 ]\n",
      " [197.30516 ]\n",
      " [154.74942 ]\n",
      " [117.66291 ]\n",
      " [141.78432 ]\n",
      " [ 96.170235]\n",
      " [189.17744 ]\n",
      " [178.54948 ]]\n",
      "540 Cost:  144.92975 \n",
      "Prediction:\n",
      " [[143.14543]\n",
      " [195.78683]\n",
      " [180.65126]\n",
      " [197.30992]\n",
      " [154.70996]\n",
      " [117.6226 ]\n",
      " [141.80481]\n",
      " [ 96.21118]\n",
      " [189.12988]\n",
      " [178.49484]]\n",
      "550 Cost:  144.1 \n",
      "Prediction:\n",
      " [[143.17839]\n",
      " [195.75607]\n",
      " [180.65738]\n",
      " [197.31465]\n",
      " [154.67062]\n",
      " [117.5824 ]\n",
      " [141.82524]\n",
      " [ 96.25202]\n",
      " [189.08246]\n",
      " [178.44041]]\n",
      "560 Cost:  143.2756 \n",
      "Prediction:\n",
      " [[143.21123 ]\n",
      " [195.72545 ]\n",
      " [180.66347 ]\n",
      " [197.31938 ]\n",
      " [154.6314  ]\n",
      " [117.54237 ]\n",
      " [141.84566 ]\n",
      " [ 96.292816]\n",
      " [189.03526 ]\n",
      " [178.38622 ]]\n",
      "570 Cost:  142.45628 \n",
      "Prediction:\n",
      " [[143.24397]\n",
      " [195.6949 ]\n",
      " [180.66954]\n",
      " [197.32413]\n",
      " [154.59229]\n",
      " [117.50248]\n",
      " [141.86603]\n",
      " [ 96.33351]\n",
      " [188.98822]\n",
      " [178.33224]]\n",
      "580 Cost:  141.642 \n",
      "Prediction:\n",
      " [[143.27655]\n",
      " [195.66443]\n",
      " [180.67557]\n",
      " [197.32883]\n",
      " [154.55327]\n",
      " [117.46273]\n",
      " [141.88632]\n",
      " [ 96.37412]\n",
      " [188.94131]\n",
      " [178.27849]]\n",
      "590 Cost:  140.83276 \n",
      "Prediction:\n",
      " [[143.30905]\n",
      " [195.63406]\n",
      " [180.68158]\n",
      " [197.33353]\n",
      " [154.51437]\n",
      " [117.42311]\n",
      " [141.90663]\n",
      " [ 96.41465]\n",
      " [188.89459]\n",
      " [178.22498]]\n",
      "600 Cost:  140.02847 \n",
      "Prediction:\n",
      " [[143.34145 ]\n",
      " [195.60379 ]\n",
      " [180.68755 ]\n",
      " [197.33821 ]\n",
      " [154.47557 ]\n",
      " [117.383644]\n",
      " [141.92686 ]\n",
      " [ 96.4551  ]\n",
      " [188.84805 ]\n",
      " [178.17165 ]]\n",
      "610 Cost:  139.22952 \n",
      "Prediction:\n",
      " [[143.37372]\n",
      " [195.57364]\n",
      " [180.69353]\n",
      " [197.34294]\n",
      " [154.43692]\n",
      " [117.34432]\n",
      " [141.9471 ]\n",
      " [ 96.49547]\n",
      " [188.80168]\n",
      " [178.11859]]\n",
      "620 Cost:  138.43529 \n",
      "Prediction:\n",
      " [[143.40588 ]\n",
      " [195.54355 ]\n",
      " [180.69943 ]\n",
      " [197.34761 ]\n",
      " [154.39836 ]\n",
      " [117.305145]\n",
      " [141.96725 ]\n",
      " [ 96.53576 ]\n",
      " [188.75546 ]\n",
      " [178.06573 ]]\n",
      "630 Cost:  137.64592 \n",
      "Prediction:\n",
      " [[143.43791]\n",
      " [195.51353]\n",
      " [180.70532]\n",
      " [197.35225]\n",
      " [154.3599 ]\n",
      " [117.26608]\n",
      " [141.98738]\n",
      " [ 96.57594]\n",
      " [188.7094 ]\n",
      " [178.01309]]\n",
      "640 Cost:  136.8616 \n",
      "Prediction:\n",
      " [[143.46986 ]\n",
      " [195.48367 ]\n",
      " [180.7112  ]\n",
      " [197.35693 ]\n",
      " [154.32156 ]\n",
      " [117.22718 ]\n",
      " [142.00748 ]\n",
      " [ 96.616066]\n",
      " [188.66351 ]\n",
      " [177.96066 ]]\n",
      "650 Cost:  136.08212 \n",
      "Prediction:\n",
      " [[143.5017 ]\n",
      " [195.45386]\n",
      " [180.71706]\n",
      " [197.36159]\n",
      " [154.28333]\n",
      " [117.18842]\n",
      " [142.02753]\n",
      " [ 96.65611]\n",
      " [188.61781]\n",
      " [177.90846]]\n",
      "660 Cost:  135.3074 \n",
      "Prediction:\n",
      " [[143.53339]\n",
      " [195.42412]\n",
      " [180.72285]\n",
      " [197.36623]\n",
      " [154.2452 ]\n",
      " [117.14979]\n",
      " [142.04755]\n",
      " [ 96.69606]\n",
      " [188.57224]\n",
      " [177.85649]]\n",
      "670 Cost:  134.53743 \n",
      "Prediction:\n",
      " [[143.56497]\n",
      " [195.3945 ]\n",
      " [180.72862]\n",
      " [197.37085]\n",
      " [154.20717]\n",
      " [117.11129]\n",
      " [142.0675 ]\n",
      " [ 96.73593]\n",
      " [188.52682]\n",
      " [177.80469]]\n",
      "680 Cost:  133.77257 \n",
      "Prediction:\n",
      " [[143.59648]\n",
      " [195.36499]\n",
      " [180.7344 ]\n",
      " [197.37547]\n",
      " [154.16928]\n",
      " [117.07295]\n",
      " [142.08745]\n",
      " [ 96.77574]\n",
      " [188.48163]\n",
      " [177.75317]]\n",
      "690 Cost:  133.01196 \n",
      "Prediction:\n",
      " [[143.62784]\n",
      " [195.33553]\n",
      " [180.74013]\n",
      " [197.38007]\n",
      " [154.13145]\n",
      " [117.03472]\n",
      " [142.10732]\n",
      " [ 96.81545]\n",
      " [188.43654]\n",
      " [177.7018 ]]\n",
      "700 Cost:  132.2562 \n",
      "Prediction:\n",
      " [[143.65912]\n",
      " [195.30617]\n",
      " [180.74582]\n",
      " [197.38466]\n",
      " [154.09377]\n",
      " [116.99664]\n",
      " [142.12718]\n",
      " [ 96.85508]\n",
      " [188.3916 ]\n",
      " [177.65067]]\n",
      "710 Cost:  131.50531 \n",
      "Prediction:\n",
      " [[143.69028]\n",
      " [195.27692]\n",
      " [180.7515 ]\n",
      " [197.38928]\n",
      " [154.05618]\n",
      " [116.9587 ]\n",
      " [142.147  ]\n",
      " [ 96.89464]\n",
      " [188.34686]\n",
      " [177.59976]]\n",
      "720 Cost:  130.75888 \n",
      "Prediction:\n",
      " [[143.72133]\n",
      " [195.24774]\n",
      " [180.75714]\n",
      " [197.39386]\n",
      " [154.0187 ]\n",
      " [116.92088]\n",
      " [142.16678]\n",
      " [ 96.93411]\n",
      " [188.30225]\n",
      " [177.54907]]\n",
      "730 Cost:  130.01718 \n",
      "Prediction:\n",
      " [[143.75229]\n",
      " [195.21867]\n",
      " [180.76279]\n",
      " [197.39844]\n",
      " [153.98134]\n",
      " [116.88321]\n",
      " [142.18651]\n",
      " [ 96.9735 ]\n",
      " [188.25783]\n",
      " [177.49857]]\n",
      "740 Cost:  129.28012 \n",
      "Prediction:\n",
      " [[143.78311]\n",
      " [195.18968]\n",
      " [180.76839]\n",
      " [197.40303]\n",
      " [153.94406]\n",
      " [116.84567]\n",
      " [142.2062 ]\n",
      " [ 97.01282]\n",
      " [188.21358]\n",
      " [177.44829]]\n",
      "750 Cost:  128.54745 \n",
      "Prediction:\n",
      " [[143.81383]\n",
      " [195.16078]\n",
      " [180.77394]\n",
      " [197.40755]\n",
      " [153.9069 ]\n",
      " [116.80825]\n",
      " [142.22586]\n",
      " [ 97.05204]\n",
      " [188.16943]\n",
      " [177.39821]]\n",
      "760 Cost:  127.81941 \n",
      "Prediction:\n",
      " [[143.84445 ]\n",
      " [195.13196 ]\n",
      " [180.77948 ]\n",
      " [197.41211 ]\n",
      " [153.86984 ]\n",
      " [116.771   ]\n",
      " [142.24548 ]\n",
      " [ 97.091194]\n",
      " [188.12547 ]\n",
      " [177.34834 ]]\n",
      "770 Cost:  127.095764 \n",
      "Prediction:\n",
      " [[143.87497]\n",
      " [195.10324]\n",
      " [180.785  ]\n",
      " [197.41666]\n",
      " [153.83289]\n",
      " [116.73386]\n",
      " [142.26508]\n",
      " [ 97.13027]\n",
      " [188.08168]\n",
      " [177.29868]]\n",
      "780 Cost:  126.376686 \n",
      "Prediction:\n",
      " [[143.90536 ]\n",
      " [195.0746  ]\n",
      " [180.79048 ]\n",
      " [197.42119 ]\n",
      " [153.79604 ]\n",
      " [116.696846]\n",
      " [142.28459 ]\n",
      " [ 97.16926 ]\n",
      " [188.03802 ]\n",
      " [177.24922 ]]\n",
      "790 Cost:  125.661964 \n",
      "Prediction:\n",
      " [[143.93568]\n",
      " [195.04605]\n",
      " [180.79596]\n",
      " [197.42569]\n",
      " [153.7593 ]\n",
      " [116.65996]\n",
      " [142.3041 ]\n",
      " [ 97.20816]\n",
      " [187.99452]\n",
      " [177.19998]]\n",
      "800 Cost:  124.951706 \n",
      "Prediction:\n",
      " [[143.96588]\n",
      " [195.0176 ]\n",
      " [180.8014 ]\n",
      " [197.4302 ]\n",
      " [153.72267]\n",
      " [116.62321]\n",
      " [142.32358]\n",
      " [ 97.247  ]\n",
      " [187.95119]\n",
      " [177.15096]]\n",
      "810 Cost:  124.24571 \n",
      "Prediction:\n",
      " [[143.99594]\n",
      " [194.9892 ]\n",
      " [180.80681]\n",
      " [197.4347 ]\n",
      " [153.68613]\n",
      " [116.58659]\n",
      " [142.34299]\n",
      " [ 97.28574]\n",
      " [187.90799]\n",
      " [177.10208]]\n",
      "820 Cost:  123.54419 \n",
      "Prediction:\n",
      " [[144.02594]\n",
      " [194.96092]\n",
      " [180.81221]\n",
      " [197.43921]\n",
      " [153.6497 ]\n",
      " [116.5501 ]\n",
      " [142.36238]\n",
      " [ 97.32442]\n",
      " [187.86496]\n",
      " [177.05345]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830 Cost:  122.8468 \n",
      "Prediction:\n",
      " [[144.05582]\n",
      " [194.93271]\n",
      " [180.81755]\n",
      " [197.44368]\n",
      " [153.61336]\n",
      " [116.51375]\n",
      " [142.38173]\n",
      " [ 97.36301]\n",
      " [187.82207]\n",
      " [177.00499]]\n",
      "840 Cost:  122.15405 \n",
      "Prediction:\n",
      " [[144.08562 ]\n",
      " [194.90462 ]\n",
      " [180.82292 ]\n",
      " [197.4482  ]\n",
      " [153.57716 ]\n",
      " [116.47753 ]\n",
      " [142.40105 ]\n",
      " [ 97.401535]\n",
      " [187.77936 ]\n",
      " [176.95679 ]]\n",
      "850 Cost:  121.46519 \n",
      "Prediction:\n",
      " [[144.11526]\n",
      " [194.87656]\n",
      " [180.82822]\n",
      " [197.4526 ]\n",
      " [153.54102]\n",
      " [116.44143]\n",
      " [142.4203 ]\n",
      " [ 97.43996]\n",
      " [187.73676]\n",
      " [176.90872]]\n",
      "860 Cost:  120.78082 \n",
      "Prediction:\n",
      " [[144.14485]\n",
      " [194.84863]\n",
      " [180.8335 ]\n",
      " [197.45709]\n",
      " [153.50502]\n",
      " [116.40546]\n",
      " [142.43954]\n",
      " [ 97.47832]\n",
      " [187.69434]\n",
      " [176.8609 ]]\n",
      "870 Cost:  120.1004 \n",
      "Prediction:\n",
      " [[144.17432 ]\n",
      " [194.82076 ]\n",
      " [180.83876 ]\n",
      " [197.46153 ]\n",
      " [153.46909 ]\n",
      " [116.369606]\n",
      " [142.45872 ]\n",
      " [ 97.516594]\n",
      " [187.65205 ]\n",
      " [176.81323 ]]\n",
      "880 Cost:  119.42446 \n",
      "Prediction:\n",
      " [[144.20369]\n",
      " [194.79298]\n",
      " [180.84401]\n",
      " [197.46596]\n",
      " [153.43327]\n",
      " [116.33389]\n",
      " [142.47787]\n",
      " [ 97.55478]\n",
      " [187.60994]\n",
      " [176.76581]]\n",
      "890 Cost:  118.75261 \n",
      "Prediction:\n",
      " [[144.23296]\n",
      " [194.7653 ]\n",
      " [180.84923]\n",
      " [197.47041]\n",
      " [153.39757]\n",
      " [116.29831]\n",
      " [142.49701]\n",
      " [ 97.5929 ]\n",
      " [187.56798]\n",
      " [176.71857]]\n",
      "900 Cost:  118.08482 \n",
      "Prediction:\n",
      " [[144.26213]\n",
      " [194.7377 ]\n",
      " [180.85443]\n",
      " [197.47485]\n",
      " [153.36197]\n",
      " [116.26285]\n",
      " [142.51608]\n",
      " [ 97.63095]\n",
      " [187.52615]\n",
      " [176.67151]]\n",
      "910 Cost:  117.42106 \n",
      "Prediction:\n",
      " [[144.2912  ]\n",
      " [194.71019 ]\n",
      " [180.85957 ]\n",
      " [197.47923 ]\n",
      " [153.32645 ]\n",
      " [116.22751 ]\n",
      " [142.53513 ]\n",
      " [ 97.668915]\n",
      " [187.48447 ]\n",
      " [176.62465 ]]\n",
      "920 Cost:  116.76143 \n",
      "Prediction:\n",
      " [[144.32013]\n",
      " [194.68272]\n",
      " [180.8647 ]\n",
      " [197.48363]\n",
      " [153.29103]\n",
      " [116.1923 ]\n",
      " [142.55412]\n",
      " [ 97.70679]\n",
      " [187.44292]\n",
      " [176.57799]]\n",
      "930 Cost:  116.105835 \n",
      "Prediction:\n",
      " [[144.349   ]\n",
      " [194.65538 ]\n",
      " [180.86981 ]\n",
      " [197.48802 ]\n",
      " [153.25572 ]\n",
      " [116.157196]\n",
      " [142.57307 ]\n",
      " [ 97.74458 ]\n",
      " [187.40154 ]\n",
      " [176.53151 ]]\n",
      "940 Cost:  115.45425 \n",
      "Prediction:\n",
      " [[144.37775 ]\n",
      " [194.6281  ]\n",
      " [180.8749  ]\n",
      " [197.4924  ]\n",
      " [153.22049 ]\n",
      " [116.122246]\n",
      " [142.592   ]\n",
      " [ 97.78231 ]\n",
      " [187.36029 ]\n",
      " [176.48523 ]]\n",
      "950 Cost:  114.80676 \n",
      "Prediction:\n",
      " [[144.40643]\n",
      " [194.60092]\n",
      " [180.88   ]\n",
      " [197.49681]\n",
      " [153.18541]\n",
      " [116.08742]\n",
      " [142.6109 ]\n",
      " [ 97.81998]\n",
      " [187.31921]\n",
      " [176.43915]]\n",
      "960 Cost:  114.163124 \n",
      "Prediction:\n",
      " [[144.435  ]\n",
      " [194.57382]\n",
      " [180.88504]\n",
      " [197.50117]\n",
      " [153.1504 ]\n",
      " [116.05268]\n",
      " [142.62976]\n",
      " [ 97.85754]\n",
      " [187.27826]\n",
      " [176.39325]]\n",
      "970 Cost:  113.52329 \n",
      "Prediction:\n",
      " [[144.46342]\n",
      " [194.54677]\n",
      " [180.89003]\n",
      " [197.50551]\n",
      " [153.11546]\n",
      " [116.01808]\n",
      " [142.64854]\n",
      " [ 97.89502]\n",
      " [187.23743]\n",
      " [176.34752]]\n",
      "980 Cost:  112.88768 \n",
      "Prediction:\n",
      " [[144.4918 ]\n",
      " [194.51985]\n",
      " [180.89503]\n",
      " [197.50987]\n",
      " [153.08066]\n",
      " [115.98363]\n",
      " [142.66733]\n",
      " [ 97.93245]\n",
      " [187.1968 ]\n",
      " [176.30202]]\n",
      "990 Cost:  112.255905 \n",
      "Prediction:\n",
      " [[144.52007 ]\n",
      " [194.49301 ]\n",
      " [180.90002 ]\n",
      " [197.51422 ]\n",
      " [153.04594 ]\n",
      " [115.94929 ]\n",
      " [142.68607 ]\n",
      " [ 97.969795]\n",
      " [187.15628 ]\n",
      " [176.2567  ]]\n",
      "1000 Cost:  111.627914 \n",
      "Prediction:\n",
      " [[144.54825 ]\n",
      " [194.46625 ]\n",
      " [180.90497 ]\n",
      " [197.51855 ]\n",
      " [153.01132 ]\n",
      " [115.915054]\n",
      " [142.70477 ]\n",
      " [ 98.00705 ]\n",
      " [187.1159  ]\n",
      " [176.21156 ]]\n",
      "1010 Cost:  111.003624 \n",
      "Prediction:\n",
      " [[144.5763 ]\n",
      " [194.43951]\n",
      " [180.90988]\n",
      " [197.52284]\n",
      " [152.97679]\n",
      " [115.88093]\n",
      " [142.7234 ]\n",
      " [ 98.04421]\n",
      " [187.07565]\n",
      " [176.16658]]\n",
      "1020 Cost:  110.3835 \n",
      "Prediction:\n",
      " [[144.60428 ]\n",
      " [194.41293 ]\n",
      " [180.91478 ]\n",
      " [197.52718 ]\n",
      " [152.94238 ]\n",
      " [115.846954]\n",
      " [142.74205 ]\n",
      " [ 98.08133 ]\n",
      " [187.0356  ]\n",
      " [176.12184 ]]\n",
      "1030 Cost:  109.76703 \n",
      "Prediction:\n",
      " [[144.63219]\n",
      " [194.3864 ]\n",
      " [180.9197 ]\n",
      " [197.53151]\n",
      " [152.90808]\n",
      " [115.8131 ]\n",
      " [142.76067]\n",
      " [ 98.11837]\n",
      " [186.99567]\n",
      " [176.07729]]\n",
      "1040 Cost:  109.15421 \n",
      "Prediction:\n",
      " [[144.65994 ]\n",
      " [194.35995 ]\n",
      " [180.92453 ]\n",
      " [197.53578 ]\n",
      " [152.87384 ]\n",
      " [115.77934 ]\n",
      " [142.77917 ]\n",
      " [ 98.155304]\n",
      " [186.95584 ]\n",
      " [176.03285 ]]\n",
      "1050 Cost:  108.54514 \n",
      "Prediction:\n",
      " [[144.68764]\n",
      " [194.33359]\n",
      " [180.92937]\n",
      " [197.54007]\n",
      " [152.83972]\n",
      " [115.74571]\n",
      " [142.7977 ]\n",
      " [ 98.19219]\n",
      " [186.91618]\n",
      " [175.98863]]\n",
      "1060 Cost:  107.939674 \n",
      "Prediction:\n",
      " [[144.7152  ]\n",
      " [194.30727 ]\n",
      " [180.93414 ]\n",
      " [197.54433 ]\n",
      " [152.80565 ]\n",
      " [115.712204]\n",
      " [142.81615 ]\n",
      " [ 98.22898 ]\n",
      " [186.87663 ]\n",
      " [175.94458 ]]\n",
      "1070 Cost:  107.33816 \n",
      "Prediction:\n",
      " [[144.7427 ]\n",
      " [194.28107]\n",
      " [180.93895]\n",
      " [197.5486 ]\n",
      " [152.77174]\n",
      " [115.67881]\n",
      " [142.83458]\n",
      " [ 98.2657 ]\n",
      " [186.83725]\n",
      " [175.90074]]\n",
      "1080 Cost:  106.74011 \n",
      "Prediction:\n",
      " [[144.77008 ]\n",
      " [194.25494 ]\n",
      " [180.9437  ]\n",
      " [197.55286 ]\n",
      " [152.73787 ]\n",
      " [115.64554 ]\n",
      " [142.85298 ]\n",
      " [ 98.302345]\n",
      " [186.798   ]\n",
      " [175.85704 ]]\n",
      "1090 Cost:  106.14577 \n",
      "Prediction:\n",
      " [[144.79741]\n",
      " [194.22888]\n",
      " [180.94846]\n",
      " [197.55711]\n",
      " [152.70413]\n",
      " [115.61238]\n",
      " [142.87135]\n",
      " [ 98.33891]\n",
      " [186.75888]\n",
      " [175.81357]]\n",
      "1100 Cost:  105.55522 \n",
      "Prediction:\n",
      " [[144.82462 ]\n",
      " [194.20293 ]\n",
      " [180.95319 ]\n",
      " [197.56137 ]\n",
      " [152.67049 ]\n",
      " [115.57934 ]\n",
      " [142.88966 ]\n",
      " [ 98.375404]\n",
      " [186.71994 ]\n",
      " [175.77026 ]]\n",
      "1110 Cost:  104.9681 \n",
      "Prediction:\n",
      " [[144.85173 ]\n",
      " [194.17702 ]\n",
      " [180.95787 ]\n",
      " [197.5656  ]\n",
      " [152.63693 ]\n",
      " [115.546425]\n",
      " [142.90796 ]\n",
      " [ 98.41183 ]\n",
      " [186.6811  ]\n",
      " [175.72713 ]]\n",
      "1120 Cost:  104.38467 \n",
      "Prediction:\n",
      " [[144.87877 ]\n",
      " [194.15123 ]\n",
      " [180.96257 ]\n",
      " [197.56984 ]\n",
      " [152.60347 ]\n",
      " [115.513626]\n",
      " [142.92622 ]\n",
      " [ 98.44818 ]\n",
      " [186.64243 ]\n",
      " [175.68419 ]]\n",
      "1130 Cost:  103.80448 \n",
      "Prediction:\n",
      " [[144.90567]\n",
      " [194.12546]\n",
      " [180.96718]\n",
      " [197.574  ]\n",
      " [152.57008]\n",
      " [115.48092]\n",
      " [142.94441]\n",
      " [ 98.48443]\n",
      " [186.60382]\n",
      " [175.64139]]\n",
      "1140 Cost:  103.22809 \n",
      "Prediction:\n",
      " [[144.93251]\n",
      " [194.09981]\n",
      " [180.97183]\n",
      " [197.57825]\n",
      " [152.53682]\n",
      " [115.44835]\n",
      " [142.96259]\n",
      " [ 98.52063]\n",
      " [186.5654 ]\n",
      " [175.59879]]\n",
      "1150 Cost:  102.65531 \n",
      "Prediction:\n",
      " [[144.95927 ]\n",
      " [194.07425 ]\n",
      " [180.97646 ]\n",
      " [197.58246 ]\n",
      " [152.50366 ]\n",
      " [115.415886]\n",
      " [142.98074 ]\n",
      " [ 98.55674 ]\n",
      " [186.52713 ]\n",
      " [175.55638 ]]\n",
      "1160 Cost:  102.0858 \n",
      "Prediction:\n",
      " [[144.98592]\n",
      " [194.04875]\n",
      " [180.98103]\n",
      " [197.58664]\n",
      " [152.47055]\n",
      " [115.38354]\n",
      " [142.99884]\n",
      " [ 98.59277]\n",
      " [186.48897]\n",
      " [175.51411]]\n",
      "1170 Cost:  101.51976 \n",
      "Prediction:\n",
      " [[145.01248 ]\n",
      " [194.0233  ]\n",
      " [180.9856  ]\n",
      " [197.59082 ]\n",
      " [152.43758 ]\n",
      " [115.351295]\n",
      " [143.01689 ]\n",
      " [ 98.62874 ]\n",
      " [186.45094 ]\n",
      " [175.47203 ]]\n",
      "1180 Cost:  100.95729 \n",
      "Prediction:\n",
      " [[145.03894]\n",
      " [193.99797]\n",
      " [180.99014]\n",
      " [197.595  ]\n",
      " [152.40466]\n",
      " [115.31919]\n",
      " [143.03494]\n",
      " [ 98.66463]\n",
      " [186.41307]\n",
      " [175.43013]]\n",
      "1190 Cost:  100.39804 \n",
      "Prediction:\n",
      " [[145.0653 ]\n",
      " [193.97269]\n",
      " [180.99466]\n",
      " [197.59917]\n",
      " [152.37184]\n",
      " [115.28718]\n",
      " [143.0529 ]\n",
      " [ 98.70044]\n",
      " [186.37529]\n",
      " [175.38838]]\n",
      "1200 Cost:  99.84232 \n",
      "Prediction:\n",
      " [[145.0916  ]\n",
      " [193.9475  ]\n",
      " [180.99916 ]\n",
      " [197.60332 ]\n",
      " [152.33913 ]\n",
      " [115.25529 ]\n",
      " [143.07086 ]\n",
      " [ 98.736176]\n",
      " [186.33768 ]\n",
      " [175.34682 ]]\n",
      "1210 Cost:  99.28984 \n",
      "Prediction:\n",
      " [[145.1178  ]\n",
      " [193.92235 ]\n",
      " [181.00362 ]\n",
      " [197.60745 ]\n",
      " [152.3065  ]\n",
      " [115.22349 ]\n",
      " [143.08878 ]\n",
      " [ 98.771835]\n",
      " [186.30019 ]\n",
      " [175.30542 ]]\n",
      "1220 Cost:  98.740906 \n",
      "Prediction:\n",
      " [[145.1439 ]\n",
      " [193.89732]\n",
      " [181.00809]\n",
      " [197.6116 ]\n",
      " [152.27399]\n",
      " [115.19182]\n",
      " [143.10667]\n",
      " [ 98.80742]\n",
      " [186.26282]\n",
      " [175.26422]]\n",
      "1230 Cost:  98.19507 \n",
      "Prediction:\n",
      " [[145.16992]\n",
      " [193.87234]\n",
      " [181.01253]\n",
      " [197.61572]\n",
      " [152.24153]\n",
      " [115.16025]\n",
      " [143.1245 ]\n",
      " [ 98.84294]\n",
      " [186.22559]\n",
      " [175.22314]]\n",
      "1240 Cost:  97.652855 \n",
      "Prediction:\n",
      " [[145.19586]\n",
      " [193.84747]\n",
      " [181.01697]\n",
      " [197.61987]\n",
      " [152.2092 ]\n",
      " [115.12881]\n",
      " [143.14233]\n",
      " [ 98.87838]\n",
      " [186.1885 ]\n",
      " [175.1823 ]]\n",
      "1250 Cost:  97.11369 \n",
      "Prediction:\n",
      " [[145.2217 ]\n",
      " [193.82265]\n",
      " [181.02135]\n",
      " [197.62398]\n",
      " [152.17693]\n",
      " [115.09748]\n",
      " [143.1601 ]\n",
      " [ 98.91375]\n",
      " [186.15154]\n",
      " [175.14157]]\n",
      "1260 Cost:  96.57796 \n",
      "Prediction:\n",
      " [[145.24745]\n",
      " [193.79793]\n",
      " [181.02574]\n",
      " [197.62808]\n",
      " [152.14478]\n",
      " [115.06626]\n",
      " [143.17783]\n",
      " [ 98.94905]\n",
      " [186.1147 ]\n",
      " [175.10103]]\n",
      "1270 Cost:  96.0452 \n",
      "Prediction:\n",
      " [[145.2731 ]\n",
      " [193.77324]\n",
      " [181.03006]\n",
      " [197.63217]\n",
      " [152.11267]\n",
      " [115.03513]\n",
      " [143.19553]\n",
      " [ 98.98426]\n",
      " [186.07797]\n",
      " [175.06064]]\n",
      "1280 Cost:  95.51586 \n",
      "Prediction:\n",
      " [[145.29866 ]\n",
      " [193.74864 ]\n",
      " [181.0344  ]\n",
      " [197.63626 ]\n",
      " [152.08069 ]\n",
      " [115.004105]\n",
      " [143.21318 ]\n",
      " [ 99.019394]\n",
      " [186.0414  ]\n",
      " [175.02043 ]]\n",
      "1290 Cost:  94.98985 \n",
      "Prediction:\n",
      " [[145.32419]\n",
      " [193.72415]\n",
      " [181.03874]\n",
      " [197.64037]\n",
      " [152.04883]\n",
      " [114.97322]\n",
      " [143.23083]\n",
      " [ 99.05448]\n",
      " [186.00497]\n",
      " [174.98041]]\n",
      "1300 Cost:  94.46672 \n",
      "Prediction:\n",
      " [[145.34955]\n",
      " [193.69969]\n",
      " [181.04301]\n",
      " [197.64441]\n",
      " [152.01698]\n",
      " [114.94241]\n",
      " [143.2484 ]\n",
      " [ 99.08946]\n",
      " [185.96861]\n",
      " [174.9405 ]]\n",
      "1310 Cost:  93.947 \n",
      "Prediction:\n",
      " [[145.37488]\n",
      " [193.67534]\n",
      " [181.04729]\n",
      " [197.64848]\n",
      " [151.98528]\n",
      " [114.91173]\n",
      " [143.26596]\n",
      " [ 99.12438]\n",
      " [185.93243]\n",
      " [174.9008 ]]\n",
      "1320 Cost:  93.43028 \n",
      "Prediction:\n",
      " [[145.4001  ]\n",
      " [193.65103 ]\n",
      " [181.05154 ]\n",
      " [197.65254 ]\n",
      " [151.95364 ]\n",
      " [114.881165]\n",
      " [143.28348 ]\n",
      " [ 99.15924 ]\n",
      " [185.89636 ]\n",
      " [174.86124 ]]\n",
      "1330 Cost:  92.91675 \n",
      "Prediction:\n",
      " [[145.42525 ]\n",
      " [193.62682 ]\n",
      " [181.05579 ]\n",
      " [197.65659 ]\n",
      " [151.92212 ]\n",
      " [114.85069 ]\n",
      " [143.30098 ]\n",
      " [ 99.194016]\n",
      " [185.86041 ]\n",
      " [174.82185 ]]\n",
      "1340 Cost:  92.406166 \n",
      "Prediction:\n",
      " [[145.45027 ]\n",
      " [193.60265 ]\n",
      " [181.05997 ]\n",
      " [197.6606  ]\n",
      " [151.89064 ]\n",
      " [114.820305]\n",
      " [143.3184  ]\n",
      " [ 99.22871 ]\n",
      " [185.82458 ]\n",
      " [174.78261 ]]\n",
      "1350 Cost:  91.89888 \n",
      "Prediction:\n",
      " [[145.47525 ]\n",
      " [193.57858 ]\n",
      " [181.06416 ]\n",
      " [197.66464 ]\n",
      " [151.85928 ]\n",
      " [114.790054]\n",
      " [143.33582 ]\n",
      " [ 99.26333 ]\n",
      " [185.7889  ]\n",
      " [174.74355 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1360 Cost:  91.394646 \n",
      "Prediction:\n",
      " [[145.50015]\n",
      " [193.5546 ]\n",
      " [181.06836]\n",
      " [197.66869]\n",
      " [151.82803]\n",
      " [114.7599 ]\n",
      " [143.3532 ]\n",
      " [ 99.29789]\n",
      " [185.75334]\n",
      " [174.70464]]\n",
      "1370 Cost:  90.89317 \n",
      "Prediction:\n",
      " [[145.52493]\n",
      " [193.53062]\n",
      " [181.07248]\n",
      " [197.67265]\n",
      " [151.79681]\n",
      " [114.72986]\n",
      " [143.37051]\n",
      " [ 99.33237]\n",
      " [185.71786]\n",
      " [174.66586]]\n",
      "1380 Cost:  90.39498 \n",
      "Prediction:\n",
      " [[145.54964]\n",
      " [193.50679]\n",
      " [181.07661]\n",
      " [197.67668]\n",
      " [151.76572]\n",
      " [114.6999 ]\n",
      " [143.38782]\n",
      " [ 99.36678]\n",
      " [185.68256]\n",
      " [174.62729]]\n",
      "1390 Cost:  89.899765 \n",
      "Prediction:\n",
      " [[145.57428 ]\n",
      " [193.483   ]\n",
      " [181.08073 ]\n",
      " [197.68068 ]\n",
      " [151.73473 ]\n",
      " [114.670074]\n",
      " [143.40509 ]\n",
      " [ 99.40112 ]\n",
      " [185.64735 ]\n",
      " [174.58885 ]]\n",
      "1400 Cost:  89.40749 \n",
      "Prediction:\n",
      " [[145.5988  ]\n",
      " [193.45927 ]\n",
      " [181.08482 ]\n",
      " [197.68465 ]\n",
      " [151.70378 ]\n",
      " [114.640335]\n",
      " [143.42233 ]\n",
      " [ 99.43538 ]\n",
      " [185.61229 ]\n",
      " [174.55058 ]]\n",
      "1410 Cost:  88.9182 \n",
      "Prediction:\n",
      " [[145.62325 ]\n",
      " [193.43564 ]\n",
      " [181.0889  ]\n",
      " [197.68863 ]\n",
      " [151.67294 ]\n",
      " [114.61071 ]\n",
      " [143.43953 ]\n",
      " [ 99.469574]\n",
      " [185.57733 ]\n",
      " [174.51245 ]]\n",
      "1420 Cost:  88.43185 \n",
      "Prediction:\n",
      " [[145.64764]\n",
      " [193.41206]\n",
      " [181.09296]\n",
      " [197.6926 ]\n",
      " [151.6422 ]\n",
      " [114.58118]\n",
      " [143.4567 ]\n",
      " [ 99.50369]\n",
      " [185.54251]\n",
      " [174.4745 ]]\n",
      "1430 Cost:  87.94846 \n",
      "Prediction:\n",
      " [[145.67194 ]\n",
      " [193.38858 ]\n",
      " [181.09698 ]\n",
      " [197.69658 ]\n",
      " [151.61153 ]\n",
      " [114.551765]\n",
      " [143.47382 ]\n",
      " [ 99.53774 ]\n",
      " [185.5078  ]\n",
      " [174.43669 ]]\n",
      "1440 Cost:  87.46788 \n",
      "Prediction:\n",
      " [[145.69615]\n",
      " [193.36513]\n",
      " [181.101  ]\n",
      " [197.70052]\n",
      " [151.58095]\n",
      " [114.52243]\n",
      " [143.4909 ]\n",
      " [ 99.57172]\n",
      " [185.4732 ]\n",
      " [174.39905]]\n",
      "1450 Cost:  86.99019 \n",
      "Prediction:\n",
      " [[145.72026]\n",
      " [193.34175]\n",
      " [181.10498]\n",
      " [197.70448]\n",
      " [151.55045]\n",
      " [114.49321]\n",
      " [143.50795]\n",
      " [ 99.60562]\n",
      " [185.43872]\n",
      " [174.36154]]\n",
      "1460 Cost:  86.5155 \n",
      "Prediction:\n",
      " [[145.7443  ]\n",
      " [193.31848 ]\n",
      " [181.10898 ]\n",
      " [197.70842 ]\n",
      " [151.52003 ]\n",
      " [114.4641  ]\n",
      " [143.52498 ]\n",
      " [ 99.639465]\n",
      " [185.40439 ]\n",
      " [174.3242  ]]\n",
      "1470 Cost:  86.043526 \n",
      "Prediction:\n",
      " [[145.76826 ]\n",
      " [193.29524 ]\n",
      " [181.11295 ]\n",
      " [197.71234 ]\n",
      " [151.48972 ]\n",
      " [114.435074]\n",
      " [143.54196 ]\n",
      " [ 99.67321 ]\n",
      " [185.37015 ]\n",
      " [174.287   ]]\n",
      "1480 Cost:  85.57449 \n",
      "Prediction:\n",
      " [[145.79216 ]\n",
      " [193.27211 ]\n",
      " [181.11688 ]\n",
      " [197.71625 ]\n",
      " [151.45947 ]\n",
      " [114.406166]\n",
      " [143.55891 ]\n",
      " [ 99.7069  ]\n",
      " [185.33604 ]\n",
      " [174.24997 ]]\n",
      "1490 Cost:  85.108215 \n",
      "Prediction:\n",
      " [[145.81596 ]\n",
      " [193.24902 ]\n",
      " [181.1208  ]\n",
      " [197.72017 ]\n",
      " [151.42932 ]\n",
      " [114.37735 ]\n",
      " [143.57582 ]\n",
      " [ 99.740524]\n",
      " [185.30205 ]\n",
      " [174.21309 ]]\n",
      "1500 Cost:  84.64479 \n",
      "Prediction:\n",
      " [[145.83968 ]\n",
      " [193.226   ]\n",
      " [181.12468 ]\n",
      " [197.72408 ]\n",
      " [151.39925 ]\n",
      " [114.34863 ]\n",
      " [143.5927  ]\n",
      " [ 99.774055]\n",
      " [185.26817 ]\n",
      " [174.17636 ]]\n",
      "1510 Cost:  84.18414 \n",
      "Prediction:\n",
      " [[145.8633 ]\n",
      " [193.20306]\n",
      " [181.12859]\n",
      " [197.72797]\n",
      " [151.36926]\n",
      " [114.32002]\n",
      " [143.60954]\n",
      " [ 99.80753]\n",
      " [185.23442]\n",
      " [174.13977]]\n",
      "1520 Cost:  83.726234 \n",
      "Prediction:\n",
      " [[145.88687]\n",
      " [193.18019]\n",
      " [181.13245]\n",
      " [197.73187]\n",
      " [151.33936]\n",
      " [114.29151]\n",
      " [143.62636]\n",
      " [ 99.84094]\n",
      " [185.2008 ]\n",
      " [174.10333]]\n",
      "1530 Cost:  83.27104 \n",
      "Prediction:\n",
      " [[145.91032]\n",
      " [193.15738]\n",
      " [181.13629]\n",
      " [197.73573]\n",
      " [151.30954]\n",
      " [114.2631 ]\n",
      " [143.64313]\n",
      " [ 99.87428]\n",
      " [185.16727]\n",
      " [174.06705]]\n",
      "1540 Cost:  82.81861 \n",
      "Prediction:\n",
      " [[145.93373]\n",
      " [193.13464]\n",
      " [181.14012]\n",
      " [197.73961]\n",
      " [151.2798 ]\n",
      " [114.23477]\n",
      " [143.65987]\n",
      " [ 99.90753]\n",
      " [185.13387]\n",
      " [174.03091]]\n",
      "1550 Cost:  82.368904 \n",
      "Prediction:\n",
      " [[145.95708 ]\n",
      " [193.11198 ]\n",
      " [181.14394 ]\n",
      " [197.74348 ]\n",
      " [151.25015 ]\n",
      " [114.206566]\n",
      " [143.67657 ]\n",
      " [ 99.94073 ]\n",
      " [185.10057 ]\n",
      " [173.99493 ]]\n",
      "1560 Cost:  81.921875 \n",
      "Prediction:\n",
      " [[145.98029 ]\n",
      " [193.08939 ]\n",
      " [181.14774 ]\n",
      " [197.74734 ]\n",
      " [151.22058 ]\n",
      " [114.17844 ]\n",
      " [143.69324 ]\n",
      " [ 99.973854]\n",
      " [185.06741 ]\n",
      " [173.95908 ]]\n",
      "1570 Cost:  81.47748 \n",
      "Prediction:\n",
      " [[146.00345 ]\n",
      " [193.06685 ]\n",
      " [181.15149 ]\n",
      " [197.75119 ]\n",
      " [151.19109 ]\n",
      " [114.150406]\n",
      " [143.70987 ]\n",
      " [100.00689 ]\n",
      " [185.03433 ]\n",
      " [173.92339 ]]\n",
      "1580 Cost:  81.0358 \n",
      "Prediction:\n",
      " [[146.02654]\n",
      " [193.04439]\n",
      " [181.15526]\n",
      " [197.75502]\n",
      " [151.16168]\n",
      " [114.1225 ]\n",
      " [143.72649]\n",
      " [100.03988]\n",
      " [185.0014 ]\n",
      " [173.88783]]\n",
      "1590 Cost:  80.59679 \n",
      "Prediction:\n",
      " [[146.04953 ]\n",
      " [193.02199 ]\n",
      " [181.15901 ]\n",
      " [197.75887 ]\n",
      " [151.13237 ]\n",
      " [114.094666]\n",
      " [143.74304 ]\n",
      " [100.07279 ]\n",
      " [184.96858 ]\n",
      " [173.85243 ]]\n",
      "1600 Cost:  80.16027 \n",
      "Prediction:\n",
      " [[146.07245]\n",
      " [192.99965]\n",
      " [181.1627 ]\n",
      " [197.76266]\n",
      " [151.10312]\n",
      " [114.06693]\n",
      " [143.75957]\n",
      " [100.10563]\n",
      " [184.93584]\n",
      " [173.81717]]\n",
      "1610 Cost:  79.72654 \n",
      "Prediction:\n",
      " [[146.09529]\n",
      " [192.97739]\n",
      " [181.16641]\n",
      " [197.76648]\n",
      " [151.07397]\n",
      " [114.03929]\n",
      " [143.77606]\n",
      " [100.1384 ]\n",
      " [184.90326]\n",
      " [173.78206]]\n",
      "1620 Cost:  79.29533 \n",
      "Prediction:\n",
      " [[146.11807]\n",
      " [192.9552 ]\n",
      " [181.1701 ]\n",
      " [197.7703 ]\n",
      " [151.04489]\n",
      " [114.01174]\n",
      " [143.79254]\n",
      " [100.17109]\n",
      " [184.87077]\n",
      " [173.7471 ]]\n",
      "1630 Cost:  78.86664 \n",
      "Prediction:\n",
      " [[146.14076 ]\n",
      " [192.93306 ]\n",
      " [181.17377 ]\n",
      " [197.7741  ]\n",
      " [151.01588 ]\n",
      " [113.984314]\n",
      " [143.80896 ]\n",
      " [100.20372 ]\n",
      " [184.83838 ]\n",
      " [173.71227 ]]\n",
      "1640 Cost:  78.44051 \n",
      "Prediction:\n",
      " [[146.16338 ]\n",
      " [192.911   ]\n",
      " [181.17741 ]\n",
      " [197.77788 ]\n",
      " [150.98697 ]\n",
      " [113.95695 ]\n",
      " [143.82535 ]\n",
      " [100.236275]\n",
      " [184.80612 ]\n",
      " [173.67757 ]]\n",
      "1650 Cost:  78.017006 \n",
      "Prediction:\n",
      " [[146.1859  ]\n",
      " [192.88899 ]\n",
      " [181.18106 ]\n",
      " [197.78166 ]\n",
      " [150.95813 ]\n",
      " [113.92971 ]\n",
      " [143.8417  ]\n",
      " [100.268776]\n",
      " [184.77396 ]\n",
      " [173.64304 ]]\n",
      "1660 Cost:  77.59603 \n",
      "Prediction:\n",
      " [[146.20837]\n",
      " [192.86707]\n",
      " [181.18466]\n",
      " [197.78545]\n",
      " [150.92938]\n",
      " [113.90254]\n",
      " [143.85803]\n",
      " [100.30119]\n",
      " [184.74193]\n",
      " [173.60864]]\n",
      "1670 Cost:  77.17752 \n",
      "Prediction:\n",
      " [[146.23074]\n",
      " [192.8452 ]\n",
      " [181.18828]\n",
      " [197.78922]\n",
      " [150.90071]\n",
      " [113.87548]\n",
      " [143.87433]\n",
      " [100.33355]\n",
      " [184.71   ]\n",
      " [173.57436]]\n",
      "1680 Cost:  76.76155 \n",
      "Prediction:\n",
      " [[146.25307 ]\n",
      " [192.8234  ]\n",
      " [181.19185 ]\n",
      " [197.79297 ]\n",
      " [150.87212 ]\n",
      " [113.84851 ]\n",
      " [143.89058 ]\n",
      " [100.365814]\n",
      " [184.67818 ]\n",
      " [173.54024 ]]\n",
      "1690 Cost:  76.34799 \n",
      "Prediction:\n",
      " [[146.2753  ]\n",
      " [192.80165 ]\n",
      " [181.19542 ]\n",
      " [197.7967  ]\n",
      " [150.84361 ]\n",
      " [113.821625]\n",
      " [143.90681 ]\n",
      " [100.39803 ]\n",
      " [184.64647 ]\n",
      " [173.50626 ]]\n",
      "1700 Cost:  75.93693 \n",
      "Prediction:\n",
      " [[146.29744 ]\n",
      " [192.77998 ]\n",
      " [181.19896 ]\n",
      " [197.80048 ]\n",
      " [150.81516 ]\n",
      " [113.79483 ]\n",
      " [143.92299 ]\n",
      " [100.430176]\n",
      " [184.61487 ]\n",
      " [173.47241 ]]\n",
      "1710 Cost:  75.52831 \n",
      "Prediction:\n",
      " [[146.31952 ]\n",
      " [192.75838 ]\n",
      " [181.20248 ]\n",
      " [197.8042  ]\n",
      " [150.7868  ]\n",
      " [113.768135]\n",
      " [143.93915 ]\n",
      " [100.46224 ]\n",
      " [184.58337 ]\n",
      " [173.43869 ]]\n",
      "1720 Cost:  75.122154 \n",
      "Prediction:\n",
      " [[146.34154 ]\n",
      " [192.73685 ]\n",
      " [181.20601 ]\n",
      " [197.80794 ]\n",
      " [150.75853 ]\n",
      " [113.741554]\n",
      " [143.95528 ]\n",
      " [100.49427 ]\n",
      " [184.55199 ]\n",
      " [173.40514 ]]\n",
      "1730 Cost:  74.71836 \n",
      "Prediction:\n",
      " [[146.36346 ]\n",
      " [192.71535 ]\n",
      " [181.2095  ]\n",
      " [197.81163 ]\n",
      " [150.73033 ]\n",
      " [113.715034]\n",
      " [143.97136 ]\n",
      " [100.5262  ]\n",
      " [184.52072 ]\n",
      " [173.37169 ]]\n",
      "1740 Cost:  74.31696 \n",
      "Prediction:\n",
      " [[146.38531]\n",
      " [192.69392]\n",
      " [181.21298]\n",
      " [197.81534]\n",
      " [150.70221]\n",
      " [113.68862]\n",
      " [143.9874 ]\n",
      " [100.55807]\n",
      " [184.48953]\n",
      " [173.33838]]\n",
      "1750 Cost:  73.91802 \n",
      "Prediction:\n",
      " [[146.40712 ]\n",
      " [192.67258 ]\n",
      " [181.21646 ]\n",
      " [197.81906 ]\n",
      " [150.67416 ]\n",
      " [113.66229 ]\n",
      " [144.00343 ]\n",
      " [100.589874]\n",
      " [184.4585  ]\n",
      " [173.30524 ]]\n",
      "1760 Cost:  73.52154 \n",
      "Prediction:\n",
      " [[146.42883 ]\n",
      " [192.6513  ]\n",
      " [181.21991 ]\n",
      " [197.82275 ]\n",
      " [150.64622 ]\n",
      " [113.636055]\n",
      " [144.01941 ]\n",
      " [100.621605]\n",
      " [184.42755 ]\n",
      " [173.27222 ]]\n",
      "1770 Cost:  73.12718 \n",
      "Prediction:\n",
      " [[146.45047 ]\n",
      " [192.63008 ]\n",
      " [181.22334 ]\n",
      " [197.82643 ]\n",
      " [150.61833 ]\n",
      " [113.60991 ]\n",
      " [144.03535 ]\n",
      " [100.653275]\n",
      " [184.39668 ]\n",
      " [173.2393  ]]\n",
      "1780 Cost:  72.73536 \n",
      "Prediction:\n",
      " [[146.47203 ]\n",
      " [192.60893 ]\n",
      " [181.22675 ]\n",
      " [197.83012 ]\n",
      " [150.59052 ]\n",
      " [113.583855]\n",
      " [144.0513  ]\n",
      " [100.68487 ]\n",
      " [184.36597 ]\n",
      " [173.20656 ]]\n",
      "1790 Cost:  72.34585 \n",
      "Prediction:\n",
      " [[146.49353]\n",
      " [192.58783]\n",
      " [181.23015]\n",
      " [197.8338 ]\n",
      " [150.56279]\n",
      " [113.55789]\n",
      " [144.06717]\n",
      " [100.7164 ]\n",
      " [184.33534]\n",
      " [173.17395]]\n",
      "1800 Cost:  71.9586 \n",
      "Prediction:\n",
      " [[146.51495]\n",
      " [192.5668 ]\n",
      " [181.23352]\n",
      " [197.83746]\n",
      " [150.53514]\n",
      " [113.53203]\n",
      " [144.08302]\n",
      " [100.74786]\n",
      " [184.30481]\n",
      " [173.14145]]\n",
      "1810 Cost:  71.57356 \n",
      "Prediction:\n",
      " [[146.53629]\n",
      " [192.5458 ]\n",
      " [181.23688]\n",
      " [197.84111]\n",
      " [150.50755]\n",
      " [113.50623]\n",
      " [144.09883]\n",
      " [100.77926]\n",
      " [184.27437]\n",
      " [173.10909]]\n",
      "1820 Cost:  71.190956 \n",
      "Prediction:\n",
      " [[146.55757 ]\n",
      " [192.52492 ]\n",
      " [181.24025 ]\n",
      " [197.84477 ]\n",
      " [150.48006 ]\n",
      " [113.480545]\n",
      " [144.11464 ]\n",
      " [100.81059 ]\n",
      " [184.24408 ]\n",
      " [173.07686 ]]\n",
      "1830 Cost:  70.810555 \n",
      "Prediction:\n",
      " [[146.57877]\n",
      " [192.50407]\n",
      " [181.24358]\n",
      " [197.8484 ]\n",
      " [150.45264]\n",
      " [113.45492]\n",
      " [144.13039]\n",
      " [100.84185]\n",
      " [184.21387]\n",
      " [173.04477]]\n",
      "1840 Cost:  70.43244 \n",
      "Prediction:\n",
      " [[146.5999 ]\n",
      " [192.48329]\n",
      " [181.24689]\n",
      " [197.85205]\n",
      " [150.4253 ]\n",
      " [113.4294 ]\n",
      " [144.1461 ]\n",
      " [100.87303]\n",
      " [184.18375]\n",
      " [173.0128 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1850 Cost:  70.05656 \n",
      "Prediction:\n",
      " [[146.62097]\n",
      " [192.46257]\n",
      " [181.2502 ]\n",
      " [197.85567]\n",
      " [150.39803]\n",
      " [113.40397]\n",
      " [144.1618 ]\n",
      " [100.90416]\n",
      " [184.15375]\n",
      " [172.98097]]\n",
      "1860 Cost:  69.682945 \n",
      "Prediction:\n",
      " [[146.64195]\n",
      " [192.44191]\n",
      " [181.2535 ]\n",
      " [197.8593 ]\n",
      " [150.37082]\n",
      " [113.37863]\n",
      " [144.17744]\n",
      " [100.93521]\n",
      " [184.12386]\n",
      " [172.94926]]\n",
      "1870 Cost:  69.311386 \n",
      "Prediction:\n",
      " [[146.66289]\n",
      " [192.42131]\n",
      " [181.25676]\n",
      " [197.8629 ]\n",
      " [150.3437 ]\n",
      " [113.35337]\n",
      " [144.19307]\n",
      " [100.96622]\n",
      " [184.09404]\n",
      " [172.91768]]\n",
      "1880 Cost:  68.942116 \n",
      "Prediction:\n",
      " [[146.68372]\n",
      " [192.40076]\n",
      " [181.26001]\n",
      " [197.86649]\n",
      " [150.31665]\n",
      " [113.32819]\n",
      " [144.20865]\n",
      " [100.99714]\n",
      " [184.06435]\n",
      " [172.88623]]\n",
      "1890 Cost:  68.575035 \n",
      "Prediction:\n",
      " [[146.7045  ]\n",
      " [192.38028 ]\n",
      " [181.26326 ]\n",
      " [197.87009 ]\n",
      " [150.28967 ]\n",
      " [113.303116]\n",
      " [144.22421 ]\n",
      " [101.028   ]\n",
      " [184.03476 ]\n",
      " [172.8549  ]]\n",
      "1900 Cost:  68.21013 \n",
      "Prediction:\n",
      " [[146.7252  ]\n",
      " [192.35986 ]\n",
      " [181.26648 ]\n",
      " [197.87367 ]\n",
      " [150.26277 ]\n",
      " [113.278114]\n",
      " [144.23973 ]\n",
      " [101.058784]\n",
      " [184.00526 ]\n",
      " [172.8237  ]]\n",
      "1910 Cost:  67.84748 \n",
      "Prediction:\n",
      " [[146.74585 ]\n",
      " [192.33952 ]\n",
      " [181.2697  ]\n",
      " [197.87726 ]\n",
      " [150.23598 ]\n",
      " [113.253204]\n",
      " [144.25522 ]\n",
      " [101.089516]\n",
      " [183.97588 ]\n",
      " [172.79265 ]]\n",
      "1920 Cost:  67.48686 \n",
      "Prediction:\n",
      " [[146.76642]\n",
      " [192.31923]\n",
      " [181.27289]\n",
      " [197.88084]\n",
      " [150.20923]\n",
      " [113.22837]\n",
      " [144.27068]\n",
      " [101.12017]\n",
      " [183.94658]\n",
      " [172.7617 ]]\n",
      "1930 Cost:  67.1284 \n",
      "Prediction:\n",
      " [[146.78693 ]\n",
      " [192.299   ]\n",
      " [181.27606 ]\n",
      " [197.8844  ]\n",
      " [150.18256 ]\n",
      " [113.20363 ]\n",
      " [144.2861  ]\n",
      " [101.150764]\n",
      " [183.91739 ]\n",
      " [172.7309  ]]\n",
      "1940 Cost:  66.771996 \n",
      "Prediction:\n",
      " [[146.80734]\n",
      " [192.2788 ]\n",
      " [181.27922]\n",
      " [197.88794]\n",
      " [150.15594]\n",
      " [113.17896]\n",
      " [144.30148]\n",
      " [101.18128]\n",
      " [183.88829]\n",
      " [172.7002 ]]\n",
      "1950 Cost:  66.41777 \n",
      "Prediction:\n",
      " [[146.82771 ]\n",
      " [192.25868 ]\n",
      " [181.28236 ]\n",
      " [197.8915  ]\n",
      " [150.12944 ]\n",
      " [113.154396]\n",
      " [144.31685 ]\n",
      " [101.211754]\n",
      " [183.8593  ]\n",
      " [172.66963 ]]\n",
      "1960 Cost:  66.06572 \n",
      "Prediction:\n",
      " [[146.848  ]\n",
      " [192.23865]\n",
      " [181.2855 ]\n",
      " [197.89503]\n",
      " [150.10298]\n",
      " [113.12991]\n",
      " [144.33217]\n",
      " [101.24214]\n",
      " [183.83041]\n",
      " [172.6392 ]]\n",
      "1970 Cost:  65.71556 \n",
      "Prediction:\n",
      " [[146.86823 ]\n",
      " [192.21863 ]\n",
      " [181.2886  ]\n",
      " [197.89856 ]\n",
      " [150.07661 ]\n",
      " [113.10551 ]\n",
      " [144.34747 ]\n",
      " [101.272484]\n",
      " [183.80162 ]\n",
      " [172.60887 ]]\n",
      "1980 Cost:  65.36763 \n",
      "Prediction:\n",
      " [[146.8884 ]\n",
      " [192.19872]\n",
      " [181.29172]\n",
      " [197.9021 ]\n",
      " [150.05031]\n",
      " [113.08119]\n",
      " [144.36273]\n",
      " [101.30274]\n",
      " [183.77293]\n",
      " [172.57867]]\n",
      "1990 Cost:  65.02177 \n",
      "Prediction:\n",
      " [[146.90848 ]\n",
      " [192.17883 ]\n",
      " [181.29482 ]\n",
      " [197.90562 ]\n",
      " [150.02408 ]\n",
      " [113.05695 ]\n",
      " [144.37796 ]\n",
      " [101.332924]\n",
      " [183.74434 ]\n",
      " [172.54861 ]]\n",
      "2000 Cost:  64.67786 \n",
      "Prediction:\n",
      " [[146.92851]\n",
      " [192.15901]\n",
      " [181.2979 ]\n",
      " [197.90913]\n",
      " [149.99792]\n",
      " [113.03281]\n",
      " [144.39316]\n",
      " [101.36307]\n",
      " [183.71585]\n",
      " [172.51866]]\n",
      "Your score will be  [[218.46158]]\n",
      "Other scores will be  [[155.01846]\n",
      " [169.75067]]\n"
     ]
    }
   ],
   "source": [
    "# lab-04-4-tf_reader_linear_regression.py\n",
    "#\n",
    "# Lab 4 Multi-variable linear regression\n",
    "# https://www.tensorflow.org/programmers_guide/reading_data\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    ['data-01-test-score.csv'], shuffle=False, name='filename_queue')\n",
    "\n",
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "# Default values, in case of empty columns. Also specifies the type of the\n",
    "# decoded result.\n",
    "record_defaults = [[0.], [0.], [0.], [0.]]\n",
    "xy = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "\n",
    "# collect batches of csv in\n",
    "train_x_batch, train_y_batch = \\\n",
    "    tf.train.batch([xy[0:-1], xy[-1:]], batch_size=10)\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Start populating the filename queue.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "for step in range(2001):\n",
    "    x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_batch, Y: y_batch})\n",
    "    if step % 100 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "\n",
    "# Ask my score\n",
    "print(\"Your score will be \",\n",
    "      sess.run(hypothesis, feed_dict={X: [[100, 70, 101]]}))\n",
    "\n",
    "print(\"Other scores will be \",\n",
    "      sess.run(hypothesis, feed_dict={X: [[60, 70, 110], [90, 100, 80]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.86682636\n",
      "\n",
      "Hypothesis:  [[0.38586986]\n",
      " [0.3441266 ]\n",
      " [0.7703627 ]\n",
      " [0.58700204]\n",
      " [0.70053726]\n",
      " [0.88356715]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.8333333\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classifier\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(123)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2],\n",
    "          [2, 3],\n",
    "          [3, 1],\n",
    "          [4, 3],\n",
    "          [5, 3],\n",
    "          [6, 2]]\n",
    "y_data = [[0],\n",
    "          [0],\n",
    "          [0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data-03-diabetes.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(759, 8) (759, 1)\n",
      "0 1.2575243\n",
      "200 0.84477425\n",
      "400 0.7656635\n",
      "600 0.73563\n",
      "800 0.7138739\n",
      "1000 0.6948192\n",
      "1200 0.6775382\n",
      "1400 0.6617814\n",
      "1600 0.64740676\n",
      "1800 0.63429487\n",
      "2000 0.6223359\n",
      "2200 0.6114284\n",
      "2400 0.6014778\n",
      "2600 0.59239745\n",
      "2800 0.58410716\n",
      "3000 0.5765334\n",
      "3200 0.5696089\n",
      "3400 0.56327236\n",
      "3600 0.5574677\n",
      "3800 0.5521446\n",
      "4000 0.54725695\n",
      "4200 0.5427635\n",
      "4400 0.538627\n",
      "4600 0.53481394\n",
      "4800 0.53129417\n",
      "5000 0.5280405\n",
      "5200 0.5250288\n",
      "5400 0.5222374\n",
      "5600 0.51964647\n",
      "5800 0.51723844\n",
      "6000 0.51499754\n",
      "6200 0.51290953\n",
      "6400 0.51096153\n",
      "6600 0.5091418\n",
      "6800 0.50744015\n",
      "7000 0.50584686\n",
      "7200 0.50435346\n",
      "7400 0.50295216\n",
      "7600 0.5016359\n",
      "7800 0.50039816\n",
      "8000 0.49923307\n",
      "8200 0.4981355\n",
      "8400 0.49710023\n",
      "8600 0.4961231\n",
      "8800 0.4951998\n",
      "9000 0.4943267\n",
      "9200 0.4935004\n",
      "9400 0.4927176\n",
      "9600 0.49197546\n",
      "9800 0.49127132\n",
      "10000 0.49060276\n",
      "\n",
      "Hypothesis:  [[0.46971   ]\n",
      " [0.92331004]\n",
      " [0.20740955]\n",
      " [0.93559515]\n",
      " [0.35543314]\n",
      " [0.68941206]\n",
      " [0.9321822 ]\n",
      " [0.58117723]\n",
      " [0.2584246 ]\n",
      " [0.4751967 ]\n",
      " [0.6515689 ]\n",
      " [0.18818055]\n",
      " [0.29142424]\n",
      " [0.42886433]\n",
      " [0.7246062 ]\n",
      " [0.46592423]\n",
      " [0.6797292 ]\n",
      " [0.87833124]\n",
      " [0.8110232 ]\n",
      " [0.57848877]\n",
      " [0.6492794 ]\n",
      " [0.10927187]\n",
      " [0.6307262 ]\n",
      " [0.66170156]\n",
      " [0.36876822]\n",
      " [0.9290053 ]\n",
      " [0.5201321 ]\n",
      " [0.58925736]\n",
      " [0.7691161 ]\n",
      " [0.44691163]\n",
      " [0.9474865 ]\n",
      " [0.78397465]\n",
      " [0.60211223]\n",
      " [0.82327443]\n",
      " [0.35102352]\n",
      " [0.6790951 ]\n",
      " [0.83528787]\n",
      " [0.6064701 ]\n",
      " [0.45613563]\n",
      " [0.38820976]\n",
      " [0.8193995 ]\n",
      " [0.20152363]\n",
      " [0.37466747]\n",
      " [0.11961372]\n",
      " [0.5737546 ]\n",
      " [0.9334609 ]\n",
      " [0.7551874 ]\n",
      " [0.6963368 ]\n",
      " [0.9207182 ]\n",
      " [0.93216586]\n",
      " [0.91526103]\n",
      " [0.24694453]\n",
      " [0.39601746]\n",
      " [0.9725417 ]\n",
      " [0.22784014]\n",
      " [0.45750007]\n",
      " [0.198678  ]\n",
      " [0.6959942 ]\n",
      " [0.86803114]\n",
      " [0.47818345]\n",
      " [0.9378523 ]\n",
      " [0.687511  ]\n",
      " [0.6431882 ]\n",
      " [0.8486981 ]\n",
      " [0.6383583 ]\n",
      " [0.615786  ]\n",
      " [0.94491494]\n",
      " [0.62721455]\n",
      " [0.8737273 ]\n",
      " [0.6318275 ]\n",
      " [0.29099706]\n",
      " [0.7198521 ]\n",
      " [0.91646636]\n",
      " [0.93198884]\n",
      " [0.859984  ]\n",
      " [0.81067586]\n",
      " [0.45266244]\n",
      " [0.867529  ]\n",
      " [0.91086274]\n",
      " [0.9014065 ]\n",
      " [0.8577928 ]\n",
      " [0.81490433]\n",
      " [0.3551445 ]\n",
      " [0.80871195]\n",
      " [0.51115316]\n",
      " [0.8720564 ]\n",
      " [0.4192692 ]\n",
      " [0.8992033 ]\n",
      " [0.92132246]\n",
      " [0.78452504]\n",
      " [0.8011481 ]\n",
      " [0.61234987]\n",
      " [0.7165272 ]\n",
      " [0.5921565 ]\n",
      " [0.9054392 ]\n",
      " [0.9715597 ]\n",
      " [0.8798206 ]\n",
      " [0.57679373]\n",
      " [0.31043008]\n",
      " [0.5879371 ]\n",
      " [0.5161349 ]\n",
      " [0.948682  ]\n",
      " [0.8015522 ]\n",
      " [0.7914609 ]\n",
      " [0.78127724]\n",
      " [0.6839253 ]\n",
      " [0.91202956]\n",
      " [0.7933274 ]\n",
      " [0.45547494]\n",
      " [0.45735925]\n",
      " [0.9023749 ]\n",
      " [0.86209166]\n",
      " [0.46289617]\n",
      " [0.41679865]\n",
      " [0.60093004]\n",
      " [0.8483816 ]\n",
      " [0.88198066]\n",
      " [0.9093025 ]\n",
      " [0.15822473]\n",
      " [0.72827286]\n",
      " [0.83266187]\n",
      " [0.5803892 ]\n",
      " [0.58374596]\n",
      " [0.8698863 ]\n",
      " [0.7290357 ]\n",
      " [0.81613946]\n",
      " [0.8043349 ]\n",
      " [0.6033464 ]\n",
      " [0.5380806 ]\n",
      " [0.4750865 ]\n",
      " [0.43172935]\n",
      " [0.81218916]\n",
      " [0.92649746]\n",
      " [0.83641326]\n",
      " [0.7934109 ]\n",
      " [0.84460205]\n",
      " [0.4327076 ]\n",
      " [0.8043819 ]\n",
      " [0.66903794]\n",
      " [0.7571532 ]\n",
      " [0.8772632 ]\n",
      " [0.61712104]\n",
      " [0.5728141 ]\n",
      " [0.76347274]\n",
      " [0.8973427 ]\n",
      " [0.7774418 ]\n",
      " [0.44548598]\n",
      " [0.93079764]\n",
      " [0.58319354]\n",
      " [0.7344966 ]\n",
      " [0.31252825]\n",
      " [0.43111566]\n",
      " [0.13259302]\n",
      " [0.28442279]\n",
      " [0.9252109 ]\n",
      " [0.8826535 ]\n",
      " [0.9283045 ]\n",
      " [0.13736667]\n",
      " [0.4841308 ]\n",
      " [0.74555606]\n",
      " [0.6234776 ]\n",
      " [0.89127356]\n",
      " [0.39426807]\n",
      " [0.8241316 ]\n",
      " [0.64815336]\n",
      " [0.63274175]\n",
      " [0.7080749 ]\n",
      " [0.8517179 ]\n",
      " [0.7130179 ]\n",
      " [0.6495921 ]\n",
      " [0.90887845]\n",
      " [0.8809382 ]\n",
      " [0.94426185]\n",
      " [0.21926682]\n",
      " [0.7891484 ]\n",
      " [0.27092588]\n",
      " [0.41400275]\n",
      " [0.43534723]\n",
      " [0.82180923]\n",
      " [0.7124419 ]\n",
      " [0.9130064 ]\n",
      " [0.8854334 ]\n",
      " [0.54109806]\n",
      " [0.20517202]\n",
      " [0.24775882]\n",
      " [0.56848884]\n",
      " [0.6860633 ]\n",
      " [0.5818769 ]\n",
      " [0.793624  ]\n",
      " [0.5689428 ]\n",
      " [0.34598905]\n",
      " [0.30278575]\n",
      " [0.9106052 ]\n",
      " [0.36599132]\n",
      " [0.84283626]\n",
      " [0.8960134 ]\n",
      " [0.7101205 ]\n",
      " [0.6559144 ]\n",
      " [0.71326506]\n",
      " [0.54385674]\n",
      " [0.7628957 ]\n",
      " [0.92803603]\n",
      " [0.78195745]\n",
      " [0.80340385]\n",
      " [0.17861411]\n",
      " [0.27775007]\n",
      " [0.9052182 ]\n",
      " [0.23331997]\n",
      " [0.9388177 ]\n",
      " [0.26609266]\n",
      " [0.27064714]\n",
      " [0.52529275]\n",
      " [0.67705464]\n",
      " [0.26043516]\n",
      " [0.7382927 ]\n",
      " [0.7034031 ]\n",
      " [0.85536534]\n",
      " [0.67319787]\n",
      " [0.25066495]\n",
      " [0.36022192]\n",
      " [0.66312116]\n",
      " [0.5649729 ]\n",
      " [0.91906786]\n",
      " [0.9164228 ]\n",
      " [0.6571904 ]\n",
      " [0.45018265]\n",
      " [0.0962911 ]\n",
      " [0.6374705 ]\n",
      " [0.37458634]\n",
      " [0.5120452 ]\n",
      " [0.9318142 ]\n",
      " [0.62522376]\n",
      " [0.9342929 ]\n",
      " [0.2558598 ]\n",
      " [0.19022201]\n",
      " [0.3390605 ]\n",
      " [0.68076974]\n",
      " [0.92426   ]\n",
      " [0.86293215]\n",
      " [0.6154038 ]\n",
      " [0.7455804 ]\n",
      " [0.5824324 ]\n",
      " [0.23011653]\n",
      " [0.5271175 ]\n",
      " [0.2085061 ]\n",
      " [0.603893  ]\n",
      " [0.8618795 ]\n",
      " [0.6779362 ]\n",
      " [0.6299014 ]\n",
      " [0.9341018 ]\n",
      " [0.822674  ]\n",
      " [0.83715653]\n",
      " [0.80263644]\n",
      " [0.78655046]\n",
      " [0.8610109 ]\n",
      " [0.4835582 ]\n",
      " [0.44236928]\n",
      " [0.5257648 ]\n",
      " [0.8315893 ]\n",
      " [0.7180199 ]\n",
      " [0.66520816]\n",
      " [0.8431455 ]\n",
      " [0.3711439 ]\n",
      " [0.5839164 ]\n",
      " [0.65783966]\n",
      " [0.5839318 ]\n",
      " [0.5274163 ]\n",
      " [0.87904346]\n",
      " [0.6938739 ]\n",
      " [0.8995553 ]\n",
      " [0.54899347]\n",
      " [0.74326265]\n",
      " [0.8283818 ]\n",
      " [0.8099792 ]\n",
      " [0.65835327]\n",
      " [0.89026034]\n",
      " [0.35329214]\n",
      " [0.5791407 ]\n",
      " [0.67770934]\n",
      " [0.31217623]\n",
      " [0.76773435]\n",
      " [0.33106282]\n",
      " [0.6432345 ]\n",
      " [0.9253547 ]\n",
      " [0.73858345]\n",
      " [0.83647937]\n",
      " [0.71573335]\n",
      " [0.56508124]\n",
      " [0.68644935]\n",
      " [0.39979625]\n",
      " [0.49595085]\n",
      " [0.61905074]\n",
      " [0.5976845 ]\n",
      " [0.6692048 ]\n",
      " [0.6083311 ]\n",
      " [0.23459478]\n",
      " [0.673572  ]\n",
      " [0.86803377]\n",
      " [0.50375026]\n",
      " [0.535278  ]\n",
      " [0.7366183 ]\n",
      " [0.46370167]\n",
      " [0.7208712 ]\n",
      " [0.5936912 ]\n",
      " [0.739712  ]\n",
      " [0.8978889 ]\n",
      " [0.6656627 ]\n",
      " [0.6801247 ]\n",
      " [0.8876077 ]\n",
      " [0.641393  ]\n",
      " [0.8525683 ]\n",
      " [0.92277956]\n",
      " [0.27607027]\n",
      " [0.7644693 ]\n",
      " [0.20779015]\n",
      " [0.80926704]\n",
      " [0.806668  ]\n",
      " [0.7211234 ]\n",
      " [0.30514815]\n",
      " [0.8062217 ]\n",
      " [0.7008855 ]\n",
      " [0.76551306]\n",
      " [0.18109357]\n",
      " [0.7934343 ]\n",
      " [0.81545144]\n",
      " [0.6742247 ]\n",
      " [0.9414982 ]\n",
      " [0.30676568]\n",
      " [0.6093293 ]\n",
      " [0.9382394 ]\n",
      " [0.23958035]\n",
      " [0.52741784]\n",
      " [0.65507036]\n",
      " [0.34897643]\n",
      " [0.18232617]\n",
      " [0.8385752 ]\n",
      " [0.90847224]\n",
      " [0.8578382 ]\n",
      " [0.5721617 ]\n",
      " [0.68630046]\n",
      " [0.5499996 ]\n",
      " [0.7984674 ]\n",
      " [0.7607256 ]\n",
      " [0.9182645 ]\n",
      " [0.7734056 ]\n",
      " [0.7539467 ]\n",
      " [0.52082926]\n",
      " [0.93786573]\n",
      " [0.9414417 ]\n",
      " [0.7608875 ]\n",
      " [0.23866218]\n",
      " [0.72499907]\n",
      " [0.4676096 ]\n",
      " [0.72554064]\n",
      " [0.23765454]\n",
      " [0.28212297]\n",
      " [0.41305563]\n",
      " [0.6919376 ]\n",
      " [0.41884848]\n",
      " [0.59861463]\n",
      " [0.8530799 ]\n",
      " [0.5975857 ]\n",
      " [0.8641688 ]\n",
      " [0.93088335]\n",
      " [0.70210433]\n",
      " [0.1514711 ]\n",
      " [0.6066399 ]\n",
      " [0.8642144 ]\n",
      " [0.83738375]\n",
      " [0.7053495 ]\n",
      " [0.3123683 ]\n",
      " [0.8514345 ]\n",
      " [0.8908144 ]\n",
      " [0.33654398]\n",
      " [0.61153775]\n",
      " [0.8317982 ]\n",
      " [0.8293491 ]\n",
      " [0.90454215]\n",
      " [0.9148842 ]\n",
      " [0.8407628 ]\n",
      " [0.91005033]\n",
      " [0.72201896]\n",
      " [0.6248861 ]\n",
      " [0.584818  ]\n",
      " [0.85112995]\n",
      " [0.8808983 ]\n",
      " [0.25979123]\n",
      " [0.8441122 ]\n",
      " [0.8645866 ]\n",
      " [0.3328041 ]\n",
      " [0.7169669 ]\n",
      " [0.86500525]\n",
      " [0.54873526]\n",
      " [0.87493783]\n",
      " [0.30419093]\n",
      " [0.80942225]\n",
      " [0.554451  ]\n",
      " [0.8819795 ]\n",
      " [0.35126588]\n",
      " [0.76018184]\n",
      " [0.6918933 ]\n",
      " [0.7340533 ]\n",
      " [0.10977235]\n",
      " [0.29402146]\n",
      " [0.680383  ]\n",
      " [0.8167473 ]\n",
      " [0.5652928 ]\n",
      " [0.76291174]\n",
      " [0.5318857 ]\n",
      " [0.3732347 ]\n",
      " [0.87971234]\n",
      " [0.5146228 ]\n",
      " [0.88912   ]\n",
      " [0.78140354]\n",
      " [0.661077  ]\n",
      " [0.91862804]\n",
      " [0.67099583]\n",
      " [0.8425358 ]\n",
      " [0.38281503]\n",
      " [0.27705166]\n",
      " [0.7332516 ]\n",
      " [0.45482922]\n",
      " [0.39295962]\n",
      " [0.8911857 ]\n",
      " [0.8542674 ]\n",
      " [0.9131244 ]\n",
      " [0.9482282 ]\n",
      " [0.61206776]\n",
      " [0.9268261 ]\n",
      " [0.3983688 ]\n",
      " [0.38636696]\n",
      " [0.44006172]\n",
      " [0.9391908 ]\n",
      " [0.5861501 ]\n",
      " [0.18436675]\n",
      " [0.92422825]\n",
      " [0.8029011 ]\n",
      " [0.58736956]\n",
      " [0.8478586 ]\n",
      " [0.04021763]\n",
      " [0.917356  ]\n",
      " [0.70876855]\n",
      " [0.7281648 ]\n",
      " [0.7445045 ]\n",
      " [0.95808226]\n",
      " [0.6000544 ]\n",
      " [0.7871295 ]\n",
      " [0.7339849 ]\n",
      " [0.872046  ]\n",
      " [0.23228236]\n",
      " [0.6581095 ]\n",
      " [0.8909006 ]\n",
      " [0.5892017 ]\n",
      " [0.7261916 ]\n",
      " [0.92295057]\n",
      " [0.83327866]\n",
      " [0.871963  ]\n",
      " [0.44966123]\n",
      " [0.79968184]\n",
      " [0.9422692 ]\n",
      " [0.7478864 ]\n",
      " [0.6159647 ]\n",
      " [0.33856973]\n",
      " [0.51362437]\n",
      " [0.5185177 ]\n",
      " [0.6704252 ]\n",
      " [0.45945773]\n",
      " [0.7320572 ]\n",
      " [0.58015764]\n",
      " [0.74773395]\n",
      " [0.8059572 ]\n",
      " [0.6533682 ]\n",
      " [0.6602085 ]\n",
      " [0.52949035]\n",
      " [0.56111103]\n",
      " [0.9274604 ]\n",
      " [0.81695133]\n",
      " [0.32300672]\n",
      " [0.45717603]\n",
      " [0.60393256]\n",
      " [0.18798622]\n",
      " [0.89103234]\n",
      " [0.1453395 ]\n",
      " [0.9036705 ]\n",
      " [0.8714605 ]\n",
      " [0.8269437 ]\n",
      " [0.74117184]\n",
      " [0.8729933 ]\n",
      " [0.3375772 ]\n",
      " [0.72572553]\n",
      " [0.9371462 ]\n",
      " [0.27635822]\n",
      " [0.45290062]\n",
      " [0.84280515]\n",
      " [0.88237923]\n",
      " [0.7156615 ]\n",
      " [0.8321868 ]\n",
      " [0.81535745]\n",
      " [0.7659821 ]\n",
      " [0.26151994]\n",
      " [0.7978279 ]\n",
      " [0.9235773 ]\n",
      " [0.581199  ]\n",
      " [0.79724616]\n",
      " [0.7101932 ]\n",
      " [0.7724838 ]\n",
      " [0.84389573]\n",
      " [0.9222042 ]\n",
      " [0.6063178 ]\n",
      " [0.4053901 ]\n",
      " [0.76088977]\n",
      " [0.7711925 ]\n",
      " [0.9493796 ]\n",
      " [0.7245818 ]\n",
      " [0.67511016]\n",
      " [0.43420717]\n",
      " [0.6890228 ]\n",
      " [0.934096  ]\n",
      " [0.9384216 ]\n",
      " [0.87051845]\n",
      " [0.6878573 ]\n",
      " [0.6281645 ]\n",
      " [0.82754153]\n",
      " [0.5195695 ]\n",
      " [0.81076545]\n",
      " [0.78550655]\n",
      " [0.9121836 ]\n",
      " [0.5964332 ]\n",
      " [0.6327793 ]\n",
      " [0.8904455 ]\n",
      " [0.4953251 ]\n",
      " [0.4933705 ]\n",
      " [0.68207496]\n",
      " [0.7365286 ]\n",
      " [0.71257377]\n",
      " [0.8853044 ]\n",
      " [0.9049947 ]\n",
      " [0.1966176 ]\n",
      " [0.2216752 ]\n",
      " [0.76014245]\n",
      " [0.46125016]\n",
      " [0.21930768]\n",
      " [0.8457366 ]\n",
      " [0.89555573]\n",
      " [0.642531  ]\n",
      " [0.9280782 ]\n",
      " [0.92353296]\n",
      " [0.71323174]\n",
      " [0.8432754 ]\n",
      " [0.6524514 ]\n",
      " [0.62430483]\n",
      " [0.7215538 ]\n",
      " [0.5824376 ]\n",
      " [0.1604781 ]\n",
      " [0.89368325]\n",
      " [0.878196  ]\n",
      " [0.68413174]\n",
      " [0.92558086]\n",
      " [0.8671216 ]\n",
      " [0.87039006]\n",
      " [0.5693511 ]\n",
      " [0.6851004 ]\n",
      " [0.8682836 ]\n",
      " [0.6277982 ]\n",
      " [0.83623827]\n",
      " [0.9138921 ]\n",
      " [0.5993142 ]\n",
      " [0.7860318 ]\n",
      " [0.8554543 ]\n",
      " [0.60128486]\n",
      " [0.47457957]\n",
      " [0.07110374]\n",
      " [0.28082904]\n",
      " [0.8294177 ]\n",
      " [0.6770997 ]\n",
      " [0.66411555]\n",
      " [0.64449924]\n",
      " [0.94532865]\n",
      " [0.43208897]\n",
      " [0.75822794]\n",
      " [0.3302095 ]\n",
      " [0.85755044]\n",
      " [0.44299603]\n",
      " [0.7157808 ]\n",
      " [0.5583333 ]\n",
      " [0.90331215]\n",
      " [0.5749886 ]\n",
      " [0.27072075]\n",
      " [0.80119556]\n",
      " [0.95499194]\n",
      " [0.3669723 ]\n",
      " [0.924759  ]\n",
      " [0.8631792 ]\n",
      " [0.8053588 ]\n",
      " [0.7807848 ]\n",
      " [0.45828518]\n",
      " [0.30108967]\n",
      " [0.7660443 ]\n",
      " [0.2349601 ]\n",
      " [0.9318145 ]\n",
      " [0.3435591 ]\n",
      " [0.9174428 ]\n",
      " [0.88588303]\n",
      " [0.4949451 ]\n",
      " [0.22590223]\n",
      " [0.7338051 ]\n",
      " [0.48328996]\n",
      " [0.7793878 ]\n",
      " [0.60078835]\n",
      " [0.97200173]\n",
      " [0.6363936 ]\n",
      " [0.6008253 ]\n",
      " [0.73513395]\n",
      " [0.8556246 ]\n",
      " [0.10411931]\n",
      " [0.7885839 ]\n",
      " [0.7899809 ]\n",
      " [0.7829187 ]\n",
      " [0.5819741 ]\n",
      " [0.45034137]\n",
      " [0.5664986 ]\n",
      " [0.90832454]\n",
      " [0.62386245]\n",
      " [0.7149677 ]\n",
      " [0.78087753]\n",
      " [0.8300685 ]\n",
      " [0.75360775]\n",
      " [0.5460001 ]\n",
      " [0.7492257 ]\n",
      " [0.8969803 ]\n",
      " [0.732083  ]\n",
      " [0.93892187]\n",
      " [0.7846069 ]\n",
      " [0.5934653 ]\n",
      " [0.43956974]\n",
      " [0.8173294 ]\n",
      " [0.83377355]\n",
      " [0.5137781 ]\n",
      " [0.6400128 ]\n",
      " [0.2772935 ]\n",
      " [0.4885938 ]\n",
      " [0.81037676]\n",
      " [0.9369273 ]\n",
      " [0.8466274 ]\n",
      " [0.672079  ]\n",
      " [0.7373556 ]\n",
      " [0.87035745]\n",
      " [0.613768  ]\n",
      " [0.8968193 ]\n",
      " [0.60087115]\n",
      " [0.85176957]\n",
      " [0.26547214]\n",
      " [0.13928588]\n",
      " [0.21965358]\n",
      " [0.3344397 ]\n",
      " [0.7213932 ]\n",
      " [0.7671206 ]\n",
      " [0.6591    ]\n",
      " [0.7402217 ]\n",
      " [0.8248246 ]\n",
      " [0.45818758]\n",
      " [0.43474734]\n",
      " [0.93102515]\n",
      " [0.85937333]\n",
      " [0.4917503 ]\n",
      " [0.68554044]\n",
      " [0.17527366]\n",
      " [0.32150346]\n",
      " [0.7329437 ]\n",
      " [0.70816875]\n",
      " [0.90882415]\n",
      " [0.9692272 ]\n",
      " [0.26304558]\n",
      " [0.73262614]\n",
      " [0.5742633 ]\n",
      " [0.46515572]\n",
      " [0.7326753 ]\n",
      " [0.6681748 ]\n",
      " [0.9116556 ]\n",
      " [0.7090021 ]\n",
      " [0.5027843 ]\n",
      " [0.608801  ]\n",
      " [0.13776617]\n",
      " [0.71077013]\n",
      " [0.5142989 ]\n",
      " [0.87663484]\n",
      " [0.57143337]\n",
      " [0.55914205]\n",
      " [0.74064463]\n",
      " [0.7462219 ]\n",
      " [0.5449352 ]\n",
      " [0.7589464 ]\n",
      " [0.66420156]\n",
      " [0.4085945 ]\n",
      " [0.6359764 ]\n",
      " [0.8572113 ]\n",
      " [0.839681  ]\n",
      " [0.5766655 ]\n",
      " [0.8303966 ]\n",
      " [0.26570073]\n",
      " [0.8599213 ]\n",
      " [0.6611391 ]\n",
      " [0.7210018 ]\n",
      " [0.5040188 ]\n",
      " [0.69618005]\n",
      " [0.78856504]\n",
      " [0.25096235]\n",
      " [0.3124612 ]\n",
      " [0.83317155]\n",
      " [0.7972253 ]\n",
      " [0.82729226]\n",
      " [0.90571   ]\n",
      " [0.80324984]\n",
      " [0.6839204 ]\n",
      " [0.7124616 ]\n",
      " [0.71809906]\n",
      " [0.709605  ]\n",
      " [0.7903465 ]\n",
      " [0.5026879 ]\n",
      " [0.33852988]\n",
      " [0.8667865 ]\n",
      " [0.7641224 ]\n",
      " [0.5498751 ]\n",
      " [0.30016252]\n",
      " [0.8759408 ]\n",
      " [0.78132683]\n",
      " [0.85552514]\n",
      " [0.6325939 ]\n",
      " [0.90533346]\n",
      " [0.9005814 ]\n",
      " [0.773969  ]\n",
      " [0.4664994 ]\n",
      " [0.9052472 ]\n",
      " [0.90656585]\n",
      " [0.31311595]\n",
      " [0.19282673]\n",
      " [0.68095976]\n",
      " [0.44289172]\n",
      " [0.8313475 ]\n",
      " [0.38067964]\n",
      " [0.41166654]\n",
      " [0.46040446]\n",
      " [0.73622614]\n",
      " [0.87053204]\n",
      " [0.15599011]\n",
      " [0.41156584]\n",
      " [0.60813296]\n",
      " [0.47020513]\n",
      " [0.52127343]\n",
      " [0.76301765]\n",
      " [0.15749383]\n",
      " [0.9126128 ]\n",
      " [0.26149994]\n",
      " [0.79790354]\n",
      " [0.67166895]\n",
      " [0.76790816]\n",
      " [0.7961613 ]\n",
      " [0.7107759 ]\n",
      " [0.8922264 ]] \n",
      "Correct (Y):  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.7641634\n"
     ]
    }
   ],
   "source": [
    "# lab-05-2-logistic_regression_diabetes.py\n",
    "#\n",
    "# Lab 5 Logistic Regression Classifier\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([8, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(-tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.5442638\n",
      "200 0.59190804\n",
      "400 0.48211646\n",
      "600 0.38764507\n",
      "800 0.29590315\n",
      "1000 0.23220599\n",
      "1200 0.21033314\n",
      "1400 0.19217286\n",
      "1600 0.17682613\n",
      "1800 0.16368735\n",
      "2000 0.152316\n",
      "--------------\n",
      "[[2.9140569e-03 9.9707472e-01 1.1208374e-05]] [1]\n",
      "--------------\n",
      "[[0.8906692  0.09332813 0.01600266]] [0]\n",
      "--------------\n",
      "[[1.34138745e-08 3.43548803e-04 9.99656439e-01]] [2]\n",
      "--------------\n",
      "[[2.91405665e-03 9.97074723e-01 1.12083744e-05]\n",
      " [8.90669227e-01 9.33281332e-02 1.60026550e-02]\n",
      " [1.34138745e-08 3.43548803e-04 9.99656439e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# softmax_classifier.py \n",
    "# \n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(123)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "\n",
    "    print('--------------')\n",
    "\n",
    "    # Testing & One-hot encoding\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n",
    "    print(a, sess.run(tf.argmax(a, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n",
    "    print(b, sess.run(tf.argmax(b, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n",
    "    print(c, sess.run(tf.argmax(c, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "\n",
    "    all = sess.run(hypothesis, feed_dict={\n",
    "                   X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n",
    "    print(all, sess.run(tf.argmax(all, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data-04-zoo.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 1)\n",
      "one_hot Tensor(\"one_hot:0\", shape=(?, 1, 7), dtype=float32)\n",
      "reshape Tensor(\"Reshape:0\", shape=(?, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Lab 6 Softmax Classifier\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(123)  # for reproducibility\n",
    "\n",
    "# Predicting animal type based on various features\n",
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "nb_classes = 7  # 0 ~ 6\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 16])\n",
    "Y = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 6\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
    "print(\"one_hot\", Y_one_hot)\n",
    "\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "print(\"reshape\", Y_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 1)\n",
      "one_hot Tensor(\"one_hot:0\", shape=(?, 1, 7), dtype=float32)\n",
      "reshape Tensor(\"Reshape:0\", shape=(?, 7), dtype=float32)\n",
      "Step:     0\tLoss: 5.873\tAcc: 12.87%\n",
      "Step:   100\tLoss: 0.685\tAcc: 87.13%\n",
      "Step:   200\tLoss: 0.437\tAcc: 88.12%\n",
      "Step:   300\tLoss: 0.322\tAcc: 92.08%\n",
      "Step:   400\tLoss: 0.250\tAcc: 93.07%\n",
      "Step:   500\tLoss: 0.201\tAcc: 93.07%\n",
      "Step:   600\tLoss: 0.168\tAcc: 96.04%\n",
      "Step:   700\tLoss: 0.143\tAcc: 99.01%\n",
      "Step:   800\tLoss: 0.125\tAcc: 99.01%\n",
      "Step:   900\tLoss: 0.111\tAcc: 99.01%\n",
      "Step:  1000\tLoss: 0.100\tAcc: 99.01%\n",
      "Step:  1100\tLoss: 0.091\tAcc: 100.00%\n",
      "Step:  1200\tLoss: 0.084\tAcc: 100.00%\n",
      "Step:  1300\tLoss: 0.078\tAcc: 100.00%\n",
      "Step:  1400\tLoss: 0.072\tAcc: 100.00%\n",
      "Step:  1500\tLoss: 0.068\tAcc: 100.00%\n",
      "Step:  1600\tLoss: 0.064\tAcc: 100.00%\n",
      "Step:  1700\tLoss: 0.060\tAcc: 100.00%\n",
      "Step:  1800\tLoss: 0.057\tAcc: 100.00%\n",
      "Step:  1900\tLoss: 0.054\tAcc: 100.00%\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n"
     ]
    }
   ],
   "source": [
    "# lab-06-2-softmax_zoo_classifier.py\n",
    "#\n",
    "# Lab 6 Softmax Classifier\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(7)  # for reproducibility\n",
    "\n",
    "# Predicting animal type based on various features\n",
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "nb_classes = 7  # 0 ~ 6\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 16])\n",
    "Y = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 6\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
    "print(\"one_hot\", Y_one_hot)\n",
    "\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "print(\"reshape\", Y_one_hot)\n",
    "\n",
    "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                 labels=tf.stop_gradient([Y_one_hot]))\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2000):\n",
    "        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={\n",
    "                                 X: x_data, Y: y_data})\n",
    "            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(\n",
    "                step, loss, acc))\n",
    "\n",
    "    # Let's see if we can predict\n",
    "    pred = sess.run(prediction, feed_dict={X: x_data})\n",
    "    # y_data: (N,1) = flatten => (N, ) matches pred.shape\n",
    "    for p, y in zip(pred, y_data.flatten()):\n",
    "        print(\"[{}] Prediction: {} True Y: {}\".format(p == int(y), p, int(y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-29-ff5e7fc5a634>:51: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "0 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "1 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "2 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "3 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "4 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "5 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "6 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "7 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "8 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "9 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "10 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "11 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "12 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "13 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "14 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "15 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "16 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "17 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "18 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "19 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "20 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "21 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "22 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "23 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "24 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "25 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "26 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "27 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "28 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "29 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "30 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "31 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "32 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "33 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "34 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "35 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "36 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "37 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "38 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "39 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "40 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "41 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "42 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "43 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "44 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "45 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "46 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "47 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "48 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "49 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "50 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "51 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "52 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "53 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "54 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "55 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "56 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "57 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "58 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "59 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "60 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "61 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "62 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "63 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "64 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "65 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "66 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "67 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "68 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "69 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "70 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "71 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "72 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "73 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "74 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "75 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "76 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "77 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "78 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "79 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "80 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "81 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "82 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "83 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "84 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "85 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "86 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "87 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "88 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "89 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "90 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "91 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "92 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "93 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "94 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "95 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "96 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "97 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "98 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "99 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "100 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "101 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "102 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "103 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "104 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "105 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "106 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "107 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "108 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "109 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "110 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "111 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "112 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "113 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "114 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "115 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "116 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "117 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "118 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "119 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "120 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "121 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "122 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "123 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "124 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "125 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "126 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "127 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "128 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "129 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "130 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "131 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "132 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "133 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "134 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "135 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "136 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "137 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "138 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "139 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "140 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "142 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "143 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "144 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "145 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "146 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "147 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "148 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "149 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "150 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "151 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "152 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "153 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "154 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "155 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "156 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "157 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "158 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "159 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "160 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "161 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "162 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "163 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "164 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "165 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "166 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "167 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "168 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "169 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "170 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "171 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "172 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "173 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "174 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "175 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "176 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "177 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "178 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "179 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "180 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "181 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "182 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "183 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "184 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "185 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "186 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "187 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "188 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "189 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "190 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "191 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "192 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "193 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "194 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "195 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "196 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "197 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "198 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "199 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "200 3.817701 [[ 0.6425708   0.5445153   0.96312946]\n",
      " [-0.0726442  -0.45325112  0.34830692]\n",
      " [ 1.7726868   0.7557273  -1.1844008 ]]\n",
      "Prediction: [2 0 0]\n",
      "Accuracy:  0.33333334\n"
     ]
    }
   ],
   "source": [
    "# lab-07-1-learning_rate_and_evaluation.py \n",
    "#\n",
    "\n",
    "# Lab 7 Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=1e-10).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.arg_max(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run(\n",
    "            [cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  4932826600000.0 \n",
      "Prediction:\n",
      " [[-1566723.8]\n",
      " [-3152018.5]\n",
      " [-2479978. ]\n",
      " [-1739010.5]\n",
      " [-2049185.4]\n",
      " [-2066413.9]\n",
      " [-1894081.6]\n",
      " [-2411025.8]]\n",
      "1 Cost:  5.4196e+27 \n",
      "Prediction:\n",
      " [[5.1929594e+13]\n",
      " [1.0453960e+14]\n",
      " [8.2237526e+13]\n",
      " [5.7648074e+13]\n",
      " [6.7941333e+13]\n",
      " [6.8513184e+13]\n",
      " [6.2794704e+13]\n",
      " [7.9950137e+13]]\n",
      "2 Cost:  inf \n",
      "Prediction:\n",
      " [[-1.7212766e+21]\n",
      " [-3.4651061e+21]\n",
      " [-2.7258740e+21]\n",
      " [-1.9108233e+21]\n",
      " [-2.2520074e+21]\n",
      " [-2.2709621e+21]\n",
      " [-2.0814153e+21]\n",
      " [-2.6500554e+21]]\n",
      "3 Cost:  inf \n",
      "Prediction:\n",
      " [[5.7054036e+28]\n",
      " [1.1485562e+29]\n",
      " [9.0352773e+28]\n",
      " [6.3336818e+28]\n",
      " [7.4645824e+28]\n",
      " [7.5274101e+28]\n",
      " [6.8991314e+28]\n",
      " [8.7839662e+28]]\n",
      "4 Cost:  inf \n",
      "Prediction:\n",
      " [[-1.8911333e+36]\n",
      " [-3.8070450e+36]\n",
      " [-2.9948648e+36]\n",
      " [-2.0993844e+36]\n",
      " [-2.4742367e+36]\n",
      " [-2.4950619e+36]\n",
      " [-2.2868105e+36]\n",
      " [-2.9115643e+36]]\n",
      "5 Cost:  inf \n",
      "Prediction:\n",
      " [[inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]]\n",
      "6 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "7 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "8 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "9 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "10 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "11 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "12 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "13 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "14 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "15 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "16 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "17 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "18 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "19 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "20 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "21 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "22 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "23 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "24 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "25 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "26 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "27 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "28 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "29 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "30 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "31 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "32 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "33 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "34 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "35 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "36 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "37 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "38 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "39 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "40 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "41 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "42 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "43 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "44 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "45 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "46 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "47 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "48 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "49 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "50 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "51 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "52 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "53 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "54 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "55 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "56 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "57 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "58 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "59 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "60 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "61 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "62 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "63 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "64 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "65 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "66 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "67 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "68 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "69 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "70 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "71 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "72 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "73 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "74 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "75 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "76 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "77 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "78 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "79 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "80 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "81 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "82 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "83 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "84 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "85 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "86 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "87 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "88 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "89 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "90 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "91 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "92 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "93 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "94 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "95 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "96 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "97 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "98 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "99 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "100 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "# lab-07-2-linear_regression_without_min_max.py \n",
    "# \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
      " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
      "0 Cost:  7.387017 \n",
      "Prediction:\n",
      " [[-3.263462  ]\n",
      " [-3.691544  ]\n",
      " [-2.357647  ]\n",
      " [-0.87667274]\n",
      " [-1.7454928 ]\n",
      " [-1.6773258 ]\n",
      " [-0.00941563]\n",
      " [-0.32243186]]\n",
      "1 Cost:  7.386514 \n",
      "Prediction:\n",
      " [[-3.2633286 ]\n",
      " [-3.6914167 ]\n",
      " [-2.3575413 ]\n",
      " [-0.87659264]\n",
      " [-1.7453966 ]\n",
      " [-1.6772332 ]\n",
      " [-0.00935644]\n",
      " [-0.3223741 ]]\n",
      "2 Cost:  7.386012 \n",
      "Prediction:\n",
      " [[-3.2631955 ]\n",
      " [-3.691289  ]\n",
      " [-2.3574362 ]\n",
      " [-0.8765125 ]\n",
      " [-1.7453007 ]\n",
      " [-1.6771406 ]\n",
      " [-0.00929725]\n",
      " [-0.3223164 ]]\n",
      "3 Cost:  7.385511 \n",
      "Prediction:\n",
      " [[-3.2630622 ]\n",
      " [-3.6911619 ]\n",
      " [-2.3573308 ]\n",
      " [-0.87643254]\n",
      " [-1.7452044 ]\n",
      " [-1.6770482 ]\n",
      " [-0.009238  ]\n",
      " [-0.3222587 ]]\n",
      "4 Cost:  7.3850083 \n",
      "Prediction:\n",
      " [[-3.262929  ]\n",
      " [-3.6910343 ]\n",
      " [-2.3572254 ]\n",
      " [-0.8763524 ]\n",
      " [-1.7451082 ]\n",
      " [-1.6769553 ]\n",
      " [-0.00917888]\n",
      " [-0.322201  ]]\n",
      "5 Cost:  7.3845053 \n",
      "Prediction:\n",
      " [[-3.2627954 ]\n",
      " [-3.6909065 ]\n",
      " [-2.3571203 ]\n",
      " [-0.87627244]\n",
      " [-1.745012  ]\n",
      " [-1.676863  ]\n",
      " [-0.00911963]\n",
      " [-0.32214326]]\n",
      "6 Cost:  7.384004 \n",
      "Prediction:\n",
      " [[-3.2626624 ]\n",
      " [-3.6907792 ]\n",
      " [-2.3570151 ]\n",
      " [-0.87619245]\n",
      " [-1.7449161 ]\n",
      " [-1.6767701 ]\n",
      " [-0.00906044]\n",
      " [-0.3220855 ]]\n",
      "7 Cost:  7.383502 \n",
      "Prediction:\n",
      " [[-3.2625291 ]\n",
      " [-3.6906512 ]\n",
      " [-2.35691   ]\n",
      " [-0.87611234]\n",
      " [-1.7448199 ]\n",
      " [-1.6766775 ]\n",
      " [-0.00900126]\n",
      " [-0.3220278 ]]\n",
      "8 Cost:  7.3829994 \n",
      "Prediction:\n",
      " [[-3.2623959 ]\n",
      " [-3.6905236 ]\n",
      " [-2.3568044 ]\n",
      " [-0.87603223]\n",
      " [-1.7447239 ]\n",
      " [-1.6765851 ]\n",
      " [-0.00894207]\n",
      " [-0.32197005]]\n",
      "9 Cost:  7.382498 \n",
      "Prediction:\n",
      " [[-3.2622623 ]\n",
      " [-3.6903963 ]\n",
      " [-2.3566995 ]\n",
      " [-0.875952  ]\n",
      " [-1.7446275 ]\n",
      " [-1.6764925 ]\n",
      " [-0.00888282]\n",
      " [-0.32191235]]\n",
      "10 Cost:  7.3819957 \n",
      "Prediction:\n",
      " [[-3.2621293 ]\n",
      " [-3.6902685 ]\n",
      " [-2.356594  ]\n",
      " [-0.87587214]\n",
      " [-1.7445315 ]\n",
      " [-1.6764001 ]\n",
      " [-0.00882369]\n",
      " [-0.3218546 ]]\n",
      "11 Cost:  7.3814945 \n",
      "Prediction:\n",
      " [[-3.261996  ]\n",
      " [-3.690141  ]\n",
      " [-2.356489  ]\n",
      " [-0.875792  ]\n",
      " [-1.7444353 ]\n",
      " [-1.6763072 ]\n",
      " [-0.00876445]\n",
      " [-0.3217969 ]]\n",
      "12 Cost:  7.3809915 \n",
      "Prediction:\n",
      " [[-3.2618623 ]\n",
      " [-3.690014  ]\n",
      " [-2.3563833 ]\n",
      " [-0.8757119 ]\n",
      " [-1.7443393 ]\n",
      " [-1.6762148 ]\n",
      " [-0.00870532]\n",
      " [-0.3217392 ]]\n",
      "13 Cost:  7.3804903 \n",
      "Prediction:\n",
      " [[-3.2617292 ]\n",
      " [-3.689886  ]\n",
      " [-2.3562784 ]\n",
      " [-0.8756319 ]\n",
      " [-1.7442431 ]\n",
      " [-1.6761222 ]\n",
      " [-0.00864613]\n",
      " [-0.3216815 ]]\n",
      "14 Cost:  7.379988 \n",
      "Prediction:\n",
      " [[-3.2615962 ]\n",
      " [-3.6897588 ]\n",
      " [-2.356173  ]\n",
      " [-0.87555194]\n",
      " [-1.7441472 ]\n",
      " [-1.6760296 ]\n",
      " [-0.00858688]\n",
      " [-0.32162374]]\n",
      "15 Cost:  7.379486 \n",
      "Prediction:\n",
      " [[-3.261463  ]\n",
      " [-3.6896307 ]\n",
      " [-2.3560681 ]\n",
      " [-0.87547183]\n",
      " [-1.744051  ]\n",
      " [-1.6759367 ]\n",
      " [-0.0085277 ]\n",
      " [-0.321566  ]]\n",
      "16 Cost:  7.3789835 \n",
      "Prediction:\n",
      " [[-3.2613292 ]\n",
      " [-3.6895032 ]\n",
      " [-2.3559628 ]\n",
      " [-0.8753917 ]\n",
      " [-1.7439548 ]\n",
      " [-1.6758443 ]\n",
      " [-0.00846851]\n",
      " [-0.3215083 ]]\n",
      "17 Cost:  7.378482 \n",
      "Prediction:\n",
      " [[-3.2611961 ]\n",
      " [-3.6893759 ]\n",
      " [-2.3558576 ]\n",
      " [-0.8753116 ]\n",
      " [-1.7438586 ]\n",
      " [-1.6757517 ]\n",
      " [-0.00840938]\n",
      " [-0.32145053]]\n",
      "18 Cost:  7.37798 \n",
      "Prediction:\n",
      " [[-3.261063  ]\n",
      " [-3.689248  ]\n",
      " [-2.3557525 ]\n",
      " [-0.8752316 ]\n",
      " [-1.7437626 ]\n",
      " [-1.6756591 ]\n",
      " [-0.00835013]\n",
      " [-0.32139283]]\n",
      "19 Cost:  7.37748 \n",
      "Prediction:\n",
      " [[-3.2609298 ]\n",
      " [-3.689121  ]\n",
      " [-2.3556473 ]\n",
      " [-0.87515175]\n",
      " [-1.7436664 ]\n",
      " [-1.6755667 ]\n",
      " [-0.00829101]\n",
      " [-0.32133508]]\n",
      "20 Cost:  7.3769774 \n",
      "Prediction:\n",
      " [[-3.2607965 ]\n",
      " [-3.6889935 ]\n",
      " [-2.3555422 ]\n",
      " [-0.87507164]\n",
      " [-1.7435704 ]\n",
      " [-1.675474  ]\n",
      " [-0.00823182]\n",
      " [-0.32127738]]\n",
      "21 Cost:  7.376476 \n",
      "Prediction:\n",
      " [[-3.2606635 ]\n",
      " [-3.6888657 ]\n",
      " [-2.3554366 ]\n",
      " [-0.87499166]\n",
      " [-1.7434742 ]\n",
      " [-1.6753814 ]\n",
      " [-0.00817263]\n",
      " [-0.32121968]]\n",
      "22 Cost:  7.3759747 \n",
      "Prediction:\n",
      " [[-3.2605305 ]\n",
      " [-3.6887383 ]\n",
      " [-2.3553314 ]\n",
      " [-0.87491155]\n",
      " [-1.7433783 ]\n",
      " [-1.675289  ]\n",
      " [-0.00811344]\n",
      " [-0.321162  ]]\n",
      "23 Cost:  7.375473 \n",
      "Prediction:\n",
      " [[-3.2603972 ]\n",
      " [-3.6886108 ]\n",
      " [-2.3552265 ]\n",
      " [-0.87483156]\n",
      " [-1.7432821 ]\n",
      " [-1.6751964 ]\n",
      " [-0.00805426]\n",
      " [-0.32110423]]\n",
      "24 Cost:  7.3749723 \n",
      "Prediction:\n",
      " [[-3.260264  ]\n",
      " [-3.6884832 ]\n",
      " [-2.3551216 ]\n",
      " [-0.87475157]\n",
      " [-1.7431861 ]\n",
      " [-1.675104  ]\n",
      " [-0.00799513]\n",
      " [-0.32104647]]\n",
      "25 Cost:  7.3744707 \n",
      "Prediction:\n",
      " [[-3.260131  ]\n",
      " [-3.688356  ]\n",
      " [-2.355016  ]\n",
      " [-0.8746716 ]\n",
      " [-1.7430902 ]\n",
      " [-1.6750116 ]\n",
      " [-0.00793594]\n",
      " [-0.32098877]]\n",
      "26 Cost:  7.373969 \n",
      "Prediction:\n",
      " [[-3.2599978 ]\n",
      " [-3.6882286 ]\n",
      " [-2.3549109 ]\n",
      " [-0.87459147]\n",
      " [-1.742994  ]\n",
      " [-1.6749188 ]\n",
      " [-0.00787675]\n",
      " [-0.32093102]]\n",
      "27 Cost:  7.3734684 \n",
      "Prediction:\n",
      " [[-3.2598646 ]\n",
      " [-3.688101  ]\n",
      " [-2.3548057 ]\n",
      " [-0.8745116 ]\n",
      " [-1.7428982 ]\n",
      " [-1.6748261 ]\n",
      " [-0.00781757]\n",
      " [-0.32087332]]\n",
      "28 Cost:  7.3729672 \n",
      "Prediction:\n",
      " [[-3.2597313 ]\n",
      " [-3.687974  ]\n",
      " [-2.3547006 ]\n",
      " [-0.8744316 ]\n",
      " [-1.7428018 ]\n",
      " [-1.6747338 ]\n",
      " [-0.00775844]\n",
      " [-0.32081556]]\n",
      "29 Cost:  7.3724656 \n",
      "Prediction:\n",
      " [[-3.2595983 ]\n",
      " [-3.6878462 ]\n",
      " [-2.3545957 ]\n",
      " [-0.87435144]\n",
      " [-1.7427061 ]\n",
      " [-1.6746411 ]\n",
      " [-0.00769925]\n",
      " [-0.32075793]]\n",
      "30 Cost:  7.3719645 \n",
      "Prediction:\n",
      " [[-3.2594652 ]\n",
      " [-3.6877189 ]\n",
      " [-2.3544903 ]\n",
      " [-0.87427163]\n",
      " [-1.74261   ]\n",
      " [-1.6745489 ]\n",
      " [-0.00764018]\n",
      " [-0.3207003 ]]\n",
      "31 Cost:  7.3714643 \n",
      "Prediction:\n",
      " [[-3.2593322 ]\n",
      " [-3.6875916 ]\n",
      " [-2.3543854 ]\n",
      " [-0.8741917 ]\n",
      " [-1.7425141 ]\n",
      " [-1.6744564 ]\n",
      " [-0.00758106]\n",
      " [-0.3206426 ]]\n",
      "32 Cost:  7.3709636 \n",
      "Prediction:\n",
      " [[-3.2591991 ]\n",
      " [-3.6874642 ]\n",
      " [-2.3542805 ]\n",
      " [-0.87411165]\n",
      " [-1.7424179 ]\n",
      " [-1.6743637 ]\n",
      " [-0.00752199]\n",
      " [-0.32058495]]\n",
      "33 Cost:  7.3704624 \n",
      "Prediction:\n",
      " [[-3.2590659 ]\n",
      " [-3.6873367 ]\n",
      " [-2.3541749 ]\n",
      " [-0.87403184]\n",
      " [-1.742322  ]\n",
      " [-1.6742713 ]\n",
      " [-0.00746286]\n",
      " [-0.32052726]]\n",
      "34 Cost:  7.3699617 \n",
      "Prediction:\n",
      " [[-3.2589328 ]\n",
      " [-3.6872094 ]\n",
      " [-2.35407   ]\n",
      " [-0.8739518 ]\n",
      " [-1.7422259 ]\n",
      " [-1.6741791 ]\n",
      " [-0.00740367]\n",
      " [-0.32046962]]\n",
      "35 Cost:  7.369461 \n",
      "Prediction:\n",
      " [[-3.2587998 ]\n",
      " [-3.687082  ]\n",
      " [-2.353965  ]\n",
      " [-0.87387174]\n",
      " [-1.74213   ]\n",
      " [-1.6740868 ]\n",
      " [-0.0073446 ]\n",
      " [-0.32041192]]\n",
      "36 Cost:  7.3689594 \n",
      "Prediction:\n",
      " [[-3.2586665 ]\n",
      " [-3.6869545 ]\n",
      " [-2.35386   ]\n",
      " [-0.87379193]\n",
      " [-1.7420338 ]\n",
      " [-1.6739942 ]\n",
      " [-0.00728548]\n",
      " [-0.32035428]]\n",
      "37 Cost:  7.368458 \n",
      "Prediction:\n",
      " [[-3.2585335 ]\n",
      " [-3.6868267 ]\n",
      " [-2.3537545 ]\n",
      " [-0.8737119 ]\n",
      " [-1.7419379 ]\n",
      " [-1.6739016 ]\n",
      " [-0.00722635]\n",
      " [-0.3202966 ]]\n",
      "38 Cost:  7.367957 \n",
      "Prediction:\n",
      " [[-3.2584004 ]\n",
      " [-3.6866994 ]\n",
      " [-2.3536496 ]\n",
      " [-0.87363195]\n",
      " [-1.7418418 ]\n",
      " [-1.673809  ]\n",
      " [-0.00716722]\n",
      " [-0.3202389 ]]\n",
      "39 Cost:  7.3674564 \n",
      "Prediction:\n",
      " [[-3.2582674 ]\n",
      " [-3.686572  ]\n",
      " [-2.3535445 ]\n",
      " [-0.87355214]\n",
      " [-1.741746  ]\n",
      " [-1.6737165 ]\n",
      " [-0.00710815]\n",
      " [-0.32018125]]\n",
      "40 Cost:  7.3669567 \n",
      "Prediction:\n",
      " [[-3.2581344 ]\n",
      " [-3.6864452 ]\n",
      " [-2.3534393 ]\n",
      " [-0.8734722 ]\n",
      " [-1.74165   ]\n",
      " [-1.6736242 ]\n",
      " [-0.00704896]\n",
      " [-0.3201236 ]]\n",
      "41 Cost:  7.366456 \n",
      "Prediction:\n",
      " [[-3.258001  ]\n",
      " [-3.6863172 ]\n",
      " [-2.3533344 ]\n",
      " [-0.87339205]\n",
      " [-1.741554  ]\n",
      " [-1.6735315 ]\n",
      " [-0.0069899 ]\n",
      " [-0.32006598]]\n",
      "42 Cost:  7.3659544 \n",
      "Prediction:\n",
      " [[-3.257868  ]\n",
      " [-3.68619   ]\n",
      " [-2.3532293 ]\n",
      " [-0.87331235]\n",
      " [-1.7414582 ]\n",
      " [-1.673439  ]\n",
      " [-0.00693083]\n",
      " [-0.32000828]]\n",
      "43 Cost:  7.3654547 \n",
      "Prediction:\n",
      " [[-3.257735  ]\n",
      " [-3.6860626 ]\n",
      " [-2.3531244 ]\n",
      " [-0.8732323 ]\n",
      " [-1.7413623 ]\n",
      " [-1.6733468 ]\n",
      " [-0.0068717 ]\n",
      " [-0.31995064]]\n",
      "44 Cost:  7.364953 \n",
      "Prediction:\n",
      " [[-3.2576017 ]\n",
      " [-3.685935  ]\n",
      " [-2.3530192 ]\n",
      " [-0.8731524 ]\n",
      " [-1.7412661 ]\n",
      " [-1.6732544 ]\n",
      " [-0.00681263]\n",
      " [-0.31989294]]\n",
      "45 Cost:  7.364453 \n",
      "Prediction:\n",
      " [[-3.257469  ]\n",
      " [-3.6858077 ]\n",
      " [-2.3529139 ]\n",
      " [-0.87307245]\n",
      " [-1.7411702 ]\n",
      " [-1.6731617 ]\n",
      " [-0.0067535 ]\n",
      " [-0.31983525]]\n",
      "46 Cost:  7.3639526 \n",
      "Prediction:\n",
      " [[-3.257336  ]\n",
      " [-3.6856809 ]\n",
      " [-2.352809  ]\n",
      " [-0.8729925 ]\n",
      " [-1.7410741 ]\n",
      " [-1.6730692 ]\n",
      " [-0.00669444]\n",
      " [-0.3197776 ]]\n",
      "47 Cost:  7.363452 \n",
      "Prediction:\n",
      " [[-3.2572029 ]\n",
      " [-3.685553  ]\n",
      " [-2.352704  ]\n",
      " [-0.8729126 ]\n",
      " [-1.7409782 ]\n",
      " [-1.672977  ]\n",
      " [-0.00663525]\n",
      " [-0.3197199 ]]\n",
      "48 Cost:  7.3629518 \n",
      "Prediction:\n",
      " [[-3.2570696 ]\n",
      " [-3.6854262 ]\n",
      " [-2.3525987 ]\n",
      " [-0.87283254]\n",
      " [-1.7408823 ]\n",
      " [-1.6728843 ]\n",
      " [-0.00657618]\n",
      " [-0.31966227]]\n",
      "49 Cost:  7.3624516 \n",
      "Prediction:\n",
      " [[-3.2569366 ]\n",
      " [-3.6852987 ]\n",
      " [-2.3524935 ]\n",
      " [-0.8727526 ]\n",
      " [-1.7407861 ]\n",
      " [-1.672792  ]\n",
      " [-0.00651705]\n",
      " [-0.31960464]]\n",
      "50 Cost:  7.361951 \n",
      "Prediction:\n",
      " [[-3.2568035 ]\n",
      " [-3.685171  ]\n",
      " [-2.3523886 ]\n",
      " [-0.8726727 ]\n",
      " [-1.7406902 ]\n",
      " [-1.6726997 ]\n",
      " [-0.00645792]\n",
      " [-0.319547  ]]\n",
      "51 Cost:  7.36145 \n",
      "Prediction:\n",
      " [[-3.2566705 ]\n",
      " [-3.6850436 ]\n",
      " [-2.3522837 ]\n",
      " [-0.87259287]\n",
      " [-1.7405941 ]\n",
      " [-1.6726072 ]\n",
      " [-0.00639886]\n",
      " [-0.3194893 ]]\n",
      "52 Cost:  7.3609495 \n",
      "Prediction:\n",
      " [[-3.2565374 ]\n",
      " [-3.6849165 ]\n",
      " [-2.3521786 ]\n",
      " [-0.8725128 ]\n",
      " [-1.7404984 ]\n",
      " [-1.6725146 ]\n",
      " [-0.00633973]\n",
      " [-0.31943166]]\n",
      "53 Cost:  7.360449 \n",
      "Prediction:\n",
      " [[-3.2564044 ]\n",
      " [-3.6847887 ]\n",
      " [-2.3520734 ]\n",
      " [-0.872433  ]\n",
      " [-1.7404022 ]\n",
      " [-1.6724222 ]\n",
      " [-0.0062806 ]\n",
      " [-0.31937397]]\n",
      "54 Cost:  7.3599486 \n",
      "Prediction:\n",
      " [[-3.2562714 ]\n",
      " [-3.6846614 ]\n",
      " [-2.3519683 ]\n",
      " [-0.87235284]\n",
      " [-1.7403064 ]\n",
      " [-1.6723297 ]\n",
      " [-0.00622147]\n",
      " [-0.31931627]]\n",
      "55 Cost:  7.3594484 \n",
      "Prediction:\n",
      " [[-3.2561383 ]\n",
      " [-3.684534  ]\n",
      " [-2.3518634 ]\n",
      " [-0.872273  ]\n",
      " [-1.7402103 ]\n",
      " [-1.6722374 ]\n",
      " [-0.00616241]\n",
      " [-0.31925863]]\n",
      "56 Cost:  7.358948 \n",
      "Prediction:\n",
      " [[-3.2560053 ]\n",
      " [-3.6844068 ]\n",
      " [-2.351758  ]\n",
      " [-0.8721931 ]\n",
      " [-1.7401143 ]\n",
      " [-1.6721448 ]\n",
      " [-0.00610328]\n",
      " [-0.31920093]]\n",
      "57 Cost:  7.358447 \n",
      "Prediction:\n",
      " [[-3.255872  ]\n",
      " [-3.6842792 ]\n",
      " [-2.3516529 ]\n",
      " [-0.87211305]\n",
      " [-1.7400181 ]\n",
      " [-1.6720521 ]\n",
      " [-0.00604421]\n",
      " [-0.3191433 ]]\n",
      "58 Cost:  7.3579473 \n",
      "Prediction:\n",
      " [[-3.255739  ]\n",
      " [-3.684152  ]\n",
      " [-2.351548  ]\n",
      " [-0.87203324]\n",
      " [-1.7399223 ]\n",
      " [-1.6719599 ]\n",
      " [-0.00598502]\n",
      " [-0.31908566]]\n",
      "59 Cost:  7.357447 \n",
      "Prediction:\n",
      " [[-3.255606  ]\n",
      " [-3.6840246 ]\n",
      " [-2.3514428 ]\n",
      " [-0.8719533 ]\n",
      " [-1.7398264 ]\n",
      " [-1.6718676 ]\n",
      " [-0.00592595]\n",
      " [-0.31902796]]\n",
      "60 Cost:  7.3569474 \n",
      "Prediction:\n",
      " [[-3.255473  ]\n",
      " [-3.6838973 ]\n",
      " [-2.351338  ]\n",
      " [-0.87187344]\n",
      " [-1.7397306 ]\n",
      " [-1.6717753 ]\n",
      " [-0.00586689]\n",
      " [-0.31897038]]\n",
      "61 Cost:  7.356447 \n",
      "Prediction:\n",
      " [[-3.25534   ]\n",
      " [-3.68377   ]\n",
      " [-2.3512328 ]\n",
      " [-0.8717933 ]\n",
      " [-1.7396345 ]\n",
      " [-1.6716828 ]\n",
      " [-0.00580782]\n",
      " [-0.31891274]]\n",
      "62 Cost:  7.3559465 \n",
      "Prediction:\n",
      " [[-3.255207  ]\n",
      " [-3.6836421 ]\n",
      " [-2.3511279 ]\n",
      " [-0.8717136 ]\n",
      " [-1.7395387 ]\n",
      " [-1.6715903 ]\n",
      " [-0.00574881]\n",
      " [-0.31885517]]\n",
      "63 Cost:  7.3554473 \n",
      "Prediction:\n",
      " [[-3.255074  ]\n",
      " [-3.6835153 ]\n",
      " [-2.351023  ]\n",
      " [-0.8716337 ]\n",
      " [-1.7394428 ]\n",
      " [-1.6714978 ]\n",
      " [-0.00568974]\n",
      " [-0.31879753]]\n",
      "64 Cost:  7.354947 \n",
      "Prediction:\n",
      " [[-3.254941  ]\n",
      " [-3.683388  ]\n",
      " [-2.3509176 ]\n",
      " [-0.87155384]\n",
      " [-1.7393467 ]\n",
      " [-1.6714056 ]\n",
      " [-0.00563073]\n",
      " [-0.31873995]]\n",
      "65 Cost:  7.3544474 \n",
      "Prediction:\n",
      " [[-3.254808  ]\n",
      " [-3.6832607 ]\n",
      " [-2.3508127 ]\n",
      " [-0.87147397]\n",
      " [-1.7392509 ]\n",
      " [-1.671313  ]\n",
      " [-0.00557166]\n",
      " [-0.3186823 ]]\n",
      "66 Cost:  7.3539476 \n",
      "Prediction:\n",
      " [[-3.2546751 ]\n",
      " [-3.6831334 ]\n",
      " [-2.3507075 ]\n",
      " [-0.8713942 ]\n",
      " [-1.739155  ]\n",
      " [-1.6712208 ]\n",
      " [-0.0055126 ]\n",
      " [-0.31862473]]\n",
      "67 Cost:  7.353448 \n",
      "Prediction:\n",
      " [[-3.254542  ]\n",
      " [-3.683006  ]\n",
      " [-2.3506026 ]\n",
      " [-0.8713142 ]\n",
      " [-1.739059  ]\n",
      " [-1.6711283 ]\n",
      " [-0.00545359]\n",
      " [-0.31856716]]\n",
      "68 Cost:  7.352948 \n",
      "Prediction:\n",
      " [[-3.254409  ]\n",
      " [-3.6828792 ]\n",
      " [-2.3504977 ]\n",
      " [-0.87123436]\n",
      " [-1.7389631 ]\n",
      " [-1.671036  ]\n",
      " [-0.00539452]\n",
      " [-0.31850952]]\n",
      "69 Cost:  7.3524485 \n",
      "Prediction:\n",
      " [[-3.254276  ]\n",
      " [-3.6827514 ]\n",
      " [-2.3503926 ]\n",
      " [-0.8711546 ]\n",
      " [-1.7388675 ]\n",
      " [-1.6709437 ]\n",
      " [-0.00533551]\n",
      " [-0.31845194]]\n",
      "70 Cost:  7.351948 \n",
      "Prediction:\n",
      " [[-3.254143 ]\n",
      " [-3.682624 ]\n",
      " [-2.3502877]\n",
      " [-0.8710746]\n",
      " [-1.7387714]\n",
      " [-1.6708512]\n",
      " [-0.0052765]\n",
      " [-0.3183943]]\n",
      "71 Cost:  7.351448 \n",
      "Prediction:\n",
      " [[-3.2540102 ]\n",
      " [-3.6824968 ]\n",
      " [-2.3501828 ]\n",
      " [-0.87099475]\n",
      " [-1.7386756 ]\n",
      " [-1.6707587 ]\n",
      " [-0.00521737]\n",
      " [-0.31833673]]\n",
      "72 Cost:  7.3509493 \n",
      "Prediction:\n",
      " [[-3.2538772 ]\n",
      " [-3.68237   ]\n",
      " [-2.3500776 ]\n",
      " [-0.8709149 ]\n",
      " [-1.7385798 ]\n",
      " [-1.6706665 ]\n",
      " [-0.00515836]\n",
      " [-0.3182791 ]]\n",
      "73 Cost:  7.3504496 \n",
      "Prediction:\n",
      " [[-3.2537441 ]\n",
      " [-3.6822422 ]\n",
      " [-2.3499727 ]\n",
      " [-0.8708351 ]\n",
      " [-1.7384837 ]\n",
      " [-1.670574  ]\n",
      " [-0.00509936]\n",
      " [-0.3182215 ]]\n",
      "74 Cost:  7.34995 \n",
      "Prediction:\n",
      " [[-3.2536113 ]\n",
      " [-3.6821148 ]\n",
      " [-2.3498678 ]\n",
      " [-0.87075514]\n",
      " [-1.7383878 ]\n",
      " [-1.6704817 ]\n",
      " [-0.00504023]\n",
      " [-0.31816387]]\n",
      "75 Cost:  7.3494506 \n",
      "Prediction:\n",
      " [[-3.2534785 ]\n",
      " [-3.681988  ]\n",
      " [-2.3497627 ]\n",
      " [-0.8706754 ]\n",
      " [-1.738292  ]\n",
      " [-1.6703894 ]\n",
      " [-0.00498122]\n",
      " [-0.3181063 ]]\n",
      "76 Cost:  7.3489513 \n",
      "Prediction:\n",
      " [[-3.2533455 ]\n",
      " [-3.6818607 ]\n",
      " [-2.3496578 ]\n",
      " [-0.8705955 ]\n",
      " [-1.7381961 ]\n",
      " [-1.6702969 ]\n",
      " [-0.00492221]\n",
      " [-0.31804872]]\n",
      "77 Cost:  7.3484526 \n",
      "Prediction:\n",
      " [[-3.2532127 ]\n",
      " [-3.6817334 ]\n",
      " [-2.3495529 ]\n",
      " [-0.87051576]\n",
      " [-1.7381003 ]\n",
      " [-1.6702046 ]\n",
      " [-0.00486314]\n",
      " [-0.31799114]]\n",
      "78 Cost:  7.347953 \n",
      "Prediction:\n",
      " [[-3.2530801 ]\n",
      " [-3.681606  ]\n",
      " [-2.3494477 ]\n",
      " [-0.8704359 ]\n",
      " [-1.7380047 ]\n",
      " [-1.6701124 ]\n",
      " [-0.00480413]\n",
      " [-0.3179335 ]]\n",
      "79 Cost:  7.347453 \n",
      "Prediction:\n",
      " [[-3.252947  ]\n",
      " [-3.6814792 ]\n",
      " [-2.3493428 ]\n",
      " [-0.870356  ]\n",
      " [-1.7379084 ]\n",
      " [-1.6700199 ]\n",
      " [-0.00474507]\n",
      " [-0.31787592]]\n",
      "80 Cost:  7.3469534 \n",
      "Prediction:\n",
      " [[-3.252814  ]\n",
      " [-3.6813514 ]\n",
      " [-2.3492382 ]\n",
      " [-0.87027615]\n",
      " [-1.7378128 ]\n",
      " [-1.6699276 ]\n",
      " [-0.004686  ]\n",
      " [-0.31781834]]\n",
      "81 Cost:  7.346455 \n",
      "Prediction:\n",
      " [[-3.2526813 ]\n",
      " [-3.6812246 ]\n",
      " [-2.3491333 ]\n",
      " [-0.8701964 ]\n",
      " [-1.7377169 ]\n",
      " [-1.6698353 ]\n",
      " [-0.00462699]\n",
      " [-0.3177607 ]]\n",
      "82 Cost:  7.345955 \n",
      "Prediction:\n",
      " [[-3.2525485 ]\n",
      " [-3.6810973 ]\n",
      " [-2.349028  ]\n",
      " [-0.87011653]\n",
      " [-1.7376208 ]\n",
      " [-1.6697428 ]\n",
      " [-0.00456792]\n",
      " [-0.31770313]]\n",
      "83 Cost:  7.345456 \n",
      "Prediction:\n",
      " [[-3.2524157 ]\n",
      " [-3.68097   ]\n",
      " [-2.3489232 ]\n",
      " [-0.8700368 ]\n",
      " [-1.7375252 ]\n",
      " [-1.6696506 ]\n",
      " [-0.00450885]\n",
      " [-0.3176455 ]]\n",
      "84 Cost:  7.3449574 \n",
      "Prediction:\n",
      " [[-3.2522826 ]\n",
      " [-3.6808426 ]\n",
      " [-2.3488183 ]\n",
      " [-0.8699569 ]\n",
      " [-1.7374294 ]\n",
      " [-1.6695583 ]\n",
      " [-0.00444984]\n",
      " [-0.3175879 ]]\n",
      "85 Cost:  7.3444576 \n",
      "Prediction:\n",
      " [[-3.2521498 ]\n",
      " [-3.6807158 ]\n",
      " [-2.3487132 ]\n",
      " [-0.86987716]\n",
      " [-1.7373333 ]\n",
      " [-1.669466  ]\n",
      " [-0.00439078]\n",
      " [-0.31753033]]\n",
      "86 Cost:  7.3439593 \n",
      "Prediction:\n",
      " [[-3.2520173 ]\n",
      " [-3.6805885 ]\n",
      " [-2.3486083 ]\n",
      " [-0.8697974 ]\n",
      " [-1.7372375 ]\n",
      " [-1.6693735 ]\n",
      " [-0.00433171]\n",
      " [-0.31747276]]\n",
      "87 Cost:  7.34346 \n",
      "Prediction:\n",
      " [[-3.2518842 ]\n",
      " [-3.6804612 ]\n",
      " [-2.3485034 ]\n",
      " [-0.8697174 ]\n",
      " [-1.7371418 ]\n",
      " [-1.6692812 ]\n",
      " [-0.0042727 ]\n",
      " [-0.31741518]]\n",
      "88 Cost:  7.3429604 \n",
      "Prediction:\n",
      " [[-3.2517512 ]\n",
      " [-3.6803339 ]\n",
      " [-2.3483984 ]\n",
      " [-0.86963755]\n",
      " [-1.7370458 ]\n",
      " [-1.6691892 ]\n",
      " [-0.00421363]\n",
      " [-0.3173576 ]]\n",
      "89 Cost:  7.3424616 \n",
      "Prediction:\n",
      " [[-3.2516184 ]\n",
      " [-3.680207  ]\n",
      " [-2.3482933 ]\n",
      " [-0.8695577 ]\n",
      " [-1.7369499 ]\n",
      " [-1.6690967 ]\n",
      " [-0.00415462]\n",
      " [-0.31729996]]\n",
      "90 Cost:  7.341964 \n",
      "Prediction:\n",
      " [[-3.2514856 ]\n",
      " [-3.6800797 ]\n",
      " [-2.3481884 ]\n",
      " [-0.8694779 ]\n",
      " [-1.7368543 ]\n",
      " [-1.6690044 ]\n",
      " [-0.00409555]\n",
      " [-0.31724238]]\n",
      "91 Cost:  7.341464 \n",
      "Prediction:\n",
      " [[-3.2513528 ]\n",
      " [-3.6799524 ]\n",
      " [-2.3480835 ]\n",
      " [-0.86939806]\n",
      " [-1.7367582 ]\n",
      " [-1.6689122 ]\n",
      " [-0.00403655]\n",
      " [-0.31718475]]\n",
      "92 Cost:  7.3409643 \n",
      "Prediction:\n",
      " [[-3.2512197 ]\n",
      " [-3.679825  ]\n",
      " [-2.3479786 ]\n",
      " [-0.86931837]\n",
      " [-1.7366624 ]\n",
      " [-1.6688197 ]\n",
      " [-0.00397754]\n",
      " [-0.31712723]]\n",
      "93 Cost:  7.3404665 \n",
      "Prediction:\n",
      " [[-3.251087  ]\n",
      " [-3.6796982 ]\n",
      " [-2.3478737 ]\n",
      " [-0.86923856]\n",
      " [-1.7365668 ]\n",
      " [-1.6687274 ]\n",
      " [-0.00391859]\n",
      " [-0.31706965]]\n",
      "94 Cost:  7.3399677 \n",
      "Prediction:\n",
      " [[-3.2509546 ]\n",
      " [-3.6795712 ]\n",
      " [-2.3477688 ]\n",
      " [-0.86915874]\n",
      " [-1.736471  ]\n",
      " [-1.6686352 ]\n",
      " [-0.00385964]\n",
      " [-0.31701213]]\n",
      "95 Cost:  7.339469 \n",
      "Prediction:\n",
      " [[-3.2508216 ]\n",
      " [-3.6794438 ]\n",
      " [-2.347664  ]\n",
      " [-0.86907905]\n",
      " [-1.7363751 ]\n",
      " [-1.6685429 ]\n",
      " [-0.00380057]\n",
      " [-0.31695467]]\n",
      "96 Cost:  7.33897 \n",
      "Prediction:\n",
      " [[-3.2506888 ]\n",
      " [-3.6793165 ]\n",
      " [-2.3475592 ]\n",
      " [-0.86899936]\n",
      " [-1.7362792 ]\n",
      " [-1.6684506 ]\n",
      " [-0.00374162]\n",
      " [-0.3168971 ]]\n",
      "97 Cost:  7.3384724 \n",
      "Prediction:\n",
      " [[-3.250556  ]\n",
      " [-3.6791897 ]\n",
      " [-2.3474543 ]\n",
      " [-0.8689197 ]\n",
      " [-1.7361836 ]\n",
      " [-1.6683583 ]\n",
      " [-0.00368267]\n",
      " [-0.31683964]]\n",
      "98 Cost:  7.3379745 \n",
      "Prediction:\n",
      " [[-3.2504234e+00]\n",
      " [-3.6790628e+00]\n",
      " [-2.3473496e+00]\n",
      " [-8.6883986e-01]\n",
      " [-1.7360879e+00]\n",
      " [-1.6682662e+00]\n",
      " [-3.6237240e-03]\n",
      " [-3.1678212e-01]]\n",
      "99 Cost:  7.337476 \n",
      "Prediction:\n",
      " [[-3.2502906e+00]\n",
      " [-3.6789358e+00]\n",
      " [-2.3472447e+00]\n",
      " [-8.6876017e-01]\n",
      " [-1.7359922e+00]\n",
      " [-1.6681740e+00]\n",
      " [-3.5647750e-03]\n",
      " [-3.1672466e-01]]\n",
      "100 Cost:  7.336977 \n",
      "Prediction:\n",
      " [[-3.2501576e+00]\n",
      " [-3.6788085e+00]\n",
      " [-2.3471401e+00]\n",
      " [-8.6868048e-01]\n",
      " [-1.7358963e+00]\n",
      " [-1.6680818e+00]\n",
      " [-3.5058260e-03]\n",
      " [-3.1666720e-01]]\n"
     ]
    }
   ],
   "source": [
    "# lab-07-3-linear_regression_min_max.py\n",
    "#\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "# very important. It does not work without it.\n",
    "xy = MinMaxScaler(xy)\n",
    "print(xy)\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-32-0326f9d76198>:12: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\beomc\\anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\beomc\\anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\beomc\\anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\beomc\\anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\beomc\\anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Epoch: 0001 cost = 2.863031463\n",
      "Epoch: 0002 cost = 1.110213588\n",
      "Epoch: 0003 cost = 0.885661981\n",
      "Epoch: 0004 cost = 0.776524899\n",
      "Epoch: 0005 cost = 0.708966718\n",
      "Epoch: 0006 cost = 0.660514938\n",
      "Epoch: 0007 cost = 0.623954637\n",
      "Epoch: 0008 cost = 0.593999784\n",
      "Epoch: 0009 cost = 0.570065611\n",
      "Epoch: 0010 cost = 0.549895176\n",
      "Epoch: 0011 cost = 0.532152321\n",
      "Epoch: 0012 cost = 0.516839193\n",
      "Epoch: 0013 cost = 0.503507852\n",
      "Epoch: 0014 cost = 0.491718690\n",
      "Epoch: 0015 cost = 0.480281749\n",
      "Learning finished\n",
      "Accuracy:  0.8872\n",
      "Label:  [1]\n",
      "Prediction:  [1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lab-07-4-mnist_introduction.py\n",
    "# \n",
    "# Lab 7 Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={\n",
    "                            X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={\n",
    "          X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Prediction: \", sess.run(\n",
    "        tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r:r + 1].reshape(28, 28),\n",
    "        cmap='Greys',\n",
    "        interpolation='nearest')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7803123 [array([[ 0.309734  ,  2.3732326 ],\n",
      "       [-0.65712446, -1.1635541 ]], dtype=float32), array([[1.6738999],\n",
      "       [0.5552274]], dtype=float32)]\n",
      "100 0.67996395 [array([[ 0.09046934,  2.030461  ],\n",
      "       [-0.66752946, -1.9187658 ]], dtype=float32), array([[1.2867024],\n",
      "       [0.5365867]], dtype=float32)]\n",
      "200 0.6223151 [array([[-0.19528477,  2.9462068 ],\n",
      "       [-0.01389546, -3.0817332 ]], dtype=float32), array([[1.1116073],\n",
      "       [1.4248857]], dtype=float32)]\n",
      "300 0.51418054 [array([[-1.2620091,  4.2280636],\n",
      "       [ 1.0710982, -4.336662 ]], dtype=float32), array([[1.378322],\n",
      "       [2.526283]], dtype=float32)]\n",
      "400 0.32273716 [array([[-2.871013 ,  5.273171 ],\n",
      "       [ 2.6006896, -5.2788444]], dtype=float32), array([[2.8050458],\n",
      "       [3.5301652]], dtype=float32)]\n",
      "500 0.18220223 [array([[-4.012449 ,  6.008299 ],\n",
      "       [ 3.8227186, -5.93986  ]], dtype=float32), array([[4.1681914],\n",
      "       [4.407527 ]], dtype=float32)]\n",
      "600 0.1170037 [array([[-4.7058835,  6.499662 ],\n",
      "       [ 4.5496626, -6.4109592]], dtype=float32), array([[5.104963],\n",
      "       [5.130607]], dtype=float32)]\n",
      "700 0.08288951 [array([[-5.1741953,  6.855021 ],\n",
      "       [ 5.034504 , -6.76445  ]], dtype=float32), array([[5.797113 ],\n",
      "       [5.7256737]], dtype=float32)]\n",
      "800 0.06253534 [array([[-5.5212984,  7.130698 ],\n",
      "       [ 5.391765 , -7.043353 ]], dtype=float32), array([[6.3484473],\n",
      "       [6.2278223]], dtype=float32)]\n",
      "900 0.049232356 [array([[-5.7949996,  7.3550715],\n",
      "       [ 5.6725206, -7.27217  ]], dtype=float32), array([[6.8099566],\n",
      "       [6.6623573]], dtype=float32)]\n",
      "1000 0.039962042 [array([[-6.0201387,  7.543936 ],\n",
      "       [ 5.902926 , -7.465525 ]], dtype=float32), array([[7.2093625],\n",
      "       [7.046225 ]], dtype=float32)]\n",
      "1100 0.033190478 [array([[-6.2110267,  7.706893 ],\n",
      "       [ 6.097925 , -7.6326623]], dtype=float32), array([[7.563253 ],\n",
      "       [7.3909464]], dtype=float32)]\n",
      "1200 0.028062835 [array([[-6.376581 ,  7.8501754],\n",
      "       [ 6.2667937, -7.779734 ]], dtype=float32), array([[7.8823295],\n",
      "       [7.704618 ]], dtype=float32)]\n",
      "1300 0.024068568 [array([[-6.5227065,  7.97805  ],\n",
      "       [ 6.415656 , -7.9110107]], dtype=float32), array([[8.173899],\n",
      "       [7.993103]], dtype=float32)]\n",
      "1400 0.02088514 [array([[-6.653501 ,  8.093561 ],\n",
      "       [ 6.5487514, -8.029569 ]], dtype=float32), array([[8.443167],\n",
      "       [8.260771]], dtype=float32)]\n",
      "1500 0.018299548 [array([[-6.7719116,  8.198943 ],\n",
      "       [ 6.669128 , -8.137689 ]], dtype=float32), array([[8.694003],\n",
      "       [8.510977]], dtype=float32)]\n",
      "1600 0.01616613 [array([[-6.8801355,  8.295893 ],\n",
      "       [ 6.779049 , -8.237103 ]], dtype=float32), array([[8.929336],\n",
      "       [8.746331]], dtype=float32)]\n",
      "1700 0.014382012 [array([[-6.9798374,  8.385719 ],\n",
      "       [ 6.880234 , -8.329165 ]], dtype=float32), array([[9.15146 ],\n",
      "       [8.968918]], dtype=float32)]\n",
      "1800 0.012872558 [array([[-7.0723186,  8.469459 ],\n",
      "       [ 6.974022 , -8.414936 ]], dtype=float32), array([[9.362199],\n",
      "       [9.180413]], dtype=float32)]\n",
      "1900 0.011582606 [array([[-7.1586137,  8.547939 ],\n",
      "       [ 7.0614753, -8.495276 ]], dtype=float32), array([[9.563038],\n",
      "       [9.382201]], dtype=float32)]\n",
      "2000 0.010470519 [array([[-7.239556,  8.621842],\n",
      "       [ 7.143446, -8.570881]], dtype=float32), array([[9.755171],\n",
      "       [9.575431]], dtype=float32)]\n",
      "2100 0.009504216 [array([[-7.3158145,  8.691724 ],\n",
      "       [ 7.2206354, -8.64233  ]], dtype=float32), array([[9.939617],\n",
      "       [9.761063]], dtype=float32)]\n",
      "2200 0.00865883 [array([[-7.387955 ,  8.758043 ],\n",
      "       [ 7.2936196, -8.7101   ]], dtype=float32), array([[10.117222],\n",
      "       [ 9.939907]], dtype=float32)]\n",
      "2300 0.007914556 [array([[-7.456447,  8.821196],\n",
      "       [ 7.362878, -8.774601]], dtype=float32), array([[10.288707],\n",
      "       [10.112669]], dtype=float32)]\n",
      "2400 0.0072556 [array([[-7.521689 ,  8.881512 ],\n",
      "       [ 7.4288177, -8.836167 ]], dtype=float32), array([[10.45469  ],\n",
      "       [10.2799425]], dtype=float32)]\n",
      "2500 0.0066692973 [array([[-7.584015,  8.939275],\n",
      "       [ 7.491782, -8.8951  ]], dtype=float32), array([[10.615705],\n",
      "       [10.442253]], dtype=float32)]\n",
      "2600 0.0061452324 [array([[-7.6437144,  8.994733 ],\n",
      "       [ 7.552068 , -8.951651 ]], dtype=float32), array([[10.772219],\n",
      "       [10.600054]], dtype=float32)]\n",
      "2700 0.00567484 [array([[-7.701038,  9.048098],\n",
      "       [ 7.609932, -9.00604 ]], dtype=float32), array([[10.924638],\n",
      "       [10.753746]], dtype=float32)]\n",
      "2800 0.005251004 [array([[-7.756199 ,  9.099554 ],\n",
      "       [ 7.6655965, -9.05846  ]], dtype=float32), array([[11.073315],\n",
      "       [10.903685]], dtype=float32)]\n",
      "2900 0.0048678084 [array([[-7.8093905,  9.149264 ],\n",
      "       [ 7.719253 , -9.109079 ]], dtype=float32), array([[11.218568],\n",
      "       [11.050182]], dtype=float32)]\n",
      "3000 0.0045201774 [array([[-7.860777 ,  9.197374 ],\n",
      "       [ 7.7710724, -9.1580515]], dtype=float32), array([[11.360681],\n",
      "       [11.193518]], dtype=float32)]\n",
      "3100 0.0042039137 [array([[-7.9105067,  9.244008 ],\n",
      "       [ 7.821204 , -9.2055   ]], dtype=float32), array([[11.499905],\n",
      "       [11.333944]], dtype=float32)]\n",
      "3200 0.0039154263 [array([[-7.9587092,  9.289281 ],\n",
      "       [ 7.869784 , -9.251545 ]], dtype=float32), array([[11.636466],\n",
      "       [11.471683]], dtype=float32)]\n",
      "3300 0.003651536 [array([[-8.005504 ,  9.333296 ],\n",
      "       [ 7.9169292, -9.296293 ]], dtype=float32), array([[11.770565],\n",
      "       [11.606938]], dtype=float32)]\n",
      "3400 0.0034096055 [array([[-8.050991 ,  9.376142 ],\n",
      "       [ 7.9627476, -9.339838 ]], dtype=float32), array([[11.902389],\n",
      "       [11.739892]], dtype=float32)]\n",
      "3500 0.003187377 [array([[-8.0952635,  9.417899 ],\n",
      "       [ 8.007333 , -9.382264 ]], dtype=float32), array([[12.032103],\n",
      "       [11.870712]], dtype=float32)]\n",
      "3600 0.0029827596 [array([[-8.138409,  9.458645],\n",
      "       [ 8.050769, -9.423647]], dtype=float32), array([[12.159854],\n",
      "       [11.999549]], dtype=float32)]\n",
      "3700 0.0027940543 [array([[-8.180499 ,  9.4984455],\n",
      "       [ 8.093135 , -9.464056 ]], dtype=float32), array([[12.285787],\n",
      "       [12.126542]], dtype=float32)]\n",
      "3800 0.0026196525 [array([[-8.221606,  9.537361],\n",
      "       [ 8.134502, -9.503554]], dtype=float32), array([[12.410021],\n",
      "       [12.251816]], dtype=float32)]\n",
      "3900 0.0024582627 [array([[-8.261793,  9.575445],\n",
      "       [ 8.174935, -9.542195]], dtype=float32), array([[12.532676],\n",
      "       [12.375487]], dtype=float32)]\n",
      "4000 0.0023086837 [array([[-8.301115,  9.612751],\n",
      "       [ 8.214489, -9.580036]], dtype=float32), array([[12.653856],\n",
      "       [12.497664]], dtype=float32)]\n",
      "4100 0.0021697746 [array([[-8.339624,  9.649322],\n",
      "       [ 8.253217, -9.617123]], dtype=float32), array([[12.773656],\n",
      "       [12.618443]], dtype=float32)]\n",
      "4200 0.0020406805 [array([[-8.377367,  9.685202],\n",
      "       [ 8.291169, -9.653497]], dtype=float32), array([[12.892167],\n",
      "       [12.737913]], dtype=float32)]\n",
      "4300 0.0019205164 [array([[-8.414391 ,  9.7204275],\n",
      "       [ 8.32839  , -9.689199 ]], dtype=float32), array([[13.009474],\n",
      "       [12.856158]], dtype=float32)]\n",
      "4400 0.0018085325 [array([[-8.45073 ,  9.755036],\n",
      "       [ 8.36492 , -9.724269]], dtype=float32), array([[13.12565 ],\n",
      "       [12.973255]], dtype=float32)]\n",
      "4500 0.0017040395 [array([[-8.486426,  9.789061],\n",
      "       [ 8.400793, -9.758736]], dtype=float32), array([[13.2407675],\n",
      "       [13.089274 ]], dtype=float32)]\n",
      "4600 0.001606483 [array([[-8.521512,  9.822533],\n",
      "       [ 8.43605 , -9.792635]], dtype=float32), array([[13.354893],\n",
      "       [13.204285]], dtype=float32)]\n",
      "4700 0.0015152786 [array([[-8.556018,  9.855479],\n",
      "       [ 8.470719, -9.825992]], dtype=float32), array([[13.468085],\n",
      "       [13.318347]], dtype=float32)]\n",
      "4800 0.0014299173 [array([[-8.589975,  9.887926],\n",
      "       [ 8.504829, -9.858837]], dtype=float32), array([[13.580402],\n",
      "       [13.431518]], dtype=float32)]\n",
      "4900 0.0013499799 [array([[-8.623408,  9.919896],\n",
      "       [ 8.538412, -9.891195]], dtype=float32), array([[13.691895],\n",
      "       [13.543849]], dtype=float32)]\n",
      "5000 0.001275077 [array([[-8.656343,  9.951412],\n",
      "       [ 8.571487, -9.923086]], dtype=float32), array([[13.802614],\n",
      "       [13.655392]], dtype=float32)]\n",
      "5100 0.001204775 [array([[-8.688804,  9.982498],\n",
      "       [ 8.604082, -9.954537]], dtype=float32), array([[13.912607 ],\n",
      "       [13.7661915]], dtype=float32)]\n",
      "5200 0.001138819 [array([[-8.720812, 10.013174],\n",
      "       [ 8.636217, -9.98556 ]], dtype=float32), array([[14.021913],\n",
      "       [13.876292]], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5300 0.0010768354 [array([[ -8.752385,  10.043453],\n",
      "       [  8.667913, -10.01618 ]], dtype=float32), array([[14.130573],\n",
      "       [13.985735]], dtype=float32)]\n",
      "5400 0.0010185402 [array([[ -8.783546 ,  10.073353 ],\n",
      "       [  8.699191 , -10.0464115]], dtype=float32), array([[14.238626],\n",
      "       [14.094557]], dtype=float32)]\n",
      "5500 0.0009637237 [array([[ -8.81431 ,  10.10289 ],\n",
      "       [  8.730067, -10.076273]], dtype=float32), array([[14.346106],\n",
      "       [14.202793]], dtype=float32)]\n",
      "5600 0.00091213186 [array([[ -8.844692,  10.132083],\n",
      "       [  8.760558, -10.105779]], dtype=float32), array([[14.453048],\n",
      "       [14.310479]], dtype=float32)]\n",
      "5700 0.0008635404 [array([[ -8.874709,  10.160941],\n",
      "       [  8.790679, -10.134939]], dtype=float32), array([[14.559482],\n",
      "       [14.417643]], dtype=float32)]\n",
      "5800 0.00081775506 [array([[ -8.904377 ,  10.189482 ],\n",
      "       [  8.820447 , -10.1637745]], dtype=float32), array([[14.665433],\n",
      "       [14.524314]], dtype=float32)]\n",
      "5900 0.00077456667 [array([[ -8.933706,  10.217714],\n",
      "       [  8.849873, -10.192295]], dtype=float32), array([[14.770932],\n",
      "       [14.630525]], dtype=float32)]\n",
      "6000 0.00073387043 [array([[ -8.962717,  10.245649],\n",
      "       [  8.878973, -10.220508]], dtype=float32), array([[14.876005],\n",
      "       [14.736295]], dtype=float32)]\n",
      "6100 0.00069547235 [array([[ -8.991409,  10.273299],\n",
      "       [  8.907752, -10.248429]], dtype=float32), array([[14.980677],\n",
      "       [14.841654]], dtype=float32)]\n",
      "6200 0.00065922283 [array([[ -9.019802,  10.300673],\n",
      "       [  8.936231, -10.276071]], dtype=float32), array([[15.084967],\n",
      "       [14.94662 ]], dtype=float32)]\n",
      "6300 0.00062495784 [array([[ -9.047906,  10.327778],\n",
      "       [  8.964415, -10.303438]], dtype=float32), array([[15.188895],\n",
      "       [15.051215]], dtype=float32)]\n",
      "6400 0.0005926024 [array([[ -9.075729,  10.354631],\n",
      "       [  8.992316, -10.330539]], dtype=float32), array([[15.292491],\n",
      "       [15.155467]], dtype=float32)]\n",
      "6500 0.0005620223 [array([[ -9.103284 ,  10.381233 ],\n",
      "       [  9.019945 , -10.3573885]], dtype=float32), array([[15.39576 ],\n",
      "       [15.259381]], dtype=float32)]\n",
      "6600 0.000533113 [array([[ -9.13057 ,  10.407597],\n",
      "       [  9.04731 , -10.383993]], dtype=float32), array([[15.498729],\n",
      "       [15.362992]], dtype=float32)]\n",
      "6700 0.00050579984 [array([[ -9.157607,  10.433727],\n",
      "       [  9.074409, -10.410357]], dtype=float32), array([[15.601409],\n",
      "       [15.466303]], dtype=float32)]\n",
      "6800 0.00047993346 [array([[ -9.1844  ,  10.45964 ],\n",
      "       [  9.101271, -10.436491]], dtype=float32), array([[15.703831],\n",
      "       [15.569343]], dtype=float32)]\n",
      "6900 0.00045545422 [array([[ -9.210952,  10.485322],\n",
      "       [  9.127885, -10.462405]], dtype=float32), array([[15.805983],\n",
      "       [15.672114]], dtype=float32)]\n",
      "7000 0.00043227256 [array([[ -9.237273,  10.510793],\n",
      "       [  9.154269, -10.488096]], dtype=float32), array([[15.907896],\n",
      "       [15.774633]], dtype=float32)]\n",
      "7100 0.0004103436 [array([[ -9.263376 ,  10.536061 ],\n",
      "       [  9.180428 , -10.5135765]], dtype=float32), array([[16.009596],\n",
      "       [15.876918]], dtype=float32)]\n",
      "7200 0.0003895779 [array([[ -9.289253,  10.561132],\n",
      "       [  9.206366, -10.538854]], dtype=float32), array([[16.111074],\n",
      "       [15.978983]], dtype=float32)]\n",
      "7300 0.00036988597 [array([[ -9.31492 ,  10.586007],\n",
      "       [  9.232089, -10.563932]], dtype=float32), array([[16.21234 ],\n",
      "       [16.080853]], dtype=float32)]\n",
      "7400 0.00035126766 [array([[ -9.340384,  10.61069 ],\n",
      "       [  9.257604, -10.588817]], dtype=float32), array([[16.313414],\n",
      "       [16.1825  ]], dtype=float32)]\n",
      "7500 0.00033355894 [array([[ -9.365646,  10.635186],\n",
      "       [  9.282912, -10.613512]], dtype=float32), array([[16.414309],\n",
      "       [16.283958]], dtype=float32)]\n",
      "7600 0.00031687907 [array([[ -9.390714,  10.659509],\n",
      "       [  9.308036, -10.638023]], dtype=float32), array([[16.515017],\n",
      "       [16.385239]], dtype=float32)]\n",
      "7700 0.00030097459 [array([[ -9.415591,  10.68366 ],\n",
      "       [  9.332964, -10.662357]], dtype=float32), array([[16.61558 ],\n",
      "       [16.486353]], dtype=float32)]\n",
      "7800 0.00028589016 [array([[ -9.440284,  10.707638],\n",
      "       [  9.357702, -10.686519]], dtype=float32), array([[16.715988],\n",
      "       [16.587307]], dtype=float32)]\n",
      "7900 0.00027161083 [array([[ -9.464796,  10.731439],\n",
      "       [  9.382259, -10.710505]], dtype=float32), array([[16.816263],\n",
      "       [16.68811 ]], dtype=float32)]\n",
      "8000 0.00025806206 [array([[ -9.489133,  10.755078],\n",
      "       [  9.406631, -10.734323]], dtype=float32), array([[16.916399],\n",
      "       [16.788797]], dtype=float32)]\n",
      "8100 0.00024522893 [array([[ -9.513299,  10.778572],\n",
      "       [  9.430827, -10.757981]], dtype=float32), array([[17.016369],\n",
      "       [16.889315]], dtype=float32)]\n",
      "8200 0.00023303683 [array([[ -9.53729 ,  10.801894],\n",
      "       [  9.454859, -10.781484]], dtype=float32), array([[17.11626 ],\n",
      "       [16.989702]], dtype=float32)]\n",
      "8300 0.00022145596 [array([[ -9.561116,  10.82507 ],\n",
      "       [  9.47873 , -10.804821]], dtype=float32), array([[17.216015],\n",
      "       [17.090002]], dtype=float32)]\n",
      "8400 0.00021045652 [array([[ -9.584788,  10.848098],\n",
      "       [  9.502431, -10.828015]], dtype=float32), array([[17.315659],\n",
      "       [17.190138]], dtype=float32)]\n",
      "8500 0.00020000865 [array([[ -9.608297,  10.870975],\n",
      "       [  9.525973, -10.851052]], dtype=float32), array([[17.415222],\n",
      "       [17.29019 ]], dtype=float32)]\n",
      "8600 0.00019015704 [array([[ -9.631651,  10.893714],\n",
      "       [  9.54937 , -10.87395 ]], dtype=float32), array([[17.51465 ],\n",
      "       [17.390135]], dtype=float32)]\n",
      "8700 0.00018076754 [array([[ -9.654862 ,  10.916316 ],\n",
      "       [  9.5726185, -10.8967   ]], dtype=float32), array([[17.614023],\n",
      "       [17.490036]], dtype=float32)]\n",
      "8800 0.00017182528 [array([[ -9.677916,  10.938785],\n",
      "       [  9.595717, -10.919318]], dtype=float32), array([[17.713346],\n",
      "       [17.58979 ]], dtype=float32)]\n",
      "8900 0.00016334507 [array([[ -9.700837,  10.961122],\n",
      "       [  9.61866 , -10.94179 ]], dtype=float32), array([[17.812529],\n",
      "       [17.689524]], dtype=float32)]\n",
      "9000 0.00015528225 [array([[ -9.723603,  10.983313],\n",
      "       [  9.641462, -10.964138]], dtype=float32), array([[17.91171 ],\n",
      "       [17.789087]], dtype=float32)]\n",
      "9100 0.00014766661 [array([[ -9.746242,  11.005382],\n",
      "       [  9.664118, -10.986343]], dtype=float32), array([[18.010729],\n",
      "       [17.88865 ]], dtype=float32)]\n",
      "9200 0.00014040868 [array([[ -9.768733,  11.027324],\n",
      "       [  9.686644, -11.008423]], dtype=float32), array([[18.10972 ],\n",
      "       [17.988085]], dtype=float32)]\n",
      "9300 0.00013347868 [array([[ -9.791095,  11.049135],\n",
      "       [  9.709021, -11.030383]], dtype=float32), array([[18.208702],\n",
      "       [18.087458]], dtype=float32)]\n",
      "9400 0.00012696601 [array([[ -9.813328,  11.070829],\n",
      "       [  9.731273, -11.052204]], dtype=float32), array([[18.307503],\n",
      "       [18.186829]], dtype=float32)]\n",
      "9500 0.00012072165 [array([[ -9.835422,  11.092408],\n",
      "       [  9.753396, -11.073906]], dtype=float32), array([[18.406303],\n",
      "       [18.286066]], dtype=float32)]\n",
      "9600 0.00011480518 [array([[ -9.857395 ,  11.1138525],\n",
      "       [  9.775386 , -11.095492 ]], dtype=float32), array([[18.505104],\n",
      "       [18.385248]], dtype=float32)]\n",
      "9700 0.00010914211 [array([[ -9.879245,  11.135185],\n",
      "       [  9.797255, -11.11696 ]], dtype=float32), array([[18.603806],\n",
      "       [18.48443 ]], dtype=float32)]\n",
      "9800 0.00010382182 [array([[ -9.900961,  11.156404],\n",
      "       [  9.818998, -11.138299]], dtype=float32), array([[18.702415],\n",
      "       [18.583542]], dtype=float32)]\n",
      "9900 9.8740005e-05 [array([[ -9.922562,  11.177514],\n",
      "       [  9.840613, -11.159528]], dtype=float32), array([[18.801025],\n",
      "       [18.682533]], dtype=float32)]\n",
      "10000 9.3896655e-05 [array([[ -9.944048,  11.198515],\n",
      "       [  9.862113, -11.180646]], dtype=float32), array([[18.899635],\n",
      "       [18.781525]], dtype=float32)]\n",
      "\n",
      "Hypothesis:  [[1.02593222e-04]\n",
      " [9.99915719e-01]\n",
      " [9.99913216e-01]\n",
      " [1.01901845e-04]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "## lab-09-4-xor_tensorboard.py\n",
    "#\n",
    "# Lab 9 XOR\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "learning_rate = 0.01\n",
    "\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2], name='x-input')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name='y-input')\n",
    "\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "    b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    w1_hist = tf.summary.histogram(\"weights1\", W1)\n",
    "    b1_hist = tf.summary.histogram(\"biases1\", b1)\n",
    "    layer1_hist = tf.summary.histogram(\"layer1\", layer1)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "    w2_hist = tf.summary.histogram(\"weights2\", W2)\n",
    "    b2_hist = tf.summary.histogram(\"biases2\", b2)\n",
    "    hypothesis_hist = tf.summary.histogram(\"hypothesis\", hypothesis)\n",
    "\n",
    "# cost/loss function\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                           tf.log(1 - hypothesis))\n",
    "    cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "accuracy_summ = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/xor_logs\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_01\")\n",
    "    writer.add_graph(sess.graph)  # Show the graph\n",
    "\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        summary, _ = sess.run([merged_summary, train], feed_dict={X: x_data, Y: y_data})\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={\n",
    "                  X: x_data, Y: y_data}), sess.run([W1, W2]))\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
